{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to Cancer Genomics\n\u00b6\n\n\nThe University of Sydney\n\u00b6\n\n\n25\nth\n - 27\nth\n July 2017\n\u00b6\n\n\n\n\nThe Cancer Genomics workshop aims to provide an introduction to cancer genomics analytical pipelines for single nucleotide variations (SNV), copy number variations (CNV) and structural variations (SV).\n\n\nWorkshop Content\n\u00b6\n\n\nBy the end of the course participants will be able to:\n\n\u2022 Ability to perform NGS alignment and manipulate the output,\n\n\u2022 Consider and plan experimental design, \n\n\u2022 Be able to identify and generate variants (SNV, CNV, SV, Indels), \n\n\u2022 Interpret variants with potential clinical interest,\n\n\u2022 Be able to visualise and present data\n\n\nThis workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.\n\n\nAcknowledgements\n\u00b6\n\n\nThis workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers] (\nhttp://www.bioplatforms.com/bioinformatics-training/\n) from around Australia.\n\n\nThe Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure.\n\n\nThis workshop is possible thanks to funding support from the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-cancer-genomics",
            "text": "",
            "title": "Welcome to Cancer Genomics"
        },
        {
            "location": "/#the-university-of-sydney",
            "text": "",
            "title": "The University of Sydney"
        },
        {
            "location": "/#25th-27th-july-2017",
            "text": "The Cancer Genomics workshop aims to provide an introduction to cancer genomics analytical pipelines for single nucleotide variations (SNV), copy number variations (CNV) and structural variations (SV).",
            "title": "25th - 27th July 2017"
        },
        {
            "location": "/#workshop-content",
            "text": "By the end of the course participants will be able to: \n\u2022 Ability to perform NGS alignment and manipulate the output, \n\u2022 Consider and plan experimental design,  \n\u2022 Be able to identify and generate variants (SNV, CNV, SV, Indels),  \n\u2022 Interpret variants with potential clinical interest, \n\u2022 Be able to visualise and present data  This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.",
            "title": "Workshop Content"
        },
        {
            "location": "/#acknowledgements",
            "text": "This workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia.  The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure.  This workshop is possible thanks to funding support from the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).",
            "title": "Acknowledgements"
        },
        {
            "location": "/trainers/trainers/",
            "text": "Dr. Ann-Marie Patch\n \n\nSenior Research Officer\n\n Medical Genomics QIMR Berghofer Medical Research Institute, Brisbane  \n\n\n \n\n\nDr. Gayle Philip\n \n\nResearch Fellow (Bioinformatics)\n\nMelbourne Bioinformatics, Carlton, Melbourne  \n\n\n\n\n\n\n\n\n \n\n\nDr. Erdahl Teber\n \n\nSenior Research Officer (Bioinformatics)\n\nChildren Medical Research Institute, Kids Cancer Alliance, University of Sydney, Sydney  \n\n\n \n\n\nDr. Sonika Tyagi\n \n\nBioinformatics Manager\n\nMonash Bioinformatics Platform, Monash University Melbourne \n\n\nBioinformatics Training Platform Development\n\u00b6\n\n\n \n\n\nMr. Jerico Revote\n \n\nSoftware Developer \n\nMonash Bioinformatics Platform, Monash University, Clayton Melbourne  \n\n\n \n\n\nDr. Nathan S. Watson-Haigh\n \n\nResearch Fellow in Bioinformatics\n\nThe Australian Centre for Plant Functional Genomics (ACPFG), Adelaide  \n\n\n\n\n\nWorkshop Coordinator\n\u00b6\n\n\n \n\n\nKatherine Champ\n \n\nWorkshop Coordinator, Project Officer \n\nBioplatform Australia Ltd.",
            "title": "The Trainers"
        },
        {
            "location": "/trainers/trainers/#bioinformatics-training-platform-development",
            "text": "Mr. Jerico Revote   \nSoftware Developer  \nMonash Bioinformatics Platform, Monash University, Clayton Melbourne       Dr. Nathan S. Watson-Haigh   \nResearch Fellow in Bioinformatics \nThe Australian Centre for Plant Functional Genomics (ACPFG), Adelaide",
            "title": "Bioinformatics Training Platform Development"
        },
        {
            "location": "/trainers/trainers/#workshop-coordinator",
            "text": "Katherine Champ   \nWorkshop Coordinator, Project Officer  \nBioplatform Australia Ltd.",
            "title": "Workshop Coordinator"
        },
        {
            "location": "/timetables/timetable_cancer/",
            "text": "Cancer Genomics Course\n\u00b6\n\n\n\n\nUniversity of Sydney - 25\nth\n to 27\nth\n July 2017\n\n\nInstructors\n\u00b6\n\n\n\n\nKatherine Champ (KC)- Bioplatforms Australia, Sydney\n\n\nAnn-Marie Patch (AMP) - QIMR Berghofer, Brisbane\n\n\nGayle Philip (GP) - Melbourne Bioinformatics, Melbourne\n\n\nErdahl Teber (ET) - CMRI, Sydney\n\n\nSonika Tyagi (ST) - Monash University, Melbourne\n\n\n\n\nWorkshop Material\n\u00b6\n\n\nMaterial for the workshop can be found \nhere\n.\n\n\nTimetable\n\u00b6\n\n\nDay 1\n\u00b6\n\n\n25\nth\n July\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nWelcome\n\n\n\n\nKC\n\n\n\n\n\n\n9:15\n\n\nIntroduction to cancer genomics and NGS techniques \u2013 focus on DNA\n\n\n\n\nAMP\n\n\n\n\n\n\n10:15\n\n\nBreak\n\n\n\n\n\n\n\n\n\n\n10:30\n\n\nExperimental design (interactive/ice breaker/group activity) and caveats; considerations on processing capacities; 10 ways to ruin your experiment\n\n\n\n\nAMP\n\n\n\n\n\n\n11:30\n\n\nCommand line intro (Unix, R) \u2013 L(15\u2019) / P(30\u2019)\n\n\n[Practical]\n[Commands in slides]\n\n\nGP\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nRaw data - FASTQ format and QC\n\n\n\n\nST\n\n\n\n\n\n\n14:00\n\n\nAlignment (L)\n\n\n\n\nST\n\n\n\n\n\n\n14:30\n\n\nPractical: Manipulation of BAM files and QC\n\n\n[QC Practical]\n[Alignment Practical]\n\n\nST/GP\n\n\n\n\n\n\n15:00\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n15:15\n\n\nPractical: Manipulation of BAM files and QC (cont.)\n\n\n\n\nGP/ST\n\n\n\n\n\n\n17:00\n\n\nQ&A\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 2\n\u00b6\n\n\n26\nth\n July\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nSNV detection (review, germline vs somatic, tools, pitfalls, data visualization) \u2013 L/P. Indels \u2013 cover in SNV lecture + bonus exercises (P), specific challenges of indels analysis, tools\n\n\n\n\nST\n\n\n\n\n\n\n9:45\n\n\nPractical: SNV detection\n\n\n[Practical]\n\n\nST\n\n\n\n\n\n\n10:30\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n10:45\n\n\nVariants annotation and filtration (L) \u2013 tools landscape\n\n\n\n\nST\n\n\n\n\n\n\n11:00\n\n\nPractical: Variants visualization (IGV), annotation and filtration\n\n\n[Practical]\n\n\nST\n\n\n\n\n\n\n12:30\n\n\nLunch + coffee\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nCNV analysis using NGS data (L)\n\n\n\n\nAMP\n\n\n\n\n\n\n14:15\n\n\nPractical: CNV analysis \u2013 deletion/amplification, calling CNVs, visualization, interpretation\n\n\n[Practical]\n\n\nGP/AMP\n\n\n\n\n\n\n15:00\n\n\nBreak\n\n\n\n\n\n\n\n\n\n\n15:15\n\n\nPractical: CNV analysis \u2013 deletion/amplification, calling CNVs, visualization, interpretation (cont.)\n\n\n\n\nGP/AMP\n\n\n\n\n\n\n17:00\n\n\nQ&A\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 3\n\u00b6\n\n\n27\nth\n July\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nSV analysis \u2013 breakpoints/fusion, tools\n\n\n\n\nAMP\n\n\n\n\n\n\n9:45\n\n\nPractical: SV analysis \u2013 breakpoints/fusion\n\n\n[Practical]\n\n\nET\n\n\n\n\n\n\n10:30\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n10:45\n\n\nPractical: SV analysis \u2013 breakpoints/fusion (cont.)\n\n\n\n\nET\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nDownstream analysis and interpretation \u2013 Exploration of resources that can be used for this. E.g. databases (COSMIC, TCGA, etc.), integration with clinical information\n\n\n[Practical 1]\n \n[Practical 2]\n\n\nAMP\n\n\n\n\n\n\n14:15\n\n\nPractical: Downstream analysis and interpretation\n\n\n\n\nAMP\n\n\n\n\n\n\n15:00\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n15:15\n\n\nPractical: Downstream analysis and interpretation(cont.)\n\n\n\n\nAMP\n\n\n\n\n\n\n16:00\n\n\nHow does it all link together? Integration of different data types\n\n\n\n\nMark Cowley\n\n\n\n\n\n\n16:45\n\n\nQ&A, wrap up (how to access course\u2019s VM) & survey\n\n\n\n\nAll",
            "title": "Timetable"
        },
        {
            "location": "/timetables/timetable_cancer/#cancer-genomics-course",
            "text": "University of Sydney - 25 th  to 27 th  July 2017",
            "title": "Cancer Genomics Course"
        },
        {
            "location": "/timetables/timetable_cancer/#instructors",
            "text": "Katherine Champ (KC)- Bioplatforms Australia, Sydney  Ann-Marie Patch (AMP) - QIMR Berghofer, Brisbane  Gayle Philip (GP) - Melbourne Bioinformatics, Melbourne  Erdahl Teber (ET) - CMRI, Sydney  Sonika Tyagi (ST) - Monash University, Melbourne",
            "title": "Instructors"
        },
        {
            "location": "/timetables/timetable_cancer/#workshop-material",
            "text": "Material for the workshop can be found  here .",
            "title": "Workshop Material"
        },
        {
            "location": "/timetables/timetable_cancer/#timetable",
            "text": "",
            "title": "Timetable"
        },
        {
            "location": "/timetables/timetable_cancer/#day-1",
            "text": "25 th  July  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      9:00  Welcome   KC    9:15  Introduction to cancer genomics and NGS techniques \u2013 focus on DNA   AMP    10:15  Break      10:30  Experimental design (interactive/ice breaker/group activity) and caveats; considerations on processing capacities; 10 ways to ruin your experiment   AMP    11:30  Command line intro (Unix, R) \u2013 L(15\u2019) / P(30\u2019)  [Practical] [Commands in slides]  GP    12:30  Lunch      13:30  Raw data - FASTQ format and QC   ST    14:00  Alignment (L)   ST    14:30  Practical: Manipulation of BAM files and QC  [QC Practical] [Alignment Practical]  ST/GP    15:00  Coffee break      15:15  Practical: Manipulation of BAM files and QC (cont.)   GP/ST    17:00  Q&A   All",
            "title": "Day 1"
        },
        {
            "location": "/timetables/timetable_cancer/#day-2",
            "text": "26 th  July  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      9:00  SNV detection (review, germline vs somatic, tools, pitfalls, data visualization) \u2013 L/P. Indels \u2013 cover in SNV lecture + bonus exercises (P), specific challenges of indels analysis, tools   ST    9:45  Practical: SNV detection  [Practical]  ST    10:30  Coffee break      10:45  Variants annotation and filtration (L) \u2013 tools landscape   ST    11:00  Practical: Variants visualization (IGV), annotation and filtration  [Practical]  ST    12:30  Lunch + coffee      13:30  CNV analysis using NGS data (L)   AMP    14:15  Practical: CNV analysis \u2013 deletion/amplification, calling CNVs, visualization, interpretation  [Practical]  GP/AMP    15:00  Break      15:15  Practical: CNV analysis \u2013 deletion/amplification, calling CNVs, visualization, interpretation (cont.)   GP/AMP    17:00  Q&A   All",
            "title": "Day 2"
        },
        {
            "location": "/timetables/timetable_cancer/#day-3",
            "text": "27 th  July  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      9:00  SV analysis \u2013 breakpoints/fusion, tools   AMP    9:45  Practical: SV analysis \u2013 breakpoints/fusion  [Practical]  ET    10:30  Coffee break      10:45  Practical: SV analysis \u2013 breakpoints/fusion (cont.)   ET    12:30  Lunch      13:30  Downstream analysis and interpretation \u2013 Exploration of resources that can be used for this. E.g. databases (COSMIC, TCGA, etc.), integration with clinical information  [Practical 1]   [Practical 2]  AMP    14:15  Practical: Downstream analysis and interpretation   AMP    15:00  Coffee break      15:15  Practical: Downstream analysis and interpretation(cont.)   AMP    16:00  How does it all link together? Integration of different data types   Mark Cowley    16:45  Q&A, wrap up (how to access course\u2019s VM) & survey   All",
            "title": "Day 3"
        },
        {
            "location": "/preamble/",
            "text": "Providing Feedback\n\u00b6\n\n\nWhile we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.\n\n\nWhilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!\n\n\nClearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!\n\n\nWith that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:\n\n\n\n\n\n\nA sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!\n\n\n\n\n\n\nSome empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.\n\n\n\n\n\n\nAn online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!\n\n\n\n\n\n\nDocument Structure\n\u00b6\n\n\nWe have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.\n\n\n\n\nWhile you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!\n\n\n\n\nThe commands to enter at a terminal look something like this:\n\n\n1\ntophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n\n\n\n\n\n\nThe following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc\n\n\n1\ntophat [options]* <index_base> <reads_1> <reads_2>\n\n\n\n\n\n\nThe following is an example how of R commands are styled:\n\n\n1\n2\n3\n4\n5\nR \n--\nno\n-\nsave\n\n\nlibrary\n(\nplotrix\n)\n\ndata \n<-\n read.table\n(\n\"run_25/stats.txt\"\n,\n header\n=\nTRUE\n)\n\nweighted.hist\n(\ndata\n$\nshort1_cov\n+\ndata\n$\nshort2_cov\n,\n data\n$\nlgth\n,\n breaks\n=\n0\n:\n70\n)\n\n\nq\n()\n\n\n\n\n\n\n\nThe following icons are used throughout the documentation\nto help you navigate around the document more easily:\n\n\n\n\nQuestion\n\n\nQuestions to answer.\n\n\n\n\n\n\nAnswer\n\n\nAnswers will be provided at the end of the workskop.\n\n\n\n\n\n\nImportant\n\n\nThis is important. \n\n\n\n\n\n\nSTOP\n\n\nWarning - STOP and read.\n\n\n\n\n\n\nBonus exercise\n\n\nBonus exercise for fast learners.\n\n\n\n\n\n\nAdvanced exercise\n\n\nAdvanced exercise for super-fast learners\n\n\n\n\nResources Used\n\u00b6\n\n\nWe have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.",
            "title": "Workshop Information"
        },
        {
            "location": "/preamble/#providing-feedback",
            "text": "While we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.  Whilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!  Clearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!  With that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:    A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!    Some empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.    An online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!",
            "title": "Providing Feedback"
        },
        {
            "location": "/preamble/#document-structure",
            "text": "We have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.   While you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!   The commands to enter at a terminal look something like this:  1 tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq   The following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc  1 tophat [options]* <index_base> <reads_1> <reads_2>   The following is an example how of R commands are styled:  1\n2\n3\n4\n5 R  -- no - save  library ( plotrix ) \ndata  <-  read.table ( \"run_25/stats.txt\" ,  header = TRUE ) \nweighted.hist ( data $ short1_cov + data $ short2_cov ,  data $ lgth ,  breaks = 0 : 70 )  q ()    The following icons are used throughout the documentation\nto help you navigate around the document more easily:   Question  Questions to answer.    Answer  Answers will be provided at the end of the workskop.    Important  This is important.     STOP  Warning - STOP and read.    Bonus exercise  Bonus exercise for fast learners.    Advanced exercise  Advanced exercise for super-fast learners",
            "title": "Document Structure"
        },
        {
            "location": "/preamble/#resources-used",
            "text": "We have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.",
            "title": "Resources Used"
        },
        {
            "location": "/modules/cancer-module-cli/commandline/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nFamiliarise yourself with the command line environment on a Linux\n    operating system.\n\n\n\n\n\n\nRun some basic linux system and file operation commands\n\n\n\n\n\n\nNavigation of biological data files structure and manipulation\n\n\n\n\n\n\n\n\nResources\n\u00b6\n\n\nTools\n\u00b6\n\n\n\n\n\n\nBasic Linux system commands on an Ubuntu OS.\n\n\n\n\n\n\nBasic file operation commands\n\n\n\n\n\n\nLinks\n\u00b6\n\n\n\n\n\n\nSoftware Carpentry\n\n\n\n\n\n\nExample 1000Genome Project data\n\n\n\n\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n\nMatt Field \nmatt.field@anu.edu.au\n     \n\n\n\n\nShell Exercise\n\u00b6\n\n\nLet\u2019s try out your new shell skills on some real data.\n\n\nThe file \n1000gp.vcf\n is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the \n1000 Genomes Project\n.\nThe \u2019vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n1\n2\n3\n4\n5\ncd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf\n\n\n\n\n\n\n\nQuestion\n\n\nWhat is the file size (in kilo-bytes), and how many lines are in the file?.\n\n\nHint\nman ls\n, \nman wc\n\n\nAnswer\n3.6M\n45034 lines\n\n\n\n\nBecause this file is so large, you\u2019re going to almost always want to\npipe (\u2018|\u2019) the result of any command to less (a simple text viewer, type\n\nq\n to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.\n\n\nLet\u2019s start by printing the first 5 lines to see what it looks like.\n  \n1\nhead -5 1000gp.vcf\n\n\n\n\n\nThat isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \n#\n)!\n\n\nPrint the first 20 lines to see more of the file.\n\n1\nhead -20 1000gp.vcf\n\n\n\n\n\nOkay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:\n\n\n\n\n\n\nthe chromosome (which volume the difference is in)\n\n\n\n\n\n\nthe position (which character in the volume the difference starts\n    at)\n\n\n\n\n\n\nthe ID of the difference\n\n\n\n\n\n\nthe sequence in the reference human(s)\n\n\n\n\n\n\nThe rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.\n\n\nTo start analyzing the actual data, we have to remove the header.\n\n\n\n\nQuestion\n\n\nHow can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?\n\n\nHint\nman grep\n (remember to use pipes \u2018|\u2019)\n\n\nAnswer\n1\ngrep -v \"^#\" 1000gp.vcf | head\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an advanced section.\n\n\n\n\n\n\n\n\nQuestion\n\n\nHow many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?\n\n\nAnswer\n1\ngrep -v \"^#\" 1000gp.vcf | wc -l\n\n\n\n\n\n(should print 45024)\n\n\n\n\nWhere these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).\n\n\n\n\nQuestion\n\n\nPrint the first 10 chromosomes, one per line.\n\n\nHint\nman cut\n (remember to remove header lines first)\n\n\nAnswer\n1\ngrep -v \"^#\" 1000gp.vcf | cut -f 1 | head\n\n\n\n\n\n\n\n\n\nAs you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).\n\n\nLet\u2019s look at which chromosomes these variations are on.\n\n\n\n\nQuestion\n\n\nPrint a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).\n\n\nHint\nRemove all duplicates from your previous answer (\nman sort\n)\n\n\nAnswer\n1\ngrep -v \"^#\" 1000gp.vcf | cut -f 1 | sort -u\n\n\n\n\n\n\n\n\n\nRather than using \nsort\n to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command, \nuniq\n. The \nuniq\n\ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted, \nuniq\n won\u2019t work properly.\n\n\n\n\nQuestion\n\n\nUsing \nsort\n and \nuniq\n, print the number of times each chromosome\noccurs in the file.\n\n\nHint\nman uniq\n\n\nAnswer\n1\ngrep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nAdd to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.\n\n\nHint\nMake sure you\u2019re sorting in descending order. By default, \nsort\n\nsorts in ascending order.\n\n\nAnswer\n1\ngrep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r\n\n\n\n\n\n\n\n\n\nThis is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.\n\n\n\n\nQuestion\n\n\nSort the previous output by chromosome number\n\n\nHint\nA lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.\n\n\nAnswer\n1\ngrep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n",
            "title": "Introduction to Command Line"
        },
        {
            "location": "/modules/cancer-module-cli/commandline/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Familiarise yourself with the command line environment on a Linux\n    operating system.    Run some basic linux system and file operation commands    Navigation of biological data files structure and manipulation",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-cli/commandline/#resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/modules/cancer-module-cli/commandline/#tools",
            "text": "Basic Linux system commands on an Ubuntu OS.    Basic file operation commands",
            "title": "Tools"
        },
        {
            "location": "/modules/cancer-module-cli/commandline/#links",
            "text": "Software Carpentry    Example 1000Genome Project data",
            "title": "Links"
        },
        {
            "location": "/modules/cancer-module-cli/commandline/#author-information",
            "text": "Primary Author(s): \nMatt Field  matt.field@anu.edu.au",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-cli/commandline/#shell-exercise",
            "text": "Let\u2019s try out your new shell skills on some real data.  The file  1000gp.vcf  is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the  1000 Genomes Project .\nThe \u2019vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.  Open the Terminal and go to the directory where the data are stored: 1\n2\n3\n4\n5 cd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf    Question  What is the file size (in kilo-bytes), and how many lines are in the file?.  Hint man ls ,  man wc  Answer 3.6M 45034 lines   Because this file is so large, you\u2019re going to almost always want to\npipe (\u2018|\u2019) the result of any command to less (a simple text viewer, type q  to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.  Let\u2019s start by printing the first 5 lines to see what it looks like.\n   1 head -5 1000gp.vcf   That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with  # )!  Print the first 20 lines to see more of the file. 1 head -20 1000gp.vcf   Okay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:    the chromosome (which volume the difference is in)    the position (which character in the volume the difference starts\n    at)    the ID of the difference    the sequence in the reference human(s)    The rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.  To start analyzing the actual data, we have to remove the header.   Question  How can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?  Hint man grep  (remember to use pipes \u2018|\u2019)  Answer 1 grep -v \"^#\" 1000gp.vcf | head       This is an advanced section.     Question  How many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?  Answer 1 grep -v \"^#\" 1000gp.vcf | wc -l   (should print 45024)   Where these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).   Question  Print the first 10 chromosomes, one per line.  Hint man cut  (remember to remove header lines first)  Answer 1 grep -v \"^#\" 1000gp.vcf | cut -f 1 | head     As you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).  Let\u2019s look at which chromosomes these variations are on.   Question  Print a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).  Hint Remove all duplicates from your previous answer ( man sort )  Answer 1 grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort -u     Rather than using  sort  to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command,  uniq . The  uniq \ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted,  uniq  won\u2019t work properly.   Question  Using  sort  and  uniq , print the number of times each chromosome\noccurs in the file.  Hint man uniq  Answer 1 grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c      Question  Add to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.  Hint Make sure you\u2019re sorting in descending order. By default,  sort \nsorts in ascending order.  Answer 1 grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r     This is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.   Question  Sort the previous output by chromosome number  Hint A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.  Answer 1 grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n",
            "title": "Shell Exercise"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nAssess the overall quality of NGS (FastQ format) sequence reads\n\n\n\n\n\n\nVisualise the quality, and other associated matrices, of reads to\n    decide on filters and cutoffs for cleaning up data ready for\n    downstream analysis\n\n\n\n\n\n\nClean up adaptors and pre-process the sequence data for further\n    analysis\n\n\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nSkewer:\n\n\nhttp://sourceforge.net/projects/skewer/\n\n\nFASTX-Toolkit:\n\n\nhttp://hannonlab.cshl.edu/fastx_toolkit/\n\n\n\n\nUseful Links\n\u00b6\n\n\nFASTQ Encoding: (\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n)\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n  \n\nSonika Tyagi: \nsonika.tyagi@monash.edu\n\n\nContributor(s):\n  \n\nNandan Deshpande: \nn.deshpande@unsw.edu.au\n\n\n\n\nIntroduction\n\u00b6\n\n\nGoing on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!\n\n\nFor the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.\n\n\nOne way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.\n\n\nHighly redundant coverage (>15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.\n\n\nQuality Value Encoding Schema\n\u00b6\n\n\nQuality scoring calculates a set of predictors for each base call, and then uses the predictor\nvalues to look up the Q-score in a quality table. Quality tables are created to provide optimally\naccurate quality predictions for runs generated by a specific configuration of sequencing\nplatform and version of chemistry (\nwww.illumina.com\n).\nIn order to use a single character to encode Phred qualities, \nASCII characters\n are used. All ASCII characters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (\n!\n). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.\n\n\nEarly Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters < 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.\n\n\nFASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are (\n@ABCDEFGHI\n), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.\n\n\nFor a graphical representation of the different ASCII characters used in\nthe two encoding schema see:\n(\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n).\n\n\nQ-score encoding implemented with the Novaseq platform\n\u00b6\n\n\nIn order to reduce the data footprints Illumina has come up with a new\nmethod to reduce quality score resolution and optimise data storae. The new Q-score\nencoding now follows an 8 level mapping of individual quality scores (0-40 or >40) [See \nTable 1\n].\nWith the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that\nbin mapped to a new value of 22. This can be thought of as simply replacing all the\noccurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence.\nIllumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically > 50% and the resulting sorted BAM  les are reduced by ~30%.\n\n\n\n\n\n\n\n\nQuality Score Bins\n\n\nMapped quality scores\n\n\n\n\n\n\n\n\n\n\nN (no call)\n\n\nN (no call)\n\n\n\n\n\n\n2-9\n\n\n6\n\n\n\n\n\n\n10-19\n\n\n15\n\n\n\n\n\n\n20-24\n\n\n22\n\n\n\n\n\n\n25-29\n\n\n27\n\n\n\n\n\n\n30-34\n\n\n33\n\n\n\n\n\n\n35-39\n\n\n37\n\n\n\n\n\n\n>=40\n\n\n40\n\n\n\n\n\n\n\n\nTable 1:\n Novaseq Q-score bins mapping\n\n\n\n\nPrepare the Environment\n\u00b6\n\n\nTo investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\n4\ncd\nls\ncd qc\npwd\n\n\n\n\n\n\nAt any time, help can be displayed for FastQC using the following\ncommand:\n\n\n1\nfastqc -h\n\n\n\n\n\n\nLook at SYNOPSIS (Usage) and options after typing \nfastqc -h\n\n\n\n\nQuality Visualisation\n\u00b6\n\n\nWe have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.\n\n\nExecute the following command on the two files:\n\n\n1\n2\nfastqc -f fastq qcdemo_R1.fastq.gz\nfastqc -f fastq qcdemo_R2.fastq.gz\n\n\n\n\n\n\nView the FastQC report file of the bad data using a web browser such as\nfirefox. The \n&\n sign puts the job in the background.\n\n\n1\nfirefox qcdemo_R2_fastqc.html &\n\n\n\n\n\n\n\nThe report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics e.g.:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n1000000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n150\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nTable 2:\n Summary statistics for bad_example_untrimmed\n\n\n\n\n\n\nFigure 1:\n bad_example_untrimmed_QC_plot\n\n\nA Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base A is wrong\n(P(A)) is expressed by a quality score, Q(A), according to the\nrelationship:\n\n\n\n\nQ(A) =-10 log10(P(A))\n\n\n\n\nThe relationship between the quality score and error probability is\ndemonstrated with the following table:\n\n\n\n\n\n\n\n\nQuality score, Q(A)\n\n\nError probability, P(A)\n\n\nAccuracy of base call\n\n\n\n\n\n\n\n\n\n\n10\n\n\n0.1\n\n\n90%\n\n\n\n\n\n\n20\n\n\n0.01\n\n\n99%\n\n\n\n\n\n\n30\n\n\n0.001\n\n\n99.9%\n\n\n\n\n\n\n40\n\n\n0.0001\n\n\n99.99%\n\n\n\n\n\n\n50\n\n\n0.00001\n\n\n99.999%\n\n\n\n\n\n\n\n\nTable 3:\n Quality Error Probabilities\n\n\n\n\nQuestion\n\n\nHow many sequences were there in your file? What is the read length?\n\n\nAnswer\n1,000,000. read length=150bp\n\n\n\n\n\n\nQuestion\n\n\nDoes the quality score values vary throughout the read length?\n\n\nHint\nLook at the \u2019per base sequence quality plot\u2019\n\n\nAnswer\nYes. Quality scores are dropping towards the end of the reads.\n\n\n\n\n\n\nQuestion\n\n\nWhat is the quality score range you see?\n\n\nAnswer\n2-40\n\n\n\n\n\n\nQuestion\n\n\nAt around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?\n\n\nAnswer\nAround 30 bp position\n\n\n\n\n\n\nQuestion\n\n\nHow can we trim the reads to filter out the low quality data?\n\n\nAnswer\nBy trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.\n\n\n\n\nGood Quality Data\n\u00b6\n\n\nView the FastQC report files \nfastqc_report.html\n to see examples of a\ngood quality data and compare the quality plot with that of the\n\nbad_example_fastqc\n.\n\n\n1\nfirefox qcdemo_R1_fastqc.html &\n\n\n\n\n\n\nSequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.\n\n\n\n\nRead Trimming\n\u00b6\n\n\nRead trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.\n\n\nQuality Based Trimming\n\u00b6\n\n\nBase call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.\n\n\nThe previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.\n\n\nRun the following command to quality trim a set of paired end data.\n\n\n1\n2\ncd /home/trainee/qc\nskewer -t 4 -l 50  -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n\n\n\n\n\n\n\n\n-t:\n number of threads to use\n\n\n-l:\n min length to keep after trimming\n\n\n-q:\n Quality threshold used for trimming at 3\u2019 end\n\n\n-Q:\n mean quality threshold for a read\n\n\n-m:\n pair-end mode  \n\n\n\n\n\nRun FastQC on the quality trimmed file and visualise the quality scores.\n\n\nLook at the last files generated, are the file names same as the input ?\n\n\n1\nls -ltr\n\n\n\n\n\n\nRun Fastqc on the quality trimmed files:\n\n\n1\n2\nfastqc -f fastq qcdemo-trimmed-pair1.fastq\nfastqc -f fastq qcdemo-trimmed-pair2.fastq\n\n\n\n\n\n\nVisualise the \nfastqc\n results:\n\n\n1\n2\nfirefox qcdemo-trimmed-pair1_fastqc.html &\nfirefox qcdemo-trimmed-pair2_fastqc.html &\n\n\n\n\n\n\n\nLet\u2019s look at the quality from the second reads. The output should look\nlike:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo-trimmed-pair2.fastq\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n742262\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n50-150\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nTable 4:\n Summary Statistics of QC_demo_R1_trimmed\n\n\n\n\n\n\nFigure 2:\n bad_example_quality_trimmed_plot\n\n\n\n\nQuestion\n\n\nDid the number of total reads in R1 and R2 change after trimming?\n\n\nAnswer\nQuality trimming discarded >25000 reads. However, we retain a lot of\nmaximal length reads which have good quality all the way to the ends.\n\n\n\n\n\n\nQuestion\n\n\nWhat reads lengths were obtained after quality based trimming?\n\n\nAnswer\n50-150\n\nReads <50 bp, following quality trimming, were discarded.\n\n\n\n\n\n\nQuestion\n\n\nDid you observe adapter sequences in the data?\n\n\nAnswer\nNo. (Hint: look at the overrepresented sequences)    \n\n\n\n\n\n\nQuestion\n\n\nHow can you use -a option with fastqc? (Hint: try fastqc -h).\n\n\nAnswer\nAdaptors can be supplied in a file for screening.\n\n\n\n\nAdapter Clipping\n\u00b6\n\n\nSometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.\n\n\nThis is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.\n\n\nVarious QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from \nskewer\n which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence:\n\n\n\n\nCutadapt: \nhttp://code.google.com/p/cutadapt/\n\n\nTrimmomatic: \nhttp://www.usadellab.org/cms/?page=trimmomatic\n\n\n\n\nHere we are demonstrating \nSkewer\n to trim a given adapter sequence.\n\n\n1\n2\n3\n4\ncd /home/trainee/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nfirefox adaptorQC_fastqc.html\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n\n\n\n\n\n\n\n\n-x:\n adaptor sequence used\n\n\n-t:\n number of threads to use\n\n\n-l:\n min length to keep after trimming\n\n\n-L:\n Max length to keep after trimming, in this experiment we were expecting only small RNA fragments\n\n\n-Q:\n Quality threshold used for trimming at 3\u2019 end. Use -m option to control the end you want to trim  \n\n\n\n\n\nRun FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.\n\n\n1\n2\nfastqc adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html &\n\n\n\n\n\n\nFixed Length Trimming\n\u00b6\n\n\nWe will not cover fixed length trimming but provide the following for\nyour information.\n Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the \nfastx_trimmer\n from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type \nfastx_trimmer -h\n at anytime to display help.\n\n\nWe will now do fixed-length trimming of the \nbad_example.fastq\n file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.\n\n\n1\n2\n3\n4\ncd /home/trainee/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n\n\n\n\n\n\nWe used the following options in the command above:\n\n\n\n\n-Q 33:\n Indicates the input quality scores are Phred+33 encoded\n\n\n-f:\n First base to be retained in the output\n\n\n-l:\n Last base to be retained in the output\n\n\n-i:\n Input FASTQ file name\n\n\n-o:\n Output file name  \n\n\n\n\n\nRun FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.\n\n\n1\n2\nfastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html &\n\n\n\n\n\n\nThe output should look like:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nFilename\n\n\nbad_example_trimmed01.fastq\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n40000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n80\n\n\n\n\n\n\n%GC\n\n\n48\n\n\n\n\n\n\n\n\nTable 5:\n Summary Statistics of bad_example_trimmed summary\n\n\n\n\n\n\nFigure 3:\n bad_example_trimmed_plot\n\n\n\n\nQuestion\n\n\nWhat values would you use for \n-f\n if you wanted to trim off 10 bases at the 5\u2019 end of the reads?\n\n\nAnswer\n-f 11",
            "title": "Quality Control"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Assess the overall quality of NGS (FastQ format) sequence reads    Visualise the quality, and other associated matrices, of reads to\n    decide on filters and cutoffs for cleaning up data ready for\n    downstream analysis    Clean up adaptors and pre-process the sequence data for further\n    analysis",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#tools-used",
            "text": "FastQC:  http://www.bioinformatics.babraham.ac.uk/projects/fastqc/  Skewer:  http://sourceforge.net/projects/skewer/  FASTX-Toolkit:  http://hannonlab.cshl.edu/fastx_toolkit/",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#useful-links",
            "text": "FASTQ Encoding: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding )",
            "title": "Useful Links"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#author-information",
            "text": "Primary Author(s):    \nSonika Tyagi:  sonika.tyagi@monash.edu  Contributor(s):    \nNandan Deshpande:  n.deshpande@unsw.edu.au",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#introduction",
            "text": "Going on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!  For the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.  One way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.  Highly redundant coverage (>15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.",
            "title": "Introduction"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#quality-value-encoding-schema",
            "text": "Quality scoring calculates a set of predictors for each base call, and then uses the predictor\nvalues to look up the Q-score in a quality table. Quality tables are created to provide optimally\naccurate quality predictions for runs generated by a specific configuration of sequencing\nplatform and version of chemistry ( www.illumina.com ).\nIn order to use a single character to encode Phred qualities,  ASCII characters  are used. All ASCII characters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark ( ! ). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.  Early Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters < 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.  FASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are ( @ABCDEFGHI ), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.  For a graphical representation of the different ASCII characters used in\nthe two encoding schema see:\n( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ).",
            "title": "Quality Value Encoding Schema"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#q-score-encoding-implemented-with-the-novaseq-platform",
            "text": "In order to reduce the data footprints Illumina has come up with a new\nmethod to reduce quality score resolution and optimise data storae. The new Q-score\nencoding now follows an 8 level mapping of individual quality scores (0-40 or >40) [See  Table 1 ].\nWith the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that\nbin mapped to a new value of 22. This can be thought of as simply replacing all the\noccurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence.\nIllumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically > 50% and the resulting sorted BAM  les are reduced by ~30%.     Quality Score Bins  Mapped quality scores      N (no call)  N (no call)    2-9  6    10-19  15    20-24  22    25-29  27    30-34  33    35-39  37    >=40  40     Table 1:  Novaseq Q-score bins mapping",
            "title": "Q-score encoding implemented with the Novaseq platform"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#prepare-the-environment",
            "text": "To investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3\n4 cd\nls\ncd qc\npwd   At any time, help can be displayed for FastQC using the following\ncommand:  1 fastqc -h   Look at SYNOPSIS (Usage) and options after typing  fastqc -h",
            "title": "Prepare the Environment"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#quality-visualisation",
            "text": "We have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.  Execute the following command on the two files:  1\n2 fastqc -f fastq qcdemo_R1.fastq.gz\nfastqc -f fastq qcdemo_R2.fastq.gz   View the FastQC report file of the bad data using a web browser such as\nfirefox. The  &  sign puts the job in the background.  1 firefox qcdemo_R2_fastqc.html &   \nThe report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics e.g.:     Property  Value      Filename  qcdemo_R2.fastq.gz    File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  1000000    Filtered Sequences  0    Sequence length  150    %GC  37     Table 2:  Summary statistics for bad_example_untrimmed    Figure 1:  bad_example_untrimmed_QC_plot  A Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base A is wrong\n(P(A)) is expressed by a quality score, Q(A), according to the\nrelationship:   Q(A) =-10 log10(P(A))   The relationship between the quality score and error probability is\ndemonstrated with the following table:     Quality score, Q(A)  Error probability, P(A)  Accuracy of base call      10  0.1  90%    20  0.01  99%    30  0.001  99.9%    40  0.0001  99.99%    50  0.00001  99.999%     Table 3:  Quality Error Probabilities   Question  How many sequences were there in your file? What is the read length?  Answer 1,000,000. read length=150bp    Question  Does the quality score values vary throughout the read length?  Hint Look at the \u2019per base sequence quality plot\u2019  Answer Yes. Quality scores are dropping towards the end of the reads.    Question  What is the quality score range you see?  Answer 2-40    Question  At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?  Answer Around 30 bp position    Question  How can we trim the reads to filter out the low quality data?  Answer By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.",
            "title": "Quality Visualisation"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#good-quality-data",
            "text": "View the FastQC report files  fastqc_report.html  to see examples of a\ngood quality data and compare the quality plot with that of the bad_example_fastqc .  1 firefox qcdemo_R1_fastqc.html &   Sequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.",
            "title": "Good Quality Data"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#read-trimming",
            "text": "Read trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.",
            "title": "Read Trimming"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#quality-based-trimming",
            "text": "Base call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.  The previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.  Run the following command to quality trim a set of paired end data.  1\n2 cd /home/trainee/qc\nskewer -t 4 -l 50  -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz    -t:  number of threads to use  -l:  min length to keep after trimming  -q:  Quality threshold used for trimming at 3\u2019 end  -Q:  mean quality threshold for a read  -m:  pair-end mode     \nRun FastQC on the quality trimmed file and visualise the quality scores.  Look at the last files generated, are the file names same as the input ?  1 ls -ltr   Run Fastqc on the quality trimmed files:  1\n2 fastqc -f fastq qcdemo-trimmed-pair1.fastq\nfastqc -f fastq qcdemo-trimmed-pair2.fastq   Visualise the  fastqc  results:  1\n2 firefox qcdemo-trimmed-pair1_fastqc.html &\nfirefox qcdemo-trimmed-pair2_fastqc.html &   \nLet\u2019s look at the quality from the second reads. The output should look\nlike:     Property  Value      Filename  qcdemo-trimmed-pair2.fastq    File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  742262    Filtered Sequences  0    Sequence length  50-150    %GC  37     Table 4:  Summary Statistics of QC_demo_R1_trimmed    Figure 2:  bad_example_quality_trimmed_plot   Question  Did the number of total reads in R1 and R2 change after trimming?  Answer Quality trimming discarded >25000 reads. However, we retain a lot of\nmaximal length reads which have good quality all the way to the ends.    Question  What reads lengths were obtained after quality based trimming?  Answer 50-150 \nReads <50 bp, following quality trimming, were discarded.    Question  Did you observe adapter sequences in the data?  Answer No. (Hint: look at the overrepresented sequences)        Question  How can you use -a option with fastqc? (Hint: try fastqc -h).  Answer Adaptors can be supplied in a file for screening.",
            "title": "Quality Based Trimming"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#adapter-clipping",
            "text": "Sometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.  This is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.  Various QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from  skewer  which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence:   Cutadapt:  http://code.google.com/p/cutadapt/  Trimmomatic:  http://www.usadellab.org/cms/?page=trimmomatic   Here we are demonstrating  Skewer  to trim a given adapter sequence.  1\n2\n3\n4 cd /home/trainee/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nfirefox adaptorQC_fastqc.html\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz    -x:  adaptor sequence used  -t:  number of threads to use  -l:  min length to keep after trimming  -L:  Max length to keep after trimming, in this experiment we were expecting only small RNA fragments  -Q:  Quality threshold used for trimming at 3\u2019 end. Use -m option to control the end you want to trim     \nRun FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.  1\n2 fastqc adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html &",
            "title": "Adapter Clipping"
        },
        {
            "location": "/modules/cancer-module-qc/ngs-qc/#fixed-length-trimming",
            "text": "We will not cover fixed length trimming but provide the following for\nyour information.  Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the  fastx_trimmer  from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type  fastx_trimmer -h  at anytime to display help.  We will now do fixed-length trimming of the  bad_example.fastq  file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.  1\n2\n3\n4 cd /home/trainee/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq   We used the following options in the command above:   -Q 33:  Indicates the input quality scores are Phred+33 encoded  -f:  First base to be retained in the output  -l:  Last base to be retained in the output  -i:  Input FASTQ file name  -o:  Output file name     \nRun FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.  1\n2 fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html &   The output should look like:     Property  Value      Filename  bad_example_trimmed01.fastq    File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  40000    Filtered Sequences  0    Sequence length  80    %GC  48     Table 5:  Summary Statistics of bad_example_trimmed summary    Figure 3:  bad_example_trimmed_plot   Question  What values would you use for  -f  if you wanted to trim off 10 bases at the 5\u2019 end of the reads?  Answer -f 11",
            "title": "Fixed Length Trimming"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPerform the simple NGS data alignment task against reference data.\n\n\n\n\n\n\nLearn about the SAM/BAM formats for further manipulation.\n\n\n\n\n\n\nBe able to sort and index BAM format for visualisation purposes.\n\n\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nBWA Burrows-Wheeler Algorithm:\n\n\nhttp://bio-bwa.sourceforge.net\n\n\nSamtools:\n\n\nhttp://picard.sourceforge.net/\n\n\nUseful Links\n\u00b6\n\n\nSAM Specification:\n\n\nhttp://samtools.sourceforge.net/SAM1.pdf\n\n\nExplain SAM Flags:\n\n\nhttps://broadinstitute.github.io/picard/explain-flags.html\n\n\nSources of Data\n\u00b6\n\n\nhttp://sra.dnanexus.com/studies/ERP001071\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n\nSonika Tyagi \nsonika.tyagi@agrf.org.au\n\nGayle Philip \ngkphilip@unimelb.edu.au\n\n\nContributor(s):\n\n\n\n\nIntroduction\n\u00b6\n\n\nThe goal of this hands-on session is to perform an NGS alignment on the\nsequencing data coming from a tumour and normal group of samples. We\nwill align raw sequencing data to the human genome using the BWA aligner\nand then we will discuss the sequence alignment and mapping format\n(SAM). SAM to BAM conversion, indexing and sorting will also be\ndemonstrated. These are important and essential steps for downstream\nprocessing of the aligned BAM files.\n\n\nThis data is the whole genome sequencing of a lung adenocarcinoma\npatient AK55. It was \ndownloaded\n from\n\nERP001071\n. Only the HiSeq2000 data for \nBlood\n and \nliverMets\n were analysed.\n\n\nAccession numbers associated with read data are assigned by the European\nBioinformatics Institute (EBI) and start with \u2019ER\u2019. e.g. ERP is the study ID\nand ERR is the run ID. The original FASTQ files downloaded had the ERR\nnumber in front of each read name in the FASTQ file (e.g. @ERRxx HWI-ST478_xxxx). The read name had\nto be edited to remove the ERR number at the start of the name. This had\ncaused problems for downstream programs such as Picard for marking\noptical duplicates.\n\n\nWe have used 4 Blood samples (8 paired-end (PE) \n*.fastq.gz\n files) and 5 Liver\nsamples (10 PE \n*.fastq.gz\n files) data from this study to perform the\nwhole genome alignment using the BWA aligner. The whole process took\n>150K CPU seconds per sample and the precomputed alignment will be used\nin different sections of this workshop.\n\n\n\n\nPrepare the Environment\n\u00b6\n\n\nBy now you know about the raw sequence FASTQ format generated by the\nIllumina sequencers. Next we will see how FASTQ files are aligned to\nthe reference genome and what the resulting standard alignment\nformat is. In the interest of time, we have selected only 1 million paired\nreads from a \nBlood\n sample to demonstrate a \nBWA\n command. The remaining\nalignments have already been performed for you and will be required in\nthe subsequent modules of the workshop.\n\n\nThe input data for this section can be found in the \nalignment\n\ndirectory on your desktop. Please follow the commands below to go to the\nright folder and view the top 10 lines of the input FASTQ file:\n\n\nOpen the Terminal.\n\n\nFirst, go to the right folder, where the data are stored.\n\n\n1\n2\n3\ncd /home/trainee/alignment\nls\nzless input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz\n\n\n\n\n\n\nPress \nq\n to stop the \nzless\n command.\n\n\n\n\nOverview of the Process\n\u00b6\n\n\n\n\nFigure 1:\n A flow diagram showing the steps that will be performed in this practical. \n\n\n\n\nAlignment\n\u00b6\n\n\nYou already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will use \nBWA\n, a widely used aligner based on the\nBurrows-Wheeler Algorithm. The alignment involves two steps:  \n\n\n1) Indexing the genome.\n\n2) Running the alignment command.  \n\n\nBWA is a software package for mapping low-divergent sequences against a large\nreference genome, such as the human genome. It consists of three\nalgorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is\ndesigned for Illumina sequence reads up to 100bp, while the other two are for\nlonger sequences ranging from 70bp to 1Mbp. BWA-MEM and BWA-SW share\nsimilar features such as long-read support and split alignment, but\nBWA-MEM, which is the latest, is generally recommended for high-quality\nqueries as it is faster and more accurate. BWA-MEM also has better\nperformance than BWA-backtrack for 70-100bp Illumina reads. For more details see\nthe \nBWA manual\n.\n\n\nBWA has a number of parameters in order to perform the alignment. To\nview them all, type\n\n\n1\nbwa <press enter>\n\n\n\n\n\n\n\nBWA uses an indexed genome for the alignment in order to keep its memory\nfootprint small. Indexing a genome is similar in concept to indexing a book.\nIf you want to know on which page a certain word appears or a chapter begins,\nit is much more efficient/faster to look it up in a pre-built index than going\nthrough every page of the book until you find it. Indices allow the aligner to narrow\ndown the potential origin of a query sequence within the genome, saving both time and memory.  \n\n\nDue to time constraints, we will \nNOT\n be running the\nindexing command. It is run only once for a version of a genome, and the\ncomplete command to index the human genome version hg19 is given below.\n\n\n\n\nSTOP\n\n\nYou \nDO NOT\n need to run this command. This has already been run for you.\n\n\nbwa index -p bwaIndex/human_g1k_v37.fasta -a bwtsw human_g1k_v37.fasta\n\n\n\n\nWe have used the following arguments for the indexing of the genome.\n\n\n\n\n-p\n:  Prefix of the output database [same as db filename].  \n\n\n-a\n:  Algorithm for constructing BWT index. This method works with the\n    whole human genome.\n\n\nRef genome filename\n: the last argument is the name of the reference genome\nfile in the fasta format.\n\n\n\n\nThis command will output 6 files that constitute the index. These files\nhave the prefix \nhuman_g1k_v37.fasta\n and are stored in the \nbwaIndex\n\nsubdirectory. To view the precomputed index files, type:\n\n\n1\nls -l bwaIndex\n\n\n\n\n\n\n\nNow that the genome is indexed we can move on to the actual alignment.  \n\n\nMake a directory to store the output from your aligner.\n\n\n1\nmkdir outputs\n\n\n\n\n\n\nThe first argument for \nbwa\n is the basename of the index for the genome\nto be searched. In our case this is \nhuman_g1k_v37.fasta\n.\n\n\nAlign the reads from the \nBlood\n sample using the following command:\n\n\n1\nbwa mem -M -t 4 -R '@RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA' bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz > outputs/SM_Blood_ID_ERR059356.subset.sam\n\n\n\n\n\n\nThe above command outputs the alignment in SAM format and stores them in\nthe file \nSM_Blood_ID_ERR059356.subset.sam\n in the subdirectory \noutputs\n.\n\n\nWe have used the following arguments for the alignment of the reads.\n\n\n\n\nmem\n: fast mode of high quality input such the Illumina\n\n\n-M\n: flags extra hits as secondary. This is needed for compatibility with\n  other tools downstream.\n\n\n-t\n: Number of threads.\n\n\n-R\n: Complete read group header line.\n\n\n\n\n\nThe \nSAM (Sequence Alignment/Map)\n\nformat is currently the de facto standard for storing large nucleotide\nsequence alignments. It is a TAB-delimited text format consisting of a header section, which is\noptional, and an alignment section. If present, the header must be prior\nto the alignments. Header lines start with \n@\n, while alignment lines do\nnot. Each alignment line has 11 mandatory fields with essential alignment\ninformation such as mapping position.\n\n\nNavigate into your \noutputs\n directory and look at the top 10 lines of the SAM file by typing:\n\n\n1\n2\ncd outputs\nhead -n 10 SM_Blood_ID_ERR059356.subset.sam\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nCan you distinguish between the header of the SAM format and the actual\nalignments?\n\n\nAnswer\nThe header line starts with the letter \u2018@\u2019 i.e.\n\n\n1\n2\n3\n@SQ SN:GL000192.1   LN:547496\n@RG SM:Blood    ID:ERR059356    LB:lb   PL:ILLUMINA\n@PG ID:bwa  PN:bwa  VN:0.7.15-r1140 CL:bwa mem -M -t 4 -R @RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz\n\n\n\nThe actual alignments start with read ID i.e.\n\n\n1\n2\nHWI-ST478_0133:3:1101:1374:2056#0   147 11\nHWI-ST478_0133:3:1101:1352:2070#0   163 14\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat kind of information does the header provide?\n\n\nAnswer\n@HD: Header line; VN: Format version; SO: the sort order of\n    alignments.\n@SQ: Reference sequence information; SN: reference sequence name;\n    LN: reference sequence length.\n@PG: Program; ID: Program record identifier; VN: Program\n    version; CL: the command line that produces the alignment.\n\n\n\n\n\n\nQuestion\n\n\nTo which chromosome are the reads mapped?\n\n\nAnswer\nAll chromosomes are represented (look at the 3\nrd\n field).\n\ngrep -v \u201c^@\u201d SM_Blood_ID_ERR059356.subset.sam  | cut -f3 | sort | uniq\n\n\n\n\n\n\nManipulating SAM output\n\u00b6\n\n\nSAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.\n\n\nConvert SAM to BAM using \nsamtools view\n and store the output in the\nfile \nSM_Blood_ID_ERR059356.subset.bam\n. You have to instruct \nsamtools view\n that the input\nis in SAM format (\n-S\n), the output should be in BAM format (\n-b\n) and\nthat you want the output to be stored in the file specified by the \n-o\n\noption:\n\n\n1\nsamtools view -bSo SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sam\n\n\n\n\n\n\nBAM files are not human-readable but can be viewed with the \nsamtools view\n command.\n\n\n\n\n\n\nAdvanced exercise\nCompute summary stats for the Flag values associated with the alignments\n using:\n \n1\nsamtools flagstat SM_Blood_ID_ERR059356.subset.bam\n\n\n\n\n\n\n\n\n\n\n\nPost Alignment Visualisation option\n\u00b6\n\n\nIGV is a stand-alone genome browser that can be used to visualise the\nBAM outputs. Please check their \nwebsite\n\nfor all the formats that IGV can display.\n\n\nWe will be using IGV later in the workshop for viewing a BAM file in the\ngenome browser. It requires the index of the BAM file to be in the same\nfolder as where the BAM file is. The index file should have the same\nname as the BAM file and the suffix \n.bai\n. Finally, to create the index\nof a BAM file you need to make sure that the file is sorted according to\nchromosomal coordinates.\n\n\nSort alignments according to chromosomal position and store the result\nin the file with the prefix \nSM_Blood_ID_ERR059356.subset.sorted\n:\n\n\n1\nsamtools sort SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sorted\n\n\n\n\n\n\n\nIndex the sorted file.\n\n\n1\nsamtools index SM_Blood_ID_ERR059356.subset.sorted.bam\n\n\n\n\n\n\nThe indexing will create a file called \nSM_Blood_ID_ERR059356.subset.sorted.bam.bai\n. Note that\nyou don\u2019t have to specify the name of the index file when running\n\nsamtools index\n, it simply appends a \n.bai\n suffix to the input BAM\nfile.\n\n\n\n\n\n\nQuestion\n\n\nHow can you quickly find out whether a BAM file is already coordinate\nsorted or not?\n\n\nAnswer\nUse \nsamtools view -h\n command to look at the SAM header.\nIt will have an SO field (e.g. SO:coordinate)",
            "title": "Read Alignment"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Perform the simple NGS data alignment task against reference data.    Learn about the SAM/BAM formats for further manipulation.    Be able to sort and index BAM format for visualisation purposes.",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#tools-used",
            "text": "BWA Burrows-Wheeler Algorithm:  http://bio-bwa.sourceforge.net  Samtools:  http://picard.sourceforge.net/",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#useful-links",
            "text": "SAM Specification:  http://samtools.sourceforge.net/SAM1.pdf  Explain SAM Flags:  https://broadinstitute.github.io/picard/explain-flags.html",
            "title": "Useful Links"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#sources-of-data",
            "text": "http://sra.dnanexus.com/studies/ERP001071",
            "title": "Sources of Data"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#author-information",
            "text": "Primary Author(s): \nSonika Tyagi  sonika.tyagi@agrf.org.au \nGayle Philip  gkphilip@unimelb.edu.au  Contributor(s):",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#introduction",
            "text": "The goal of this hands-on session is to perform an NGS alignment on the\nsequencing data coming from a tumour and normal group of samples. We\nwill align raw sequencing data to the human genome using the BWA aligner\nand then we will discuss the sequence alignment and mapping format\n(SAM). SAM to BAM conversion, indexing and sorting will also be\ndemonstrated. These are important and essential steps for downstream\nprocessing of the aligned BAM files.  This data is the whole genome sequencing of a lung adenocarcinoma\npatient AK55. It was  downloaded  from ERP001071 . Only the HiSeq2000 data for  Blood  and  liverMets  were analysed.  Accession numbers associated with read data are assigned by the European\nBioinformatics Institute (EBI) and start with \u2019ER\u2019. e.g. ERP is the study ID\nand ERR is the run ID. The original FASTQ files downloaded had the ERR\nnumber in front of each read name in the FASTQ file (e.g. @ERRxx HWI-ST478_xxxx). The read name had\nto be edited to remove the ERR number at the start of the name. This had\ncaused problems for downstream programs such as Picard for marking\noptical duplicates.  We have used 4 Blood samples (8 paired-end (PE)  *.fastq.gz  files) and 5 Liver\nsamples (10 PE  *.fastq.gz  files) data from this study to perform the\nwhole genome alignment using the BWA aligner. The whole process took\n>150K CPU seconds per sample and the precomputed alignment will be used\nin different sections of this workshop.",
            "title": "Introduction"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#prepare-the-environment",
            "text": "By now you know about the raw sequence FASTQ format generated by the\nIllumina sequencers. Next we will see how FASTQ files are aligned to\nthe reference genome and what the resulting standard alignment\nformat is. In the interest of time, we have selected only 1 million paired\nreads from a  Blood  sample to demonstrate a  BWA  command. The remaining\nalignments have already been performed for you and will be required in\nthe subsequent modules of the workshop.  The input data for this section can be found in the  alignment \ndirectory on your desktop. Please follow the commands below to go to the\nright folder and view the top 10 lines of the input FASTQ file:  Open the Terminal.  First, go to the right folder, where the data are stored.  1\n2\n3 cd /home/trainee/alignment\nls\nzless input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz   Press  q  to stop the  zless  command.",
            "title": "Prepare the Environment"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#overview-of-the-process",
            "text": "Figure 1:  A flow diagram showing the steps that will be performed in this practical.",
            "title": "Overview of the Process"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#alignment",
            "text": "You already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will use  BWA , a widely used aligner based on the\nBurrows-Wheeler Algorithm. The alignment involves two steps:    1) Indexing the genome. \n2) Running the alignment command.    BWA is a software package for mapping low-divergent sequences against a large\nreference genome, such as the human genome. It consists of three\nalgorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is\ndesigned for Illumina sequence reads up to 100bp, while the other two are for\nlonger sequences ranging from 70bp to 1Mbp. BWA-MEM and BWA-SW share\nsimilar features such as long-read support and split alignment, but\nBWA-MEM, which is the latest, is generally recommended for high-quality\nqueries as it is faster and more accurate. BWA-MEM also has better\nperformance than BWA-backtrack for 70-100bp Illumina reads. For more details see\nthe  BWA manual .  BWA has a number of parameters in order to perform the alignment. To\nview them all, type  1 bwa <press enter>   \nBWA uses an indexed genome for the alignment in order to keep its memory\nfootprint small. Indexing a genome is similar in concept to indexing a book.\nIf you want to know on which page a certain word appears or a chapter begins,\nit is much more efficient/faster to look it up in a pre-built index than going\nthrough every page of the book until you find it. Indices allow the aligner to narrow\ndown the potential origin of a query sequence within the genome, saving both time and memory.    Due to time constraints, we will  NOT  be running the\nindexing command. It is run only once for a version of a genome, and the\ncomplete command to index the human genome version hg19 is given below.   STOP  You  DO NOT  need to run this command. This has already been run for you.  bwa index -p bwaIndex/human_g1k_v37.fasta -a bwtsw human_g1k_v37.fasta   We have used the following arguments for the indexing of the genome.   -p :  Prefix of the output database [same as db filename].    -a :  Algorithm for constructing BWT index. This method works with the\n    whole human genome.  Ref genome filename : the last argument is the name of the reference genome\nfile in the fasta format.   This command will output 6 files that constitute the index. These files\nhave the prefix  human_g1k_v37.fasta  and are stored in the  bwaIndex \nsubdirectory. To view the precomputed index files, type:  1 ls -l bwaIndex   \nNow that the genome is indexed we can move on to the actual alignment.    Make a directory to store the output from your aligner.  1 mkdir outputs   The first argument for  bwa  is the basename of the index for the genome\nto be searched. In our case this is  human_g1k_v37.fasta .  Align the reads from the  Blood  sample using the following command:  1 bwa mem -M -t 4 -R '@RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA' bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz > outputs/SM_Blood_ID_ERR059356.subset.sam   The above command outputs the alignment in SAM format and stores them in\nthe file  SM_Blood_ID_ERR059356.subset.sam  in the subdirectory  outputs .  We have used the following arguments for the alignment of the reads.   mem : fast mode of high quality input such the Illumina  -M : flags extra hits as secondary. This is needed for compatibility with\n  other tools downstream.  -t : Number of threads.  -R : Complete read group header line.   \nThe  SAM (Sequence Alignment/Map) \nformat is currently the de facto standard for storing large nucleotide\nsequence alignments. It is a TAB-delimited text format consisting of a header section, which is\noptional, and an alignment section. If present, the header must be prior\nto the alignments. Header lines start with  @ , while alignment lines do\nnot. Each alignment line has 11 mandatory fields with essential alignment\ninformation such as mapping position.  Navigate into your  outputs  directory and look at the top 10 lines of the SAM file by typing:  1\n2 cd outputs\nhead -n 10 SM_Blood_ID_ERR059356.subset.sam     Question  Can you distinguish between the header of the SAM format and the actual\nalignments?  Answer The header line starts with the letter \u2018@\u2019 i.e.  1\n2\n3 @SQ SN:GL000192.1   LN:547496\n@RG SM:Blood    ID:ERR059356    LB:lb   PL:ILLUMINA\n@PG ID:bwa  PN:bwa  VN:0.7.15-r1140 CL:bwa mem -M -t 4 -R @RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz  The actual alignments start with read ID i.e.  1\n2 HWI-ST478_0133:3:1101:1374:2056#0   147 11\nHWI-ST478_0133:3:1101:1352:2070#0   163 14     Question  What kind of information does the header provide?  Answer @HD: Header line; VN: Format version; SO: the sort order of\n    alignments. @SQ: Reference sequence information; SN: reference sequence name;\n    LN: reference sequence length. @PG: Program; ID: Program record identifier; VN: Program\n    version; CL: the command line that produces the alignment.    Question  To which chromosome are the reads mapped?  Answer All chromosomes are represented (look at the 3 rd  field). \ngrep -v \u201c^@\u201d SM_Blood_ID_ERR059356.subset.sam  | cut -f3 | sort | uniq",
            "title": "Alignment"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#manipulating-sam-output",
            "text": "SAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.  Convert SAM to BAM using  samtools view  and store the output in the\nfile  SM_Blood_ID_ERR059356.subset.bam . You have to instruct  samtools view  that the input\nis in SAM format ( -S ), the output should be in BAM format ( -b ) and\nthat you want the output to be stored in the file specified by the  -o \noption:  1 samtools view -bSo SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sam   BAM files are not human-readable but can be viewed with the  samtools view  command.    Advanced exercise Compute summary stats for the Flag values associated with the alignments\n using:\n  1 samtools flagstat SM_Blood_ID_ERR059356.subset.bam",
            "title": "Manipulating SAM output"
        },
        {
            "location": "/modules/cancer-module-alignment/alignment/#post-alignment-visualisation-option",
            "text": "IGV is a stand-alone genome browser that can be used to visualise the\nBAM outputs. Please check their  website \nfor all the formats that IGV can display.  We will be using IGV later in the workshop for viewing a BAM file in the\ngenome browser. It requires the index of the BAM file to be in the same\nfolder as where the BAM file is. The index file should have the same\nname as the BAM file and the suffix  .bai . Finally, to create the index\nof a BAM file you need to make sure that the file is sorted according to\nchromosomal coordinates.  Sort alignments according to chromosomal position and store the result\nin the file with the prefix  SM_Blood_ID_ERR059356.subset.sorted :  1 samtools sort SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sorted   \nIndex the sorted file.  1 samtools index SM_Blood_ID_ERR059356.subset.sorted.bam   The indexing will create a file called  SM_Blood_ID_ERR059356.subset.sorted.bam.bai . Note that\nyou don\u2019t have to specify the name of the index file when running samtools index , it simply appends a  .bai  suffix to the input BAM\nfile.    Question  How can you quickly find out whether a BAM file is already coordinate\nsorted or not?  Answer Use  samtools view -h  command to look at the SAM header.\nIt will have an SO field (e.g. SO:coordinate)",
            "title": "Post Alignment Visualisation option"
        },
        {
            "location": "/modules/cancer-module-snv/snv/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPrepare raw BAM alignments for variant detection\n\n\n\n\n\n\nPerform QC measures on BAM files\n\n\n\n\n\n\nPerform simple variant detection on paired NGS data\n\n\n\n\n\n\nAdd annotation information to raw variant calls\n\n\n\n\n\n\nVisualise variant calls using IGV\n\n\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nSAMTools:\n\n\nhttps://samtools.github.io/\n\n\nIGV:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nGenome Analysis Toolkit:\n\n\nhttp://www.broadinstitute.org/gatk/\n\n\nPicard:\n\n\nhttps://broadinstitute.github.io/picard/\n\n\nMuTect:\n\n\nhttp://www.broadinstitute.org/cancer/cga/mutect/\n\n\nStrelka:\n\n\nhttps://sites.google.com/site/strelkasomaticvariantcaller/\n\n\nVarScan2:\n\n\nhttps://dkoboldt.github.io/varscan/\n\n\nVariant Effect Predictor:\n\n\nhttp://www.ensembl.org/info/docs/tools/vep\n\n\nGEMINI:\n\n\nhttp://gemini.readthedocs.org\n\n\nSources of Data\n\u00b6\n\n\nhttp://sra.dnanexus.com/studies/ERP001071\n\n\nhttp://www.ncbi.nlm.nih.gov/pubmed/22194472\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n\nMatt Field \nmatt.field@anu.edu.au\n\nDan Andrews \ndan.andrews@anu.edu.au\n\nVelimir Gayevskiy \nv.gayevskiy@garvan.org.au\n\nMathieu Bourgey \nmathieu.bourgey@mcgill.ca\n  \n\n\nContributor(s):\n\nGayle Philip \nSonika.Tyagi@agrf.org.au\n\nSonika Tyagi \ngkphilip@unimelb.edu.au\n  \n\n\n\n\nIntroduction\n\u00b6\n\n\nThe goal of this hands-on session is to present the main steps that are\ncommonly used to process and to analyze cancer sequencing data. We will\nfocus only on whole genome data and provide command lines that allow\ndetecting Single Nucleotide Variants (SNV). This workshop will show you\nhow to launch individual steps of a complete DNA-Seq SNV pipeline using\ncancer data.\n\n\nIn the second part of the tutorial we will also be using IGV to\nvisualise and manually inspect candidate variant calls.\n\n\n\n\nPrepare the Environment\n\u00b6\n\n\nWe will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.\n\n\nThe data consists of whole genome sequencing of liver metastatic lung\ncancer (frozen), primary lung cancer (FFPE) and blood tissue of a lung\nadenocarcinoma patient (AK55).\n\n\nOpen the Terminal and go to the \nsnv\n working directory:\n\n\n1\ncd /home/trainee/snv/\n\n\n\n\n\n\n\n\nAll commands entered into the terminal for this tutorial should be from\nwithin the \nsnv\n directory.\n\n\n\n\nThe BAM alignment files are contained in the subdirectory called\n\nalignment\n and are located in the following subdirectories:\n\n\n\n\nnormal/normal.sorted.bam\n and \nnormal/normal.sorted.bam.bai\n\n\ntumour/tumor.sorted.bam\n and \ntumour/tumour.sorted.bam.bai\n\n\n\n\nCheck that the \nalignment\n directory contains the above-mentioned files by typing:\n\n\n1\nls -l alignment/*\n\n\n\n\n\n\n\nThese files are based on subsetting the whole genomes derived from blood\nand liver metastases to the first 10Mb of chromosome 4. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that this is less < 0.5% of the genome which\nhighlights the length of time and resources required to perform cancer\ngenomics on full genomes!\n\n\nThe initial structure of your folders should look like this (type \nls -l\n):\n\n\n1\n2\n3\n4\n-- alignment/             # bam files\n  -- normal/                # The blood sample directory containing bam files\n  -- tumour/                # The tumour sample directory containing bam files\n-- ref/                   # Contains reference genome files      \n\n\n\n\n\n\n\nNow we need to set some environment variables to save typing lengthy\nfile paths over and over. Copy and paste the following commands into\nyour terminal.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nexport\n \nAPP_ROOT\n=\n/home/trainee/snv/Applications\n\nexport\n \nIGVTOOLS_PATH\n=\n$APP_ROOT\n/igvtools/\n\nexport\n \nPICARD_JAR\n=\n$APP_ROOT\n/picard/picard.jar\n\nexport\n \nGATK_JAR\n=\n$APP_ROOT\n/gatk/GenomeAnalysisTK.jar\n\nexport\n \nSTRELKA_HOME\n=\n$APP_ROOT\n/strelka/\n\nexport\n \nMUTECT_JAR\n=\n$APP_ROOT\n/mutect/muTect-1.1.5.jar\n\nexport\n \nVARSCAN_JAR\n=\n$APP_ROOT\n/varscan/VarScan.v2.4.1.jar\n\nexport\n \nREF\n=\n/home/trainee/snv/ref\n\nexport\n \nSNV_BASE\n=\n/home/trainee/snv\n\nexport\n \nJAVA7\n=\n/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n\nexport\n \nIGV\n=\n$APP_ROOT\n/igv/igv.sh\n\nexport\n \nVEP\n=\n$APP_ROOT\n/ensembl-tools/scripts/variant_effect_predictor/variant_effect_predictor.pl\n\nexport\n \nVEP_CACHE\n=\n/mnt/workshop/data/bgdata/datasets/vepcache/82\n\n\n\n\n\n\n\nMake sure you are in the correct directory by typing:\n\n\n1\ncd $SNV_BASE\n\n\n\n\n\n\n\n\nBAM Files\n\u00b6\n\n\nLet\u2019s spend some time exploring BAM files.\n\n\nExploring BAM files\n\u00b6\n\n\n1\nsamtools view alignment/normal/normal.sorted.bam | head -n4\n\n\n\n\n\n\nHere you have examples of alignment results. A full description of the\nflags can be found in the \nSAM specification\n.\n\n\nAnother useful bit of information in the SAM is the CIGAR string. It\u2019s\nthe 6\nth\n column in the file.\n\n\nThis column explains how the alignment was achieved.\n\n\n\n\nM == base aligns \nbut doesn\u2019t have to be a match.\n A SNP will have an M even if it disagrees with the reference.\n\nI == Insertion\n\nD == Deletion\n\nS == soft-clips. These are handy to find un removed adapters, viral insertions, etc.\n\n\n\n\nAn in-depth explanation of the CIGAR can be found \nhere\n.\nThe exact details of the CIGAR string can be found in the SAM specification as\nwell. We won\u2019t go into too much detail at this point since we want to concentrate on\ncancer specific issues now.\n\n\nNow, you can try using Picard\u2019s \nexplain flag\n\nsite to understand what is going on with your reads.\n\n\n\n\nQuestion\n\n\nThere are 3 unique flags, what do they mean? The flag is the second column.\n\n\nAnswer\n129:\n\nread paired\n\nsecond in pair  \n113:\n\nread paired\n\nread reverse strand\n\nmate reverse strand\n\nfirst in pair  \n161:\n\nread paired\n\nmate reverse strand\n\nsecond in pair  \n\n\n\n\n\nThere are lots of possible different flags, let\u2019s look at a few more\n\n\n1\nsamtools view alignment/normal/normal.sorted.bam | head -n 100\n\n\n\n\n\n\n\n\nQuestion\n\n\nLet\u2019s take the last read, which looks properly paired and find its mate pair.\n\n\nHint\na)    Instead of using \nhead\n, what unix command could we pipe the output to?\n\nb)    Once we\u2019ve found both reads, the command can be stopped by typing \nCTRL-C\n  \n\n\nAnswer\n1\nsamtools view alignment/normal/normal.sorted.bam | grep HWI-ST478_0133:4:2205:14675:32513\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nUsing the cigar string, what can we tell about the alignment of the mate pair?\n\n\nAnswer\nThe mate pair has a less convincing alignment with two insertions and\nsoft clipping reported.\n\n\n\n\n\n\nQuestion\n\n\nHow might the alignment information from the original read be used by the aligner?\n\n\nAnswer\nEven though the alignment of the mate pair is questionable the presence\n  of it\u2019s properly paired mate helps the aligner in deciding where to put\n  the less-certain read.\n\n\n\n\nYou can use \nSamtools\n to filter reads as well.\n\n\n\n\nQuestion\n\n\nHow many reads mapped and unmapped were there?\n\n\nHint\nLook at the samtools view help menu by typing \nsamtools view\n without any arguments\n\n\nAnswer\n1\nsamtools view -c -f4 alignment/normal/normal.sorted.bam\n\n\n\n\n77229\n\n1\nsamtools view -c -F4 alignment/normal/normal.sorted.bam\n\n\n\n\n22972373\n\n\n\n\nStep 1: Pre-processing: Indel Realignment\n\u00b6\n\n\nThe first step for this is to realign around indels and SNP dense regions.\nThe Genome Analysis toolkit (\nGATK\n) has a tool for this called IndelRealigner.\nIt basically runs in 2 steps:  \n\n\n\n\nFind the targets\n\n\nRealign them\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n$JAVA7\n \n-\nXmx2G\n  \n-\njar\n \n$\n{\nGATK_JAR\n}\n \n\\\n\n  \n-\nT\n \nRealignerTargetCreator\n \n\\\n\n  \n-\nR\n \n$\n{\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \n\\\n\n  \n-\no\n \nalignment\n/\nnormal\n/\nrealign\n.\nintervals\n \n\\\n\n  \n-\nI\n \nalignment\n/\nnormal\n/\nnormal\n.\nsorted\n.\nbam\n \n\\\n\n  \n-\nI\n \nalignment\n/\ntumour\n/\ntumour\n.\nsorted\n.\nbam\n \n\\\n\n  \n-\nL\n \n$\n{\nREF\n}/\nhuman_g1k_v37\n.\nintervals\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n$JAVA7\n \n-\nXmx2G\n \n-\njar\n \n$\n{\nGATK_JAR\n}\n \n\\\n\n  \n-\nT\n \nIndelRealigner\n \n\\\n\n  \n-\nR\n \n$\n{\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \n\\\n\n  \n-\ntargetIntervals\n \nalignment\n/\nnormal\n/\nrealign\n.\nintervals\n \n\\\n\n  \n--\nnWayOut\n \n.\nrealigned\n.\nbam\n \n\\\n\n  \n-\nI\n \nalignment\n/\nnormal\n/\nnormal\n.\nsorted\n.\nbam\n \n\\\n\n  \n-\nI\n \nalignment\n/\ntumour\n/\ntumour\n.\nsorted\n.\nbam\n \n\\\n\n  \n-\nL\n \n$\n{\nREF\n}/\nhuman_g1k_v37\n.\nintervals\n\n\n\n\n\n\n\nExplanation of parameters:\n\n\n\n\n-I\n: BAM file(s)\n\n\n-T\n: GATK algorithm to run\n\n\n-R\n: the reference genome used for mapping (b37 from GATK here)\n\n\n-jar\n: Path to GATK jar file\n\n\n-L\n: Genomic intervals to operate on\n\n\n\n\n\nMove the realigned BAMs and index files to the corresponding normal and\ntumour directories.\n\n\n1\n2\nmv normal.sorted.realigned.ba* alignment/normal/\nmv tumour.sorted.realigned.ba* alignment/tumour/\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhy did we use both normal and tumor together?\n\n\nAnswer\nBecause if a region needs realignment, maybe one of the samples in the\npair has less reads or was excluded from the target creation.\nThis makes sure the normal and tumor are all in-sync for the somatic\ncalling step.\n\n\n\n\n\n\nQuestion\n\n\nHow many regions did it think needed cleaning?\n\n\nAnswer\n1\nwc -l alignment/normal/realign.intervals\n\n\n\n\n\n27300\n\n\n\n\n\nIndel Realigner also makes sure the called deletions are left aligned\nwhen there is a microsatellite or homopolymer. e.g.\n\n\nThis\n\n  ATCGAAAA-TCG\n\n  into\n\n  ATCG-AAAATCG  \n\n\nor  \n\n\nATCGATATATATA\u2013TCG\n\n  into\n\n  ATCG\u2013ATATATATATCG  \n\n\n\n\nQuestion\n\n\nWhy is it important?\n\n\nAnswer\nThis makes it easier for downstream analysis tools.\nFor NGS analysis, the convention is to left align indels.\nThis is only really needed when calling variants with legacy locus-based\ntools such as samtools or GATK UnifiedGenotyper. Otherwise you will have\nworse performance and accuracy.\nWith more sophisticated tools (like GATK HaplotypeCaller) that involve\nreconstructing haplotypes (e.g. through reassembly), the problem of\nmultiple valid representations is handled internally and does not need\nto be corrected explicitly.\n\n\n\n\nStep 2: Pre-processing: Fixmates\n\u00b6\n\n\nSome read entries don\u2019t have their mate information written properly.\n\nWe use \nPicard\n to do this:\n\n\nNormal sample:\n  \n1\n2\n3\n4\n5\n6\n7\n$JAVA7\n \n-\nXmx2G\n \n-\njar\n \n$\n{\nPICARD_JAR\n}\n \nFixMateInformation\n \n\\\n\n  \nVALIDATION_STRINGENCY\n=\nSILENT\n \n\\\n\n  \nCREATE_INDEX\n=\ntrue\n \n\\\n\n  \nSORT_ORDER\n=\ncoordinate\n \n\\\n\n  \nMAX_RECORDS_IN_RAM\n=\n500000\n \n\\\n\n  \nINPUT\n=\nalignment\n/\nnormal\n/\nnormal\n.\nsorted\n.\nrealigned\n.\nbam\n \n\\\n\n  \nOUTPUT\n=\nalignment\n/\nnormal\n/\nnormal\n.\nmatefixed\n.\nbam\n\n\n\n\n\n\nTumour sample: \n\n  \n1\n2\n3\n4\n5\n6\n7\n$JAVA7\n \n-\nXmx2G\n \n-\njar\n \n$\n{\nPICARD_JAR\n}\n \nFixMateInformation\n \n\\\n\n  \nVALIDATION_STRINGENCY\n=\nSILENT\n \n\\\n\n  \nCREATE_INDEX\n=\ntrue\n \n\\\n\n  \nSORT_ORDER\n=\ncoordinate\n \n\\\n\n  \nMAX_RECORDS_IN_RAM\n=\n500000\n \n\\\n\n  \nINPUT\n=\nalignment\n/\ntumour\n/\ntumour\n.\nsorted\n.\nrealigned\n.\nbam\n \n\\\n\n  \nOUTPUT\n=\nalignment\n/\ntumour\n/\ntumour\n.\nmatefixed\n.\nbam\n\n\n\n\n\n\nStep 3: Pre-processing: Mark Duplicates\n\u00b6\n\n\n\n\nQuestion\n\n\nWhat are duplicate reads?\n\n\nAnswer\nDifferent read pairs representing the same initial DNA fragment.\n\n\n\n\n\n\nQuestion\n\n\nWhat are they caused by?\n\n\nAnswer\nPCR reactions (PCR duplicates).  \nSome clusters that are thought of being separate in the flowcell but are\nthe same (optical duplicates)\n\n\n\n\n\n\nQuestion\n\n\nWhat are the ways to detect them?\n\n\nAnswer\nPicard and samtools uses the alignment positions:  \nBoth 5\u2019 ends of both reads need to have the same positions.\nEach reads have to be on the same strand as well.\nAnother method is to use a kmer approach:\nTake a part of both ends of the fragment.\nBuild a hash table.\nCount the similar hits.\nBrute force, compare all the sequences.\n\n\n\n\n\nHere we will use \nPicard\n\u2018s approach:\n\n\nNormal Sample:\n  \n1\n2\n3\n4\n5\n6\n7\n8\n$JAVA7\n \n-\nXmx2G\n \n-\njar\n \n$\n{\nPICARD_JAR\n}\n \nMarkDuplicates\n \n\\\n\n  \nREMOVE_DUPLICATES\n=\nfalse\n \n\\\n\n  \nCREATE_MD5_FILE\n=\ntrue\n \n\\\n\n  \nVALIDATION_STRINGENCY\n=\nSILENT\n \n\\\n\n  \nCREATE_INDEX\n=\ntrue\n \n\\\n\n  \nINPUT\n=\nalignment\n/\nnormal\n/\nnormal\n.\nmatefixed\n.\nbam\n \n\\\n\n  \nOUTPUT\n=\nalignment\n/\nnormal\n/\nnormal\n.\nsorted\n.\ndup\n.\nbam\n \n\\\n\n  \nMETRICS_FILE\n=\nalignment\n/\nnormal\n/\nnormal\n.\nsorted\n.\ndup\n.\nmetrics\n\n\n\n\n\n\nTumour Sample:\n\n  \n1\n2\n3\n4\n5\n6\n7\n8\n$JAVA7\n \n-\nXmx2G\n \n-\njar\n \n$\n{\nPICARD_JAR\n}\n \nMarkDuplicates\n \n\\\n\n  \nREMOVE_DUPLICATES\n=\nfalse\n \n\\\n\n  \nCREATE_MD5_FILE\n=\ntrue\n \n\\\n\n  \nVALIDATION_STRINGENCY\n=\nSILENT\n \n\\\n\n  \nCREATE_INDEX\n=\ntrue\n \n\\\n\n  \nINPUT\n=\nalignment\n/\ntumour\n/\ntumour\n.\nmatefixed\n.\nbam\n \n\\\n\n  \nOUTPUT\n=\nalignment\n/\ntumour\n/\ntumour\n.\nsorted\n.\ndup\n.\nbam\n \n\\\n\n  \nMETRICS_FILE\n=\nalignment\n/\ntumour\n/\ntumour\n.\nsorted\n.\ndup\n.\nmetrics\n\n\n\n\n\n\n\nWe can look in the metrics output to see what happened.\n\n\n1\nless alignment/normal/normal.sorted.dup.metrics\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat percent of reads are duplicates?\n\n\nAnswer\n0.046996%\n\n\n\n\n\n\nQuestion\n\n\nOften, we have multiple libraries and when this occurs separate measures\nare calculated for each library. Why is it important to not combine everything?\n\n\nAnswer\nEach library represents a set of different DNA fragments.\nEach library involves different PCR reactions\nPCR duplicates can not occur between fragments of two different\nlibraries. However, similar fragments could be found between\nlibraries when the coverage is high.\n\n\n\n\nStep 4: Pre-processing: Base Quality Recalibration\n\u00b6\n\n\n\n\nQuestion\n\n\nWhy do we need to recalibrate base quality scores?\n\n\nAnswer\nThe vendors tend to inflate the values of the bases in the reads. The\nrecalibration tries to lower the scores of some biased motifs for some\ntechnologies.\n\n\n\n\n\nBase Quality Recalibration runs in 2 steps:\n\n\n\n\nBuild covariates based on context and known SNP sites.\n\n\nCorrect the reads based on these metrics.\n\n\n\n\nGATK BaseRecalibrator:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nGATK_JAR\n} \\\n    -\nT\n \nBaseRecalibrator\n \\\n    -\nnct\n \n2\n \\\n    -\nR\n ${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    -\nknownSites\n ${\nREF\n}/\ndbSnp-138_chr4\n.\nvcf\n \\\n    -\nL\n \n4\n:\n1\n-\n10000000\n \\\n    -\no\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecalibration_report\n.\ngrp\n \\\n    -\nI\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nbam\n\n\n    \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nGATK_JAR\n} \\\n      -\nT\n \nPrintReads\n \\\n      -\nnct\n \n2\n \\\n      -\nR\n ${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n      -\nBQSR\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecalibration_report\n.\ngrp\n \\\n      -\no\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n      -\nI\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nbam\n\n\ndone\n\n\n\n\n\n\n\n\n\nBAM QC\n\u00b6\n\n\nOnce your whole BAM is generated, it\u2019s always a good thing to check the\ndata again to see if everything makes sense.\n\n\nStep 1: BAM QC: Compute Coverage\n\u00b6\n\n\nIf you have data from a capture kit, you should see how well your\ntargets worked. \nGATK\n has a depth of coverage tool to do this:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n  -\nXmx2G\n -\njar\n ${\nGATK_JAR\n} \\\n    -\nT\n \nDepthOfCoverage\n \\\n    --\nomitDepthOutputAtEachBase\n \\\n    --\nsummaryCoverageThreshold\n \n10\n \\\n    --\nsummaryCoverageThreshold\n \n25\n \\\n    --\nsummaryCoverageThreshold\n \n50\n \\\n    --\nsummaryCoverageThreshold\n \n100\n \\\n    --\nstart\n \n1\n --\nstop\n \n500\n --\nnBins\n \n499\n -\ndt\n \nNONE\n \\\n    -\nR\n ${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    -\no\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\ncoverage\n \\\n    -\nI\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    -\nL\n \n4\n:\n1\n-\n10000000\n\n\ndone\n\n\n\n\n\n\n\nExplanation of parameters:\n\n\n\n\n--omitBaseOutputAtEachBase\n: Do not output depth of coverage at each base\n\n\n--summaryCoverageThreshold\n: Coverage threshold (in percent) for summarizing statistics\n\n\n-dt\n: Downsampling\n\n\n-L\n: Genomic intervals to operate on  \n\n\n\n\n\nIn this project, coverage is expected to be 25x. Look at the coverage:\n\n\n1\nless -S alignment/normal/normal.sorted.dup.recal.coverage.sample_interval_summary\n\n\n\n\n\n\nType \nq\n to return to the prompt.\n\n\n1\nless -S alignment/tumour/tumour.sorted.dup.recal.coverage.sample_interval_summary\n\n\n\n\n\n\n\n\nQuestion\n\n\nDoes the coverage fit with the expectation?\n\n\nAnswer\nYes the mean coverage of the region is 25x.\nsummaryCoverageThreshold\n is a useful function to see if your coverage is\nuniform.\nAnother way is to compare the mean to the median. If both are quite\ndifferent that means something is wrong in your coverage.\n\n\n\n\nStep 2: BAM QC: Insert Size\n\u00b6\n\n\nInsert size corresponds to the size of DNA fragments sequenced. It is different\nfrom the gap size (= distance between reads)!\n\n\n\nThese metrics are computed using \nPicard\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nPICARD_JAR\n} \nCollectInsertSizeMetrics\n \\\n    \nVALIDATION_STRINGENCY\n=\nSILENT\n \\\n    \nREFERENCE_SEQUENCE\n=${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    \nINPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    \nOUTPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nmetric\n.\ninsertSize\n.\ntsv\n \\\n    \nHISTOGRAM_FILE\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nmetric\n.\ninsertSize\n.\nhisto\n.\npdf\n \\\n    \nMETRIC_ACCUMULATION_LEVEL\n=\nLIBRARY\n\n\ndone\n\n\n\n\n\n\n\n\nLook at the output:\n\n\n1\n2\nless -S alignment/normal/normal.sorted.dup.recal.metric.insertSize.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.insertSize.tsv\n\n\n\n\n\n\n\n\nQuestion\n\n\nHow do the two libraries compare?    \n\n\nAnswer\nThe tumour sample has a larger median insert size than the\nnormal sample (405 vs. 329).\n\n\n\n\nStep 3: BAM QC: Alignment metrics\n\u00b6\n\n\nAlignment metrics tells you if your sample and your reference fit together.\n\n\nFor the alignment metrics, \nsamtools flagstat\n is very fast but\n\nbwa-mem\n breaks some reads into pieces, the numbers can be a bit confusing.\n\n\nInstead, we will use \nPicard\n to compute the metrics:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nPICARD_JAR\n} \nCollectAlignmentSummaryMetrics\n \\\n    \nVALIDATION_STRINGENCY\n=\nSILENT\n \\\n    \nREFERENCE_SEQUENCE\n=${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    \nINPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    \nOUTPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nmetric\n.\nalignment\n.\ntsv\n \\\n    \nMETRIC_ACCUMULATION_LEVEL\n=\nLIBRARY\n\n\ndone\n\n\n\n\n\n\n\n\nExplore the results\n\n\n1\n2\nless -S alignment/normal/normal.sorted.dup.recal.metric.alignment.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.alignment.tsv\n\n\n\n\n\n\n\n\nQuestion\n\n\nDo you think the sample and the reference genome fit together?\n\n\nAnswer\nYes, 99% of the reads have been aligned.\n\nUsually, we consider:\nA good alignment if > 85%\nReference assembly issues if [60-85]%\nProbably a mismatch between sample and ref if < 60 %\n\n\n\n\n\n\nVariant Calling\n\u00b6\n\n\nMost of SNV caller use either a Bayesian, a threshold or a t-test\napproach to do the calling\n\n\nHere we will try 3 variant callers:\n\n\n\n\nVarscan 2\n\n\nMuTecT\n\n\nStrelka\n\n\n\n\nOther candidates:\n\n\n\n\nVirmid\n\n\nSomatic sniper\n\n\n\n\nMany, MANY others can be found here: \nhttps://www.biostars.org/p/19104/\n\n\nIn our case, let\u2019s create a new work directory to start with (from base\ndirectory):\n\n\n1\n2\ncd $SNV_BASE\nmkdir variant_calling\n\n\n\n\n\n\nVarscan 2\n\u00b6\n\n\nVarscan 2\n calls somatic variants (SNPs and indels) using a\nheuristic method and a statistical test based on the number of aligned\nreads supporting each allele. It expects both a normal and a tumour file in\n\nSAMtools pileup\n format from sequence alignments in binary alignment/map\n(BAM) format. To build a pileup file, you will need:\n\n\n\n\nA SAM/BAM file (\n*.sorted.dup.recal.bam\n) that has been sorted using the \nsort\n command\nof \nsamtools\n.\n\n\nThe reference sequence (\nhuman_g1k_v37.fasta\n) to which reads were aligned, in\nFASTA format.\n\n\nThe \nsamtools\n software package.\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nfor\n i in normal tumour\n\ndo\n\nsamtools mpileup -L \n1000\n -B -q \n1\n \n\\\n\n  -f \n${\nREF\n}\n/human_g1k_v37.fasta \n\\\n\n  -r \n4\n:1-10000000 \n\\\n\n  alignment/\n${\ni\n}\n/\n${\ni\n}\n.sorted.dup.recal.bam \n\\\n\n  > variant_calling/\n${\ni\n}\n.mpileup\n\ndone\n\n\n\n\n\n\n\nNotes on \nsamtools\n arguments:\n\n\n\n\n-L\n: max per-sample depth for INDEL calling [1000]\n\n\n-B\n: disable BAQ (per-Base Alignment Quality)\n\n\n-q\n: skip alignments with mapQ smaller than 1\n\n\n-g\n: generate genotype likelihoods in BCF format\n\n\n\n\n\n  \n1\n2\n3\n4\n5\n6\n7\n$JAVA7\n \n-\nXmx2G\n \n-\njar\n \n$\n{\nVARSCAN_JAR\n}\n \n\\\n\n\nsomatic\n \nvariant_calling\n/\nnormal\n.\nmpileup\n \n\\\n\n\nvariant_calling\n/\ntumour\n.\nmpileup\n \n\\\n\n\nvariant_calling\n/\nvarscan\n \n\\\n\n\n--\noutput\n-\nvcf\n \n1\n \n\\\n\n\n--\nstrand\n-\nfilter\n \n1\n \n\\\n\n\n--\nsomatic\n-\np\n-\nvalue\n \n0.001\n\n\n\n\n\n\nMuTect\n\u00b6\n\n\nNow let\u2019s try a different variant caller, \nMuTect\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n$JAVA7\n \n-\nXmx2G\n \n-\njar\n \n$\n{\nMUTECT_JAR\n}\n \n\\\n\n  \n-\nT\n \nMuTect\n \n\\\n\n  \n-\nR\n \n$\n{\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \n\\\n\n  \n-\ndt\n \nNONE\n \n-\nbaq\n \nOFF\n \n--\nvalidation_strictness\n \nLENIENT\n \n\\\n\n  \n--\ndbsnp\n \n$\n{\nREF\n}/\ndbSnp\n-\n138_\nchr4\n.\nvcf\n \n\\\n\n  \n--\ninput_file\n:\nnormal\n \nalignment\n/\nnormal\n/\nnormal\n.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \n\\\n\n  \n--\ninput_file\n:\ntumor\n \nalignment\n/\ntumour\n/\ntumour\n.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \n\\\n\n  \n--\nout\n \nvariant_calling\n/\nmutect\n.\ncall_stats\n.\ntxt\n \n\\\n\n  \n--\ncoverage_file\n \nvariant_calling\n/\nmutect\n.\nwig\n.\ntxt\n \n\\\n\n  \n-\npow\n \nvariant_calling\n/\nmutect\n.\npower\n \n\\\n\n  \n-\nvcf\n \nvariant_calling\n/\nmutect\n.\nvcf\n \n\\\n\n  \n-\nL\n \n4\n:\n1\n-\n10000000\n\n\n\n\n\n\n\nStrelka\n\u00b6\n\n\nAnd finally let\u2019s try Illumina\u2019s \nStrelka\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\ncp \n${\nSTRELKA_HOME\n}\n/etc/strelka_config_bwa_default.ini .\n\nsed 's/isSkipDepthFilters =.*/isSkipDepthFilters = 1/g' -i strelka_config_bwa_default.ini\n\n\n${\nSTRELKA_HOME\n}\n/bin/configureStrelkaWorkflow.pl \\\n  --normal=alignment/normal/normal.sorted.dup.recal.bam \\\n  --tumor=alignment/tumour/tumour.sorted.dup.recal.bam \\\n  --ref=\n${\nREF\n}\n/human_g1k_v37.fasta \\\n  --config=\n${\nSNV_BASE\n}\n/strelka_config_bwa_default.ini \\\n  --output-dir=variant_calling/strelka/\n\ncd variant_calling/strelka/\nmake -j2\ncd ../..\n\ncp variant_calling/strelka/results/passed.somatic.snvs.vcf variant_calling/strelka.vcf\n\n\n\n\n\n\nComparing variant callers\n\u00b6\n\n\nNow we have variants from all three methods. Let\u2019s compress and index\nthe VCFs for future visualisation.\n\n\n1\nfor\n i in variant_calling/*.vcf\n;\n \ndo\n bgzip -c \n$i\n > \n$i\n.gz \n;\n tabix -p vcf \n$i\n.gz\n;\n \ndone\n\n\n\n\n\n\n\nLet\u2019s look at a compressed VCF. Details on the VCF spec can be found \nhere\n.\n\n\n1\nzless -S variant_calling/varscan.snp.vcf.gz\n\n\n\n\n\n\n\nFields vary from caller to caller. Some values are are almost always there:\n\n\n\n\nRef vs. alt alleles\n\n\nVariant quality (QUAL column)\n\n\nThe per-sample genotype (GT) values.\n\n\n\n\nNote on VCF fields:\n\n\n\n\nDP\n: Raw read depth\n\n\nGT\n: Genotype\n\n\nPL\n: List of Phred-scaled genotype likelihoods. (min is better)\n\n\nDP\n: \u201c\u201d# high-quality bases\u201d\n\n\nSP\n: Phred-scaled strand bias P-value\n\n\nGQ\n: Genotype Quality  \n\n\n\n\n\n\n\n\nQuestion\n\n\nLooking at the three vcf files, how can we detect only somatic variants?\n\n\nAnswer\nSome commands to find somatic variant in the vcf file:\nvarscan:\n1\ngrep SOMATIC variant_calling/varscan.snp.vcf\n\n\n\n\n\nMuTecT:\n1\ngrep -v REJECT variant_calling/mutect.vcf | grep -v \"^#\"\n\n\n\n\n\nStrelka:\n1\ngrep -v \"^#\" variant_calling/strelka.vcf\n\n\n\n\n\n\n\n\n\n\n\nVariant Visualisation\n\u00b6\n\n\nThe Integrative Genomics Viewer (\nIGV\n) is an efficient visualization tool\nfor interactive exploration of large genome datasets.\n\n\nBefore jumping into \nIGV\n, we\u2019ll generate a track IGV that can be used to plot\ncoverage:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\njar\n ${\nIGVTOOLS_PATH\n}/\nigvtools\n.\njar\n \ncount\n \\\n    -\nf\n \nmin\n,\nmax\n,\nmean\n \\\n    \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n.\ntdf\n \\\n    \nb37\n\n\ndone\n\n\n\n\n\n\n\nOpen \nIGV\n\n\n1\n$IGV\n\n\n\n\n\n\nThen:\n\n\n\n\nChoose the reference genome corresponding to those use for alignment (b37).\n\n\nLoad BAM files from the \nalignment\n folder (\ntumour.sorted.dup.recal.bam\n and \nnormal.sorted.dup.recal.bam\n).\n\n\nLoad three VCF files (from \nvariant_calling\n directory).\n\n\n\n\n\n\nExplore and play with the data:\n\n\n\n\nFind germline variants\n\n\nFind somatic variants\n\n\nLook around\u2026\n\n\n\n\n\n\n\n\nVariant Annotation\n\u00b6\n\n\nFollowing variant calling, we end up with a VCF file of genomic\ncoordinates with the genotype(s) and quality information for each\nvariant. By itself, this information is not much use to us unless there\nis a specific genomic location we are interested in. Generally, we next\nwant to annotate these variants to determine whether they impact any\ngenes and if so what is their level of impact (e.g. are they causing a\npremature stop codon gain or are they likely less harmful missense\nmutations).\n\n\nThe sections above have dealt with calling somatic variants from the\nfirst 10Mb of chromosome 4. This is important in finding variants that\nare unique to the tumour sample(s) and may have driven both tumour\ngrowth and/or metastasis. An important secondary question is whether the\ngermline genome of the patient contains any variants that may have\ncontributed to the development of the initial tumour through\npredisposing the patient to cancer. These variants \nmay not\n be captured\nby somatic variant analysis as their allele frequency may not change in\nthe tumour genome compared with the normal.\n\n\nFor this section, we will use \nall\n variants from the first 60Mb of\nchromosome 5 that have been pre-generated using the GATK \nHaplotypeCaller\n\nvariant caller on both the normal and tumour genomes. The output of this\nwas GVCF files which were fed into GATK \nGenotypeGVCFs\n to produce a\nmerged VCF file. We will use this pre-generated file as we are primarily\ninterested in the annotation of variants rather than their generation.\nThe annotation method we will use is called \nVariant Effect Predictor\n\nor \nVEP\n for short and is available from Ensembl \nhere\n.\n\n\n\nOur pre-generated VCF file is located in the \nvariants\n folder. Let\u2019s\nhave a quick look at the variants:\n\n\n1\nzless variants/HC.chr5.60Mb.vcf.gz\n\n\n\n\n\n\nNotice how there are two genotype blocks at the end of each line for the\nnormal (\nBlood\n) and tumour (\nliverMets\n) samples.\n\n\n\nLet\u2019s now run \nVEP\n on this VCF file to annotate each variant with its\nimpact(s) on the genome.\n\n\n1\nperl\n \n$VEP\n \n--\ndir_cache\n \n$VEP_CACHE\n \n-\ni\n \nvariants\n/HC.chr5.60Mb.vcf.gz --vcf -o variants/\nHC\n.\nchr5\n.60\nMb\n.\nvep\n.\nvcf\n \n--\nstats_file\n \nvariants\n/HC.chr5.60Mb.vep.html --format vcf --offline -fork 4 --fasta ref/\nhuman_g1k_v37\n.\nfasta\n \n--\nfields\n \nConsequence\n,\nCodons\n,\nAmino_acids\n,\nGene\n,\nSYMBOL\n,\nFeature\n,\nEXON\n,\nPolyPhen\n,\nSIFT\n,\nProtein_position\n,\nBIOTYPE\n \n--\nspecies\n \nhomo_sapiens\n\n\n\n\n\n\n\n\n\nVEP\n will take approximately 10 minutes to run and once it is finished\nyou will have a new VCF file with all of the information in the input\nfile but with added annotations in the INFO block. \nVEP\n also produces an\nHTML report summarising the distribution and impact of variants\nidentified.\n\n\nOnce \nVEP\n is done running, let\u2019s first look at the HTML report it\nproduced with the following command:\n\n\n1\nfirefox variants/HC.chr5.60Mb.vep.html\n\n\n\n\n\n\nThis report shows information on the \nVEP\n run, the number of variants,\nthe classes of variants detected, the variant consequences and the\ndistributions of variants through the genome. Close Firefox to resume\nthe terminal prompt.\n\n\n\nNow let\u2019s look at the variant annotations that \nVEP\n has added to the VCF\nfile by focussing on a single variant. Let\u2019s fetch the same variant from\nthe original VCF file and the annotated VCF file to see what has been\nchanged.\n\n\n1\n2\nzcat variants/HC.chr5.60Mb.vcf.gz | grep '5\\s174106\\s'\ngrep '5\\s174106\\s' variants/HC.chr5.60Mb.vep.vcf\n\n\n\n\n\n\n\nThese commands give us the original variant:\n\n\n1\n5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446   GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145\n\n\n\n\n\n\nand the same variant annotated is:\n\n\n1\n5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446;CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant&non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron  GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145\n\n\n\n\n\n\n\nYou can see that \nVEP\n has added:\n\n\n1\nCSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant&non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron\n\n\n\n\n\n\n\nThis is further composed of two annotations for this variant:\n\n1\nmissense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding\n\n\n\n\n\nand\n\n\n1\nnon_coding_transcript_exon_variant&non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron\n\n\n\n\n\n\n\nThe first of these is saying that this variant is a missense variant in\nthe gene PLEKHG4B for the transcript ENST00000283426 and the second that\nit is also a non_coding_transcript_exon_variant in the transcript\nENST00000504041.\n\n\n\n\nVariant Filtration\n\u00b6\n\n\nWe now have a VCF file where each variant has been annotated with one or\nmore impacts for one or more genes. In a typical whole cancer genome,\nyou will have about 4-5 million variants, and therefore rows, in a VCF\nfile which takes up gigabytes of space. In our small example, we have\njust 100,000 variants which is already too large to make any kind of\nmeaningful sense out of by just opening up the VCF file in a text\neditor. We need a solution that allows us to perform intelligent queries\non our variants to search this mass of noise for the signal we are\ninterested in.\n\n\nLuckily, such a free tool exists and is called \nGEMINI\n. \nGEMINI\n takes as\nan input your annotated VCF file and creates a database file which it\ncan then query using Structured Query Language (SQL) commands. Not only\ndoes \nGEMINI\n make your variants easily searchable, it also brings in many\nexternal annotations to add more information about your variants (such\nas their frequencies in international databases).\n\n\nTo get started with \nGEMINI\n, let\u2019s make a database out of our annotated\nVCF file.\n\n\n1\n/\nusr\n/\nlocal\n/\nbin\n/\ngemini\n \nload\n \n-\nv\n \nvariants\n/\nHC\n.\nchr5\n.\n60\nMb\n.\nvep\n.\nvcf\n \n--cores 4 --skip-gerp-bp --skip-cadd -t VEP variants/HC.chr5.60Mb.vep.vcf.db\n\n\n\n\n\n\n\nThis will take approximately 10 minutes. You will see a few errors due\nto multiallelic sites, normally these sites are decomposed and\nnormalized before creating the \nGEMINI\n database but this is outside the\nscope of this workshop.\n\n\nOnce the database has been created let\u2019s run a basic query to see what\nkind of information we get out of \nGEMINI\n.\n\n\n1\n/\nusr\n/\nlocal\n/\nbin\n/\ngemini\n \nquery\n \n-\nq\n \n\"SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants LIMIT 10;\"\n \n--header variants/HC.chr5.60Mb.vep.vcf.db\n\n\n\n\n\n\n\nThis will output a bunch of ordered information for your query to the\ncommand line, this is usually saved to a TSV file and opened in a\nspreadsheet as we will do for the next query. In the mean time, let\u2019s\ndissect this query to understand the syntax we need to use to filter our\nvariants. First, we have a SELECT statement which simply specifies that\nwe want to select data from the database. The following comma-separated\nvalues are the columns that we want to output from the database, in this\ncase we are selecting all columns with the star character and then all\nsub-columns for each sample with the other values. Then, we have a \u201cFROM\nvariants\u201d statement which is specifying the table within the database\nthat we want to fetch information from. Finally, the \u201cLIMIT 10\u201d\nstatement specifies that no more than 10 rows should be returned. In\nsummary then, we are asking for all columns for 10 rows from the table\n\nvariants\n. If you haven\u2019t used SQL before don\u2019t worry, the \nGEMINI\n\nwebsite is very helpful and provides many examples for how to query your\ndatabase.\n\n\nLet\u2019s now perform a more interesting query to find variants that have a\nmedium or high impact on a gene and are rare or not present in existing\ninternational allele frequency databases. We will save the output of\nthis query to a file and open it up in a spreadsheet.\n\n\n1\n/\nusr\n/\nlocal\n/\nbin\n/\ngemini\n \nquery\n \n-\nq\n \n\"SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants WHERE (impact_severity = 'HIGH' OR impact_severity = 'MED') AND (aaf_1kg_all < 0.01 OR aaf_1kg_all is null) AND (aaf_esp_all < 0.01 OR aaf_esp_all is null) AND (aaf_exac_all < 0.01 OR aaf_exac_all is null);\"\n \n--header variants/HC.chr5.60Mb.vep.vcf.db > variants/gemini-result.tsv\n\n\n\n\n\n\n\nNotice that we have added a WHERE statement which restricts the rows\nthat are returned based on values that we specify for specific columns.\nHere, we are asking to return variants where their impact on the gene\n(impact_severity column) is medium or high and the allele frequency in\n1000Genomes, ESP and EXaC is less than 1% or the variant is not present\nin any of these databases.\n\n\nNow let\u2019s open the result in a spreadsheet to look at the annotations:\n\n\n1\nlibreoffice --calc variants/gemini-result.tsv\n\n\n\n\n\n\nTick the \nTab\n under \nSeparated by\n on the dialog window that comes up.  \n\n\nYou can see that the first 14 columns contain information on the variant\nincluding its location, ref, alt, dbSNP ID, quality and type. Slowly\nscroll to the right and look at the columns of data that are provided.\nMost importantly, column BD includes the gene this variant impacts, BN\nthe impact itself and BP the impact severity. Scroll towards the end of\nthe spreadsheet until you get to columns ED and EE, these contain the\ngenotype for each of the samples. Columns EH and EI contain the total\ndepth for each variant in each sample and the 4 following columns\ncontain the reference and alternate depths for each sample. Finally,\ncolumns EN and EO contain the genotype qualities (from the GQ field in\nthe VCF) for each sample. As you scroll back and forth through this\nspreadsheet, you will see that \nGEMINI\n brings in information from a\nvariety of sources including: OMIM, ClinVar, GERP, PolyPhen 2, SIFT,\nESP, 1000 Genomes, ExAC, ENCODE, CADD and more! We are only looking at a\nsmall number of variants from the start of a chromosome so not many of\nthese annotations will be present but in a full genome database they are\nincredibly useful.\n\n\nGEMINI\n allows you to filter your variants based on any column that you\nsee in this results file. For example, you may want all variants in a\nspecific gene, in which case you would simply add \u201cWHERE gene = \u2019BRCA1\u2019\u201d\nto your query. For complete documentation with many examples of queries,\nsee the GEMINI documentation \nhere\n.\n\n\n\n\nReferences\n\u00b6\n\n\n\n\n\n\nPaila U, Chapman BA, Kirchner R and Quinlan AR. \u201cGEMINI: Integrative\n    Exploration of Genetic Variation and Genome Annotations\u201d. PLoS\n    Comput Biol, 2013, 9(7): e1003153. doi:10.1371/journal.pcbi.1003153\n\n\n\n\n\n\nMcLaren W, Pritchard B, Rios D, Chen Y, Flicek P and Cunningham F.\n    \u201cDeriving the consequences of genomic variants with the Ensembl API\n    and SNP Effect Predictor\u201d. Bioinformatics, 2010, 26(16):2069-70,\n    doi:10.1093/bioinformatics/btq330\n\n\n\n\n\n\n\n\nAcknowledgements\n\u00b6\n\n\nThis is based on an Introduction to DNA-Seq processing for cancer data\nby Mathieu Bourgey, Ph.D.  \n\n\nThis tutorial is an adaptation of the one created by Louis letourneau.\nMathieu Bourgey would like to thank and acknowledge Louis for this help and for\nsharing his material. The format of the tutorial has been inspired from\nMar Gonzalez Porta. He also wants to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.",
            "title": "Single Nucleotide Variant Calling and Annotation"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Prepare raw BAM alignments for variant detection    Perform QC measures on BAM files    Perform simple variant detection on paired NGS data    Add annotation information to raw variant calls    Visualise variant calls using IGV",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#tools-used",
            "text": "SAMTools:  https://samtools.github.io/  IGV:  http://www.broadinstitute.org/igv/  Genome Analysis Toolkit:  http://www.broadinstitute.org/gatk/  Picard:  https://broadinstitute.github.io/picard/  MuTect:  http://www.broadinstitute.org/cancer/cga/mutect/  Strelka:  https://sites.google.com/site/strelkasomaticvariantcaller/  VarScan2:  https://dkoboldt.github.io/varscan/  Variant Effect Predictor:  http://www.ensembl.org/info/docs/tools/vep  GEMINI:  http://gemini.readthedocs.org",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#sources-of-data",
            "text": "http://sra.dnanexus.com/studies/ERP001071  http://www.ncbi.nlm.nih.gov/pubmed/22194472",
            "title": "Sources of Data"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#author-information",
            "text": "Primary Author(s): \nMatt Field  matt.field@anu.edu.au \nDan Andrews  dan.andrews@anu.edu.au \nVelimir Gayevskiy  v.gayevskiy@garvan.org.au \nMathieu Bourgey  mathieu.bourgey@mcgill.ca     Contributor(s): \nGayle Philip  Sonika.Tyagi@agrf.org.au \nSonika Tyagi  gkphilip@unimelb.edu.au",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#introduction",
            "text": "The goal of this hands-on session is to present the main steps that are\ncommonly used to process and to analyze cancer sequencing data. We will\nfocus only on whole genome data and provide command lines that allow\ndetecting Single Nucleotide Variants (SNV). This workshop will show you\nhow to launch individual steps of a complete DNA-Seq SNV pipeline using\ncancer data.  In the second part of the tutorial we will also be using IGV to\nvisualise and manually inspect candidate variant calls.",
            "title": "Introduction"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#prepare-the-environment",
            "text": "We will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.  The data consists of whole genome sequencing of liver metastatic lung\ncancer (frozen), primary lung cancer (FFPE) and blood tissue of a lung\nadenocarcinoma patient (AK55).  Open the Terminal and go to the  snv  working directory:  1 cd /home/trainee/snv/    All commands entered into the terminal for this tutorial should be from\nwithin the  snv  directory.   The BAM alignment files are contained in the subdirectory called alignment  and are located in the following subdirectories:   normal/normal.sorted.bam  and  normal/normal.sorted.bam.bai  tumour/tumor.sorted.bam  and  tumour/tumour.sorted.bam.bai   Check that the  alignment  directory contains the above-mentioned files by typing:  1 ls -l alignment/*   \nThese files are based on subsetting the whole genomes derived from blood\nand liver metastases to the first 10Mb of chromosome 4. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that this is less < 0.5% of the genome which\nhighlights the length of time and resources required to perform cancer\ngenomics on full genomes!  The initial structure of your folders should look like this (type  ls -l ):  1\n2\n3\n4 -- alignment/             # bam files\n  -- normal/                # The blood sample directory containing bam files\n  -- tumour/                # The tumour sample directory containing bam files\n-- ref/                   # Contains reference genome files         \nNow we need to set some environment variables to save typing lengthy\nfile paths over and over. Copy and paste the following commands into\nyour terminal.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 export   APP_ROOT = /home/trainee/snv/Applications export   IGVTOOLS_PATH = $APP_ROOT /igvtools/ export   PICARD_JAR = $APP_ROOT /picard/picard.jar export   GATK_JAR = $APP_ROOT /gatk/GenomeAnalysisTK.jar export   STRELKA_HOME = $APP_ROOT /strelka/ export   MUTECT_JAR = $APP_ROOT /mutect/muTect-1.1.5.jar export   VARSCAN_JAR = $APP_ROOT /varscan/VarScan.v2.4.1.jar export   REF = /home/trainee/snv/ref export   SNV_BASE = /home/trainee/snv export   JAVA7 = /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java export   IGV = $APP_ROOT /igv/igv.sh export   VEP = $APP_ROOT /ensembl-tools/scripts/variant_effect_predictor/variant_effect_predictor.pl export   VEP_CACHE = /mnt/workshop/data/bgdata/datasets/vepcache/82   \nMake sure you are in the correct directory by typing:  1 cd $SNV_BASE",
            "title": "Prepare the Environment"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#bam-files",
            "text": "Let\u2019s spend some time exploring BAM files.",
            "title": "BAM Files"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#exploring-bam-files",
            "text": "1 samtools view alignment/normal/normal.sorted.bam | head -n4   Here you have examples of alignment results. A full description of the\nflags can be found in the  SAM specification .  Another useful bit of information in the SAM is the CIGAR string. It\u2019s\nthe 6 th  column in the file.  This column explains how the alignment was achieved.   M == base aligns  but doesn\u2019t have to be a match.  A SNP will have an M even if it disagrees with the reference. \nI == Insertion \nD == Deletion \nS == soft-clips. These are handy to find un removed adapters, viral insertions, etc.   An in-depth explanation of the CIGAR can be found  here .\nThe exact details of the CIGAR string can be found in the SAM specification as\nwell. We won\u2019t go into too much detail at this point since we want to concentrate on\ncancer specific issues now.  Now, you can try using Picard\u2019s  explain flag \nsite to understand what is going on with your reads.   Question  There are 3 unique flags, what do they mean? The flag is the second column.  Answer 129: \nread paired \nsecond in pair   113: \nread paired \nread reverse strand \nmate reverse strand \nfirst in pair   161: \nread paired \nmate reverse strand \nsecond in pair     \nThere are lots of possible different flags, let\u2019s look at a few more  1 samtools view alignment/normal/normal.sorted.bam | head -n 100    Question  Let\u2019s take the last read, which looks properly paired and find its mate pair.  Hint a)    Instead of using  head , what unix command could we pipe the output to? \nb)    Once we\u2019ve found both reads, the command can be stopped by typing  CTRL-C     Answer 1 samtools view alignment/normal/normal.sorted.bam | grep HWI-ST478_0133:4:2205:14675:32513      Question  Using the cigar string, what can we tell about the alignment of the mate pair?  Answer The mate pair has a less convincing alignment with two insertions and\nsoft clipping reported.    Question  How might the alignment information from the original read be used by the aligner?  Answer Even though the alignment of the mate pair is questionable the presence\n  of it\u2019s properly paired mate helps the aligner in deciding where to put\n  the less-certain read.   You can use  Samtools  to filter reads as well.   Question  How many reads mapped and unmapped were there?  Hint Look at the samtools view help menu by typing  samtools view  without any arguments  Answer 1 samtools view -c -f4 alignment/normal/normal.sorted.bam  \n77229 1 samtools view -c -F4 alignment/normal/normal.sorted.bam  \n22972373",
            "title": "Exploring BAM files"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#step-1-pre-processing-indel-realignment",
            "text": "The first step for this is to realign around indels and SNP dense regions.\nThe Genome Analysis toolkit ( GATK ) has a tool for this called IndelRealigner.\nIt basically runs in 2 steps:     Find the targets  Realign them   1\n2\n3\n4\n5\n6\n7 $JAVA7   - Xmx2G    - jar   $ { GATK_JAR }   \\ \n   - T   RealignerTargetCreator   \\ \n   - R   $ { REF }/ human_g1k_v37 . fasta   \\ \n   - o   alignment / normal / realign . intervals   \\ \n   - I   alignment / normal / normal . sorted . bam   \\ \n   - I   alignment / tumour / tumour . sorted . bam   \\ \n   - L   $ { REF }/ human_g1k_v37 . intervals    1\n2\n3\n4\n5\n6\n7\n8 $JAVA7   - Xmx2G   - jar   $ { GATK_JAR }   \\ \n   - T   IndelRealigner   \\ \n   - R   $ { REF }/ human_g1k_v37 . fasta   \\ \n   - targetIntervals   alignment / normal / realign . intervals   \\ \n   -- nWayOut   . realigned . bam   \\ \n   - I   alignment / normal / normal . sorted . bam   \\ \n   - I   alignment / tumour / tumour . sorted . bam   \\ \n   - L   $ { REF }/ human_g1k_v37 . intervals    Explanation of parameters:   -I : BAM file(s)  -T : GATK algorithm to run  -R : the reference genome used for mapping (b37 from GATK here)  -jar : Path to GATK jar file  -L : Genomic intervals to operate on   \nMove the realigned BAMs and index files to the corresponding normal and\ntumour directories.  1\n2 mv normal.sorted.realigned.ba* alignment/normal/\nmv tumour.sorted.realigned.ba* alignment/tumour/    Question  Why did we use both normal and tumor together?  Answer Because if a region needs realignment, maybe one of the samples in the\npair has less reads or was excluded from the target creation.\nThis makes sure the normal and tumor are all in-sync for the somatic\ncalling step.    Question  How many regions did it think needed cleaning?  Answer 1 wc -l alignment/normal/realign.intervals   27300   \nIndel Realigner also makes sure the called deletions are left aligned\nwhen there is a microsatellite or homopolymer. e.g.  This \n  ATCGAAAA-TCG \n  into \n  ATCG-AAAATCG    or    ATCGATATATATA\u2013TCG \n  into \n  ATCG\u2013ATATATATATCG     Question  Why is it important?  Answer This makes it easier for downstream analysis tools. For NGS analysis, the convention is to left align indels. This is only really needed when calling variants with legacy locus-based\ntools such as samtools or GATK UnifiedGenotyper. Otherwise you will have\nworse performance and accuracy. With more sophisticated tools (like GATK HaplotypeCaller) that involve\nreconstructing haplotypes (e.g. through reassembly), the problem of\nmultiple valid representations is handled internally and does not need\nto be corrected explicitly.",
            "title": "Step 1: Pre-processing: Indel Realignment"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#step-2-pre-processing-fixmates",
            "text": "Some read entries don\u2019t have their mate information written properly. \nWe use  Picard  to do this:  Normal sample:\n   1\n2\n3\n4\n5\n6\n7 $JAVA7   - Xmx2G   - jar   $ { PICARD_JAR }   FixMateInformation   \\ \n   VALIDATION_STRINGENCY = SILENT   \\ \n   CREATE_INDEX = true   \\ \n   SORT_ORDER = coordinate   \\ \n   MAX_RECORDS_IN_RAM = 500000   \\ \n   INPUT = alignment / normal / normal . sorted . realigned . bam   \\ \n   OUTPUT = alignment / normal / normal . matefixed . bam    Tumour sample:  \n   1\n2\n3\n4\n5\n6\n7 $JAVA7   - Xmx2G   - jar   $ { PICARD_JAR }   FixMateInformation   \\ \n   VALIDATION_STRINGENCY = SILENT   \\ \n   CREATE_INDEX = true   \\ \n   SORT_ORDER = coordinate   \\ \n   MAX_RECORDS_IN_RAM = 500000   \\ \n   INPUT = alignment / tumour / tumour . sorted . realigned . bam   \\ \n   OUTPUT = alignment / tumour / tumour . matefixed . bam",
            "title": "Step 2: Pre-processing: Fixmates"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#step-3-pre-processing-mark-duplicates",
            "text": "Question  What are duplicate reads?  Answer Different read pairs representing the same initial DNA fragment.    Question  What are they caused by?  Answer PCR reactions (PCR duplicates).   Some clusters that are thought of being separate in the flowcell but are\nthe same (optical duplicates)    Question  What are the ways to detect them?  Answer Picard and samtools uses the alignment positions:   Both 5\u2019 ends of both reads need to have the same positions. Each reads have to be on the same strand as well. Another method is to use a kmer approach: Take a part of both ends of the fragment. Build a hash table. Count the similar hits. Brute force, compare all the sequences.   \nHere we will use  Picard \u2018s approach:  Normal Sample:\n   1\n2\n3\n4\n5\n6\n7\n8 $JAVA7   - Xmx2G   - jar   $ { PICARD_JAR }   MarkDuplicates   \\ \n   REMOVE_DUPLICATES = false   \\ \n   CREATE_MD5_FILE = true   \\ \n   VALIDATION_STRINGENCY = SILENT   \\ \n   CREATE_INDEX = true   \\ \n   INPUT = alignment / normal / normal . matefixed . bam   \\ \n   OUTPUT = alignment / normal / normal . sorted . dup . bam   \\ \n   METRICS_FILE = alignment / normal / normal . sorted . dup . metrics    Tumour Sample: \n   1\n2\n3\n4\n5\n6\n7\n8 $JAVA7   - Xmx2G   - jar   $ { PICARD_JAR }   MarkDuplicates   \\ \n   REMOVE_DUPLICATES = false   \\ \n   CREATE_MD5_FILE = true   \\ \n   VALIDATION_STRINGENCY = SILENT   \\ \n   CREATE_INDEX = true   \\ \n   INPUT = alignment / tumour / tumour . matefixed . bam   \\ \n   OUTPUT = alignment / tumour / tumour . sorted . dup . bam   \\ \n   METRICS_FILE = alignment / tumour / tumour . sorted . dup . metrics    \nWe can look in the metrics output to see what happened.  1 less alignment/normal/normal.sorted.dup.metrics    Question  What percent of reads are duplicates?  Answer 0.046996%    Question  Often, we have multiple libraries and when this occurs separate measures\nare calculated for each library. Why is it important to not combine everything?  Answer Each library represents a set of different DNA fragments. Each library involves different PCR reactions PCR duplicates can not occur between fragments of two different\nlibraries. However, similar fragments could be found between\nlibraries when the coverage is high.",
            "title": "Step 3: Pre-processing: Mark Duplicates"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#step-4-pre-processing-base-quality-recalibration",
            "text": "Question  Why do we need to recalibrate base quality scores?  Answer The vendors tend to inflate the values of the bases in the reads. The\nrecalibration tries to lower the scores of some biased motifs for some\ntechnologies.   \nBase Quality Recalibration runs in 2 steps:   Build covariates based on context and known SNP sites.  Correct the reads based on these metrics.   GATK BaseRecalibrator:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 for   i   in   normal   tumour  do \n   $JAVA7  - Xmx2G  - jar  ${ GATK_JAR } \\\n    - T   BaseRecalibrator  \\\n    - nct   2  \\\n    - R  ${ REF }/ human_g1k_v37 . fasta  \\\n    - knownSites  ${ REF }/ dbSnp-138_chr4 . vcf  \\\n    - L   4 : 1 - 10000000  \\\n    - o   alignment /${ i }/${ i }. sorted . dup . recalibration_report . grp  \\\n    - I   alignment /${ i }/${ i }. sorted . dup . bam \n\n     $JAVA7  - Xmx2G  - jar  ${ GATK_JAR } \\\n      - T   PrintReads  \\\n      - nct   2  \\\n      - R  ${ REF }/ human_g1k_v37 . fasta  \\\n      - BQSR   alignment /${ i }/${ i }. sorted . dup . recalibration_report . grp  \\\n      - o   alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n      - I   alignment /${ i }/${ i }. sorted . dup . bam  done",
            "title": "Step 4: Pre-processing: Base Quality Recalibration"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#bam-qc",
            "text": "Once your whole BAM is generated, it\u2019s always a good thing to check the\ndata again to see if everything makes sense.",
            "title": "BAM QC"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#step-1-bam-qc-compute-coverage",
            "text": "If you have data from a capture kit, you should see how well your\ntargets worked.  GATK  has a depth of coverage tool to do this:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 for   i   in   normal   tumour  do \n   $JAVA7   - Xmx2G  - jar  ${ GATK_JAR } \\\n    - T   DepthOfCoverage  \\\n    -- omitDepthOutputAtEachBase  \\\n    -- summaryCoverageThreshold   10  \\\n    -- summaryCoverageThreshold   25  \\\n    -- summaryCoverageThreshold   50  \\\n    -- summaryCoverageThreshold   100  \\\n    -- start   1  -- stop   500  -- nBins   499  - dt   NONE  \\\n    - R  ${ REF }/ human_g1k_v37 . fasta  \\\n    - o   alignment /${ i }/${ i }. sorted . dup . recal . coverage  \\\n    - I   alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n    - L   4 : 1 - 10000000  done    Explanation of parameters:   --omitBaseOutputAtEachBase : Do not output depth of coverage at each base  --summaryCoverageThreshold : Coverage threshold (in percent) for summarizing statistics  -dt : Downsampling  -L : Genomic intervals to operate on     \nIn this project, coverage is expected to be 25x. Look at the coverage:  1 less -S alignment/normal/normal.sorted.dup.recal.coverage.sample_interval_summary   Type  q  to return to the prompt.  1 less -S alignment/tumour/tumour.sorted.dup.recal.coverage.sample_interval_summary    Question  Does the coverage fit with the expectation?  Answer Yes the mean coverage of the region is 25x. summaryCoverageThreshold  is a useful function to see if your coverage is\nuniform. Another way is to compare the mean to the median. If both are quite\ndifferent that means something is wrong in your coverage.",
            "title": "Step 1: BAM QC: Compute Coverage"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#step-2-bam-qc-insert-size",
            "text": "Insert size corresponds to the size of DNA fragments sequenced. It is different\nfrom the gap size (= distance between reads)!  \nThese metrics are computed using  Picard :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 for   i   in   normal   tumour  do \n   $JAVA7  - Xmx2G  - jar  ${ PICARD_JAR }  CollectInsertSizeMetrics  \\\n     VALIDATION_STRINGENCY = SILENT  \\\n     REFERENCE_SEQUENCE =${ REF }/ human_g1k_v37 . fasta  \\\n     INPUT = alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n     OUTPUT = alignment /${ i }/${ i }. sorted . dup . recal . metric . insertSize . tsv  \\\n     HISTOGRAM_FILE = alignment /${ i }/${ i }. sorted . dup . recal . metric . insertSize . histo . pdf  \\\n     METRIC_ACCUMULATION_LEVEL = LIBRARY  done    \nLook at the output:  1\n2 less -S alignment/normal/normal.sorted.dup.recal.metric.insertSize.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.insertSize.tsv    Question  How do the two libraries compare?      Answer The tumour sample has a larger median insert size than the\nnormal sample (405 vs. 329).",
            "title": "Step 2: BAM QC: Insert Size"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#step-3-bam-qc-alignment-metrics",
            "text": "Alignment metrics tells you if your sample and your reference fit together.  For the alignment metrics,  samtools flagstat  is very fast but bwa-mem  breaks some reads into pieces, the numbers can be a bit confusing.  Instead, we will use  Picard  to compute the metrics:  1\n2\n3\n4\n5\n6\n7\n8\n9 for   i   in   normal   tumour  do \n   $JAVA7  - Xmx2G  - jar  ${ PICARD_JAR }  CollectAlignmentSummaryMetrics  \\\n     VALIDATION_STRINGENCY = SILENT  \\\n     REFERENCE_SEQUENCE =${ REF }/ human_g1k_v37 . fasta  \\\n     INPUT = alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n     OUTPUT = alignment /${ i }/${ i }. sorted . dup . recal . metric . alignment . tsv  \\\n     METRIC_ACCUMULATION_LEVEL = LIBRARY  done    \nExplore the results  1\n2 less -S alignment/normal/normal.sorted.dup.recal.metric.alignment.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.alignment.tsv    Question  Do you think the sample and the reference genome fit together?  Answer Yes, 99% of the reads have been aligned. \nUsually, we consider: A good alignment if > 85% Reference assembly issues if [60-85]% Probably a mismatch between sample and ref if < 60 %",
            "title": "Step 3: BAM QC: Alignment metrics"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#variant-calling",
            "text": "Most of SNV caller use either a Bayesian, a threshold or a t-test\napproach to do the calling  Here we will try 3 variant callers:   Varscan 2  MuTecT  Strelka   Other candidates:   Virmid  Somatic sniper   Many, MANY others can be found here:  https://www.biostars.org/p/19104/  In our case, let\u2019s create a new work directory to start with (from base\ndirectory):  1\n2 cd $SNV_BASE\nmkdir variant_calling",
            "title": "Variant Calling"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#varscan-2",
            "text": "Varscan 2  calls somatic variants (SNPs and indels) using a\nheuristic method and a statistical test based on the number of aligned\nreads supporting each allele. It expects both a normal and a tumour file in SAMtools pileup  format from sequence alignments in binary alignment/map\n(BAM) format. To build a pileup file, you will need:   A SAM/BAM file ( *.sorted.dup.recal.bam ) that has been sorted using the  sort  command\nof  samtools .  The reference sequence ( human_g1k_v37.fasta ) to which reads were aligned, in\nFASTA format.  The  samtools  software package.   1\n2\n3\n4\n5\n6\n7\n8 for  i in normal tumour do \nsamtools mpileup -L  1000  -B -q  1   \\ \n  -f  ${ REF } /human_g1k_v37.fasta  \\ \n  -r  4 :1-10000000  \\ \n  alignment/ ${ i } / ${ i } .sorted.dup.recal.bam  \\ \n  > variant_calling/ ${ i } .mpileup done    Notes on  samtools  arguments:   -L : max per-sample depth for INDEL calling [1000]  -B : disable BAQ (per-Base Alignment Quality)  -q : skip alignments with mapQ smaller than 1  -g : generate genotype likelihoods in BCF format   \n   1\n2\n3\n4\n5\n6\n7 $JAVA7   - Xmx2G   - jar   $ { VARSCAN_JAR }   \\  somatic   variant_calling / normal . mpileup   \\  variant_calling / tumour . mpileup   \\  variant_calling / varscan   \\  -- output - vcf   1   \\  -- strand - filter   1   \\  -- somatic - p - value   0.001",
            "title": "Varscan 2"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#mutect",
            "text": "Now let\u2019s try a different variant caller,  MuTect .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 $JAVA7   - Xmx2G   - jar   $ { MUTECT_JAR }   \\ \n   - T   MuTect   \\ \n   - R   $ { REF }/ human_g1k_v37 . fasta   \\ \n   - dt   NONE   - baq   OFF   -- validation_strictness   LENIENT   \\ \n   -- dbsnp   $ { REF }/ dbSnp - 138_ chr4 . vcf   \\ \n   -- input_file : normal   alignment / normal / normal . sorted . dup . recal . bam   \\ \n   -- input_file : tumor   alignment / tumour / tumour . sorted . dup . recal . bam   \\ \n   -- out   variant_calling / mutect . call_stats . txt   \\ \n   -- coverage_file   variant_calling / mutect . wig . txt   \\ \n   - pow   variant_calling / mutect . power   \\ \n   - vcf   variant_calling / mutect . vcf   \\ \n   - L   4 : 1 - 10000000",
            "title": "MuTect"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#strelka",
            "text": "And finally let\u2019s try Illumina\u2019s  Strelka .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 cp  ${ STRELKA_HOME } /etc/strelka_config_bwa_default.ini .\n\nsed 's/isSkipDepthFilters =.*/isSkipDepthFilters = 1/g' -i strelka_config_bwa_default.ini ${ STRELKA_HOME } /bin/configureStrelkaWorkflow.pl \\\n  --normal=alignment/normal/normal.sorted.dup.recal.bam \\\n  --tumor=alignment/tumour/tumour.sorted.dup.recal.bam \\\n  --ref= ${ REF } /human_g1k_v37.fasta \\\n  --config= ${ SNV_BASE } /strelka_config_bwa_default.ini \\\n  --output-dir=variant_calling/strelka/\n\ncd variant_calling/strelka/\nmake -j2\ncd ../..\n\ncp variant_calling/strelka/results/passed.somatic.snvs.vcf variant_calling/strelka.vcf",
            "title": "Strelka"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#comparing-variant-callers",
            "text": "Now we have variants from all three methods. Let\u2019s compress and index\nthe VCFs for future visualisation.  1 for  i in variant_calling/*.vcf ;   do  bgzip -c  $i  >  $i .gz  ;  tabix -p vcf  $i .gz ;   done    Let\u2019s look at a compressed VCF. Details on the VCF spec can be found  here .  1 zless -S variant_calling/varscan.snp.vcf.gz   \nFields vary from caller to caller. Some values are are almost always there:   Ref vs. alt alleles  Variant quality (QUAL column)  The per-sample genotype (GT) values.   Note on VCF fields:   DP : Raw read depth  GT : Genotype  PL : List of Phred-scaled genotype likelihoods. (min is better)  DP : \u201c\u201d# high-quality bases\u201d  SP : Phred-scaled strand bias P-value  GQ : Genotype Quality       Question  Looking at the three vcf files, how can we detect only somatic variants?  Answer Some commands to find somatic variant in the vcf file: varscan: 1 grep SOMATIC variant_calling/varscan.snp.vcf   MuTecT: 1 grep -v REJECT variant_calling/mutect.vcf | grep -v \"^#\"   Strelka: 1 grep -v \"^#\" variant_calling/strelka.vcf",
            "title": "Comparing variant callers"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#variant-visualisation",
            "text": "The Integrative Genomics Viewer ( IGV ) is an efficient visualization tool\nfor interactive exploration of large genome datasets.  Before jumping into  IGV , we\u2019ll generate a track IGV that can be used to plot\ncoverage:  1\n2\n3\n4\n5\n6\n7\n8 for   i   in   normal   tumour  do \n   $JAVA7  - jar  ${ IGVTOOLS_PATH }/ igvtools . jar   count  \\\n    - f   min , max , mean  \\\n     alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n     alignment /${ i }/${ i }. sorted . dup . recal . bam . tdf  \\\n     b37  done    Open  IGV  1 $IGV   Then:   Choose the reference genome corresponding to those use for alignment (b37).  Load BAM files from the  alignment  folder ( tumour.sorted.dup.recal.bam  and  normal.sorted.dup.recal.bam ).  Load three VCF files (from  variant_calling  directory).    Explore and play with the data:   Find germline variants  Find somatic variants  Look around\u2026",
            "title": "Variant Visualisation"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#variant-annotation",
            "text": "Following variant calling, we end up with a VCF file of genomic\ncoordinates with the genotype(s) and quality information for each\nvariant. By itself, this information is not much use to us unless there\nis a specific genomic location we are interested in. Generally, we next\nwant to annotate these variants to determine whether they impact any\ngenes and if so what is their level of impact (e.g. are they causing a\npremature stop codon gain or are they likely less harmful missense\nmutations).  The sections above have dealt with calling somatic variants from the\nfirst 10Mb of chromosome 4. This is important in finding variants that\nare unique to the tumour sample(s) and may have driven both tumour\ngrowth and/or metastasis. An important secondary question is whether the\ngermline genome of the patient contains any variants that may have\ncontributed to the development of the initial tumour through\npredisposing the patient to cancer. These variants  may not  be captured\nby somatic variant analysis as their allele frequency may not change in\nthe tumour genome compared with the normal.  For this section, we will use  all  variants from the first 60Mb of\nchromosome 5 that have been pre-generated using the GATK  HaplotypeCaller \nvariant caller on both the normal and tumour genomes. The output of this\nwas GVCF files which were fed into GATK  GenotypeGVCFs  to produce a\nmerged VCF file. We will use this pre-generated file as we are primarily\ninterested in the annotation of variants rather than their generation.\nThe annotation method we will use is called  Variant Effect Predictor \nor  VEP  for short and is available from Ensembl  here .  \nOur pre-generated VCF file is located in the  variants  folder. Let\u2019s\nhave a quick look at the variants:  1 zless variants/HC.chr5.60Mb.vcf.gz   Notice how there are two genotype blocks at the end of each line for the\nnormal ( Blood ) and tumour ( liverMets ) samples.  \nLet\u2019s now run  VEP  on this VCF file to annotate each variant with its\nimpact(s) on the genome.  1 perl   $VEP   -- dir_cache   $VEP_CACHE   - i   variants /HC.chr5.60Mb.vcf.gz --vcf -o variants/ HC . chr5 .60 Mb . vep . vcf   -- stats_file   variants /HC.chr5.60Mb.vep.html --format vcf --offline -fork 4 --fasta ref/ human_g1k_v37 . fasta   -- fields   Consequence , Codons , Amino_acids , Gene , SYMBOL , Feature , EXON , PolyPhen , SIFT , Protein_position , BIOTYPE   -- species   homo_sapiens     VEP  will take approximately 10 minutes to run and once it is finished\nyou will have a new VCF file with all of the information in the input\nfile but with added annotations in the INFO block.  VEP  also produces an\nHTML report summarising the distribution and impact of variants\nidentified.  Once  VEP  is done running, let\u2019s first look at the HTML report it\nproduced with the following command:  1 firefox variants/HC.chr5.60Mb.vep.html   This report shows information on the  VEP  run, the number of variants,\nthe classes of variants detected, the variant consequences and the\ndistributions of variants through the genome. Close Firefox to resume\nthe terminal prompt.  \nNow let\u2019s look at the variant annotations that  VEP  has added to the VCF\nfile by focussing on a single variant. Let\u2019s fetch the same variant from\nthe original VCF file and the annotated VCF file to see what has been\nchanged.  1\n2 zcat variants/HC.chr5.60Mb.vcf.gz | grep '5\\s174106\\s'\ngrep '5\\s174106\\s' variants/HC.chr5.60Mb.vep.vcf   \nThese commands give us the original variant:  1 5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446   GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145   and the same variant annotated is:  1 5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446;CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant&non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron  GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145   \nYou can see that  VEP  has added:  1 CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant&non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron   \nThis is further composed of two annotations for this variant: 1 missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding   and  1 non_coding_transcript_exon_variant&non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron   \nThe first of these is saying that this variant is a missense variant in\nthe gene PLEKHG4B for the transcript ENST00000283426 and the second that\nit is also a non_coding_transcript_exon_variant in the transcript\nENST00000504041.",
            "title": "Variant Annotation"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#variant-filtration",
            "text": "We now have a VCF file where each variant has been annotated with one or\nmore impacts for one or more genes. In a typical whole cancer genome,\nyou will have about 4-5 million variants, and therefore rows, in a VCF\nfile which takes up gigabytes of space. In our small example, we have\njust 100,000 variants which is already too large to make any kind of\nmeaningful sense out of by just opening up the VCF file in a text\neditor. We need a solution that allows us to perform intelligent queries\non our variants to search this mass of noise for the signal we are\ninterested in.  Luckily, such a free tool exists and is called  GEMINI .  GEMINI  takes as\nan input your annotated VCF file and creates a database file which it\ncan then query using Structured Query Language (SQL) commands. Not only\ndoes  GEMINI  make your variants easily searchable, it also brings in many\nexternal annotations to add more information about your variants (such\nas their frequencies in international databases).  To get started with  GEMINI , let\u2019s make a database out of our annotated\nVCF file.  1 / usr / local / bin / gemini   load   - v   variants / HC . chr5 . 60 Mb . vep . vcf   --cores 4 --skip-gerp-bp --skip-cadd -t VEP variants/HC.chr5.60Mb.vep.vcf.db    This will take approximately 10 minutes. You will see a few errors due\nto multiallelic sites, normally these sites are decomposed and\nnormalized before creating the  GEMINI  database but this is outside the\nscope of this workshop.  Once the database has been created let\u2019s run a basic query to see what\nkind of information we get out of  GEMINI .  1 / usr / local / bin / gemini   query   - q   \"SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants LIMIT 10;\"   --header variants/HC.chr5.60Mb.vep.vcf.db    This will output a bunch of ordered information for your query to the\ncommand line, this is usually saved to a TSV file and opened in a\nspreadsheet as we will do for the next query. In the mean time, let\u2019s\ndissect this query to understand the syntax we need to use to filter our\nvariants. First, we have a SELECT statement which simply specifies that\nwe want to select data from the database. The following comma-separated\nvalues are the columns that we want to output from the database, in this\ncase we are selecting all columns with the star character and then all\nsub-columns for each sample with the other values. Then, we have a \u201cFROM\nvariants\u201d statement which is specifying the table within the database\nthat we want to fetch information from. Finally, the \u201cLIMIT 10\u201d\nstatement specifies that no more than 10 rows should be returned. In\nsummary then, we are asking for all columns for 10 rows from the table variants . If you haven\u2019t used SQL before don\u2019t worry, the  GEMINI \nwebsite is very helpful and provides many examples for how to query your\ndatabase.  Let\u2019s now perform a more interesting query to find variants that have a\nmedium or high impact on a gene and are rare or not present in existing\ninternational allele frequency databases. We will save the output of\nthis query to a file and open it up in a spreadsheet.  1 / usr / local / bin / gemini   query   - q   \"SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants WHERE (impact_severity = 'HIGH' OR impact_severity = 'MED') AND (aaf_1kg_all < 0.01 OR aaf_1kg_all is null) AND (aaf_esp_all < 0.01 OR aaf_esp_all is null) AND (aaf_exac_all < 0.01 OR aaf_exac_all is null);\"   --header variants/HC.chr5.60Mb.vep.vcf.db > variants/gemini-result.tsv    Notice that we have added a WHERE statement which restricts the rows\nthat are returned based on values that we specify for specific columns.\nHere, we are asking to return variants where their impact on the gene\n(impact_severity column) is medium or high and the allele frequency in\n1000Genomes, ESP and EXaC is less than 1% or the variant is not present\nin any of these databases.  Now let\u2019s open the result in a spreadsheet to look at the annotations:  1 libreoffice --calc variants/gemini-result.tsv   Tick the  Tab  under  Separated by  on the dialog window that comes up.    You can see that the first 14 columns contain information on the variant\nincluding its location, ref, alt, dbSNP ID, quality and type. Slowly\nscroll to the right and look at the columns of data that are provided.\nMost importantly, column BD includes the gene this variant impacts, BN\nthe impact itself and BP the impact severity. Scroll towards the end of\nthe spreadsheet until you get to columns ED and EE, these contain the\ngenotype for each of the samples. Columns EH and EI contain the total\ndepth for each variant in each sample and the 4 following columns\ncontain the reference and alternate depths for each sample. Finally,\ncolumns EN and EO contain the genotype qualities (from the GQ field in\nthe VCF) for each sample. As you scroll back and forth through this\nspreadsheet, you will see that  GEMINI  brings in information from a\nvariety of sources including: OMIM, ClinVar, GERP, PolyPhen 2, SIFT,\nESP, 1000 Genomes, ExAC, ENCODE, CADD and more! We are only looking at a\nsmall number of variants from the start of a chromosome so not many of\nthese annotations will be present but in a full genome database they are\nincredibly useful.  GEMINI  allows you to filter your variants based on any column that you\nsee in this results file. For example, you may want all variants in a\nspecific gene, in which case you would simply add \u201cWHERE gene = \u2019BRCA1\u2019\u201d\nto your query. For complete documentation with many examples of queries,\nsee the GEMINI documentation  here .",
            "title": "Variant Filtration"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#references",
            "text": "Paila U, Chapman BA, Kirchner R and Quinlan AR. \u201cGEMINI: Integrative\n    Exploration of Genetic Variation and Genome Annotations\u201d. PLoS\n    Comput Biol, 2013, 9(7): e1003153. doi:10.1371/journal.pcbi.1003153    McLaren W, Pritchard B, Rios D, Chen Y, Flicek P and Cunningham F.\n    \u201cDeriving the consequences of genomic variants with the Ensembl API\n    and SNP Effect Predictor\u201d. Bioinformatics, 2010, 26(16):2069-70,\n    doi:10.1093/bioinformatics/btq330",
            "title": "References"
        },
        {
            "location": "/modules/cancer-module-snv/snv/#acknowledgements",
            "text": "This is based on an Introduction to DNA-Seq processing for cancer data\nby Mathieu Bourgey, Ph.D.    This tutorial is an adaptation of the one created by Louis letourneau.\nMathieu Bourgey would like to thank and acknowledge Louis for this help and for\nsharing his material. The format of the tutorial has been inspired from\nMar Gonzalez Porta. He also wants to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.",
            "title": "Acknowledgements"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nUnderstand and perform a simple copy number variation analysis on\n    NGS data\n\n\n\n\n\n\nBecome familiar with Sequenza\n\n\n\n\n\n\nUnderstand the CNV inference process as an interplay between depth\n    of sequencing, cellularity and B-allele frequency\n\n\n\n\n\n\nVisualize CNV events by manual inspection\n\n\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nSequenza:\n\n\nhttp://www.cbs.dtu.dk/biotools/sequenza/\n\n\nIGV:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nSources of Data\n\u00b6\n\n\nRaw data download:\n\n\nhttp://sra.dnanexus.com/studies/ERP001071\n\n\nData publication:\n\n\nhttp://www.ncbi.nlm.nih.gov/pubmed/22194472\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n\nVelimir Gayevskiy, Garvan Institute \nv.gayevskiy@garvan.org.au\n\nSonika Tyagi, AGRF \nsonika.tyagi@agrf.org.au\n  \n\n\nContributor(s):\n  \n\n\n\n\nIntroduction\n\u00b6\n\n\nThe goal of this hands-on session is to perform a copy number variation\nanalysis (CNV) on a normal/tumour pair of alignment files (BAMs)\nproduced by the mapping of Illumina short read sequencing data.\n\n\nTo ensure reasonable analysis times, we will perform the analysis on a\nheavily subsetted pair of BAM files. These files contain just the first\n60Mb of chromosome 5 but contain several examples of inferred copy\nnumber events to enable interpretation and visualisation of the copy\nnumber variation that is present in entire cancer genomes. \nSequenza\n is\nthe tool we will use to perform this analysis. It consists of two\n\nPython\n pre-processing steps followed by a third step in \nR\n to infer the\ndepth ratio, cellularity, ploidy and to plot the results for\ninterpretation.\n\n\nIn the second part of the tutorial we will also be using \nIGV\n to\nvisualise and manually inspect the copy number variation we inferred in\nthe first part for validation purposes. This section will also include a\ndiscussion on the importance of good quality data by highlighting the\ninadequacies of the workshop dataset and the implications this has on\nanalysis results.\n\n\n\n\nPrepare the Environment\n\u00b6\n\n\nWe will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.\n\n\nThe data files are contained in the subdirectory called \ndata\n and are\nthe following:\n\n\n\n\nnormal.chr5.60Mb.bam\n and \nnormal.chr5.60Mb.bam.bai\n\n\ntumour.chr5.60Mb.bam\n and \ntumour.chr5.60Mb.bam.bai\n\n\n\n\n\nThese files are based on subsetting the whole genomes derived from \nblood\n\nand \nliver metastases\n to the first 60Mb of chromosome 5. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that we are analysing just 1.9% of the genome which\nwill highlight the length of time and resources required to perform\ncancer genomics on full genomes!\n\n\nOpen the Terminal and go to the \ncnv\n working directory:\n\n\n1\ncd /home/trainee/cnv/\n\n\n\n\n\n\n\n\nAll commands entered into the terminal for this tutorial should be from\nwithin the \ncnv\n directory.\n\n\n\n\nCheck that the \ndata\n directory contains the above-mentioned files by\ntyping:\n\n\n1\nls -l data\n\n\n\n\n\n\n\n\nSequenza CNV Analysis\n\u00b6\n\n\nSequenza is run in three steps. The first pre-processing step is run on\nthe final normal and tumour mapped data (BAM files) in order to walk the\ngenome in a pileup format (automatically generated by \nsamtools\n). This\nfirst step finds high quality sites in the genomes and extracts their\ndepth and genotype in the normal genome and calculates the variant\nalleles and allele frequencies in the tumour genome. The second step is\nto perform a binning on these sites to save space and analysis time in\nthe third step. Finally, the third step is run in \nR\n to normalise the depth\nratio between the normal/tumour genomes, infer cellularity and ploidy\nand graphically output results for interpretation.\n\n\nStep 1: Pre-processing \u2013 Walking the Genome\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\npypy software/sequenza/sequenza-utils.py bam2seqz \\\n-n data/normal.chr5.60Mb.bam \\\n-t data/tumour.chr5.60Mb.bam \\\n--fasta assets/human_g1k_v37.fasta \\\n-gc assets/human_g1k_v37.gc50Base.txt.gz \\\n-C 5:1-60000000 | gzip > stage1.seqz.gz\n\n\n\n\n\n\n\n\nHint\n\n\nPress tab after typing a few characters of a directory of filename to\nauto-complete the rest. This makes entering long file names very quick.\n\n\n\n\nExplanation of parameters:\n\n\n\n\n-n\n: the normal BAM\n\n\n-t\n: the tumour BAM\n\n\n--fasta\n: the reference genome used for mapping (b37 here)\n\n\n-gc\n: GC content as windows through the genome (pre-generated and downloadable from the Sequenza website)\n\n\n-C\n: specifies the genomic location to process  \n\n\n\n\n\n\nThere will not be any indication that it is running once you launch the\ncommand, to make sure it is running open a new Terminal tab with\n\nShift + Control + T\n (or from the menu with File then Open Tab) and type\nthe command \ntop\n. You should see the top line being the command \u2019pypy\u2019 with\na % CPU usage of 98/99%. Press \nq\n to quit out of this process view and go\nback to the tab running \nSequenza\n. \nIf everything is running correctly, it\nwill take approximately 40 minutes to run\n. Go have a coffee!\n\n\n\n\n\nOnce the command is done you will be returned to the terminal prompt.\nMake sure the output file is the correct size by typing \nls -lh\n from the\nTerminal window that you ran \nSequenza\n from, there should be a file\ncalled \nstage1.seqz.gz\n of the size ~395M.\n\n\nYou can look at the first few lines of the output in the file\n\nstage1.seqz.gz\n with:\n\n\n1\nzcat stage1.seqz.gz \n|\n head -n \n20\n\n\n\n\n\n\n\nThis output has one line for each position in the BAMs and includes\ninformation on the position, depths, allele frequencies, zygosity, GC in\nthe location.\n\n\nStep 2: Perform Binning\n\u00b6\n\n\nThe binning step takes the rows of genomic positions and compresses them\ndown to 1 row for every 200 rows previously. This massively reduces the\nfile size and processing time in the third step.\n\n\n1\n2\n3\npypy software/sequenza/sequenza-utils.py seqz-binning \\\n-w 200 \\\n-s stage1.seqz.gz | gzip > stage2.seqz.gz\n\n\n\n\n\n\nExplanation of parameters:\n\n\n\n\n-w\n: the window size (typically 50 for exomes, 200 for genomes)\n\n\n-s\n: the large seqz file generated in the first step\n\n\n\n\nThis step should take approximately 4 minutes to complete.\n\n\nStep 3: Running Sequenza Algorithms and Plotting Results\n\u00b6\n\n\nWe will now perform the CNV analysis and output the results using the \nR\n\npart of Sequenza.\n\n\nOpen the \nR\n terminal:\n\n\n1\nR\n\n\n\n\n\n\nYou should now see the \nR\n prompt identified with \u201c>\u201d.\n\n\nRun the Sequenza \nR\n commands:\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\n\"sequenza\"\n)\n\n\nsetwd\n(\n\"/home/trainee/cnv\"\n)\n\ndata.file \n<-\n \n\"stage2.seqz.gz\"\n\nseqzdata \n<-\n sequenza.extract\n(\ndata.file\n)\n\nCP.example \n<-\n sequenza.fit\n(\nseqzdata\n)\n\nsequenza.results\n(\nsequenza.extract \n=\n seqzdata\n,\n cp.table \n=\n CP.example\n,\n sample.id \n=\n \n\"CanGenWorkshop\"\n,\n out.dir\n=\n\"sequenza_results\"\n)\n\n\n\n\n\n\n\nQuit \nR\n:  \n\n\n1\nq\n()\n\n\n\n\n\n\n\nThen enter \nn\n at the \u201cSave workspace image\u201d prompt.\n\n\nIf every command ran successfully, you will now have a \nsequenza_results\n\nfolder containing 13 files (type \nls -l sequenza_results/\n).\n\n\n\n\nSequenza Analysis Results and Visualisation\n\u00b6\n\n\nOne of the first and most important estimates that Sequenza provides is\nthe tumour cellularity (the estimated percentage of tumour cells in the\ntumour genome). This estimate is based on the B allele frequency and\ndepth ratio through the genome and is an important metric to know for\ninterpretation of Sequenza results and for other analyses. Lets look at\nthe cellularity estimate for our analysis by opening\n\nCanGenWorkshop_model_fit.pdf\n with the command:\n\n\n1\nevince sequenza_results/CanGenWorkshop_model_fit.pdf\n\n\n\n\n\n\n\nThe cellularity estimate is at the top along with the average ploidy\nestimate and the standard deviation of the B allele frequency. We can\nsee that the cellularity has been estimated at 24% which is fairly low\nand we will see why this is bad in the next section on CNV\nvisualisation. The ploidy value of 2.1 indicates this piece of the\ngenome is not hugely amplified or deleted and the BAF standard deviation\nindicates there are no significant long losses of heterozygosity.\n\n\nClose the PDF window to resume the Terminal prompt.\n\n\nLet\u2019s now look at the CNV inferences through our genomic block. Open the\ngenome copy number visualisation file with:\n\n\n1\nevince sequenza_results/CanGenWorkshop_genome_view.pdf\n\n\n\n\n\n\n\nThis file contains three \u201cpages\u201d of copy number events through the\nentire genomic block.\n\n\n\n\nThe first page shows copy numbers of the A (red) and B (blue) alleles,\n\n\nThe second page shows overall copy number changes, and\n\n\nThe third page shows the B allele frequency and depth ratio through\ngenomic block.  \n\n\n\n\nLooking at the overall copy number changes, we see that\nour block is at a copy number of 2 with a small duplication to copy\nnumber 4 about \u2153 of the way through the block and another just after\nhalfway through the block. There is also a reduction in copy number to 1\ncopy about \u2158 of the way through the block. The gap that you see just\nbefore this reduction in copy number is the chromosomal centromere - an\narea that is notoriously difficult to sequence so always ends up in a\ngap with short read data.\n\n\nYou can see how this is a very easy to read output and lets you\nimmediately see the frequency and severity of copy number events through\nyour genome. Let\u2019s compare the small genomic block we ran with the same\noutput from the entire genome which has been pre-computed for you. This\nis located in the \npre_generated/results_whole_genome\n folder and\ncontains the same 13 output files as for the small genomic block. As\nbefore, let\u2019s look at the cellularity estimate with:\n\n\n1\nevince pre_generated/results_whole_genome/CanGenWorkshop_model_fit.pdf\n\n\n\n\n\n\n\nIt now looks like it\u2019s even worse at just 16%! A change is to be\nexpected as we were only analysing 1.9% of the genome. Let\u2019s now look at\nthe whole genome copy number profile with:\n\n\n1\nevince pre_generated/results_whole_genome/CanGenWorkshop_genome_view.pdf\n\n\n\n\n\n\n\nYou can see that there are a number of copy number events across the\ngenome and our genomic block (the first 60Mb of chromosome 5) is\ninferred as mostly copy number 4 followed by a reduction to copy number\n2, rather than 2 to 1 as we saw in the output we generated. The reason\nfor this is that \nSequenza\n uses the genome-wide depth ratio and BAF in\norder to estimate copy number, if you ask it to analyse a small block\nmostly at copy number 4 with a small reduction to copy number 2, the\nmost likely scenario in lieu of more data is that this is a copy number\n2 block with a reduction to 1. It\u2019s important to carefully examine the\ncellularity, ploidy and BAF estimates of your sample along with the\nplots of model fit (\nCanGenWorkshop_model_fit.pdf\n) and\ncellularity/ploidy contours (\nCanGenWorkshop_CP_contours.pdf\n) in order\nto decide if you believe Sequenza\u2019s inference of the copy numbers. Have\na look at these for yourself if you want to get a better idea of how\n\nSequenza\n makes its inferences and conclusions.\n\n\n\n\nCNV Visualisation/Confirmation in IGV\n\u00b6\n\n\nLet\u2019s see if we can visualise one of the CNV events where copy number\nincreased significantly. We\u2019ll focus on the copy number 4 event seen at\nabout \u2153 of the way through the \nCanGenWorkshop_genome_view.pdf\n output\nwe generated. First, we need to find the coordinates that have been\npredicted for this event. Have a look at the \nCanGenWorkshop_segments.txt\n\nfile in the results folder to view all predicted CNV events with:\n\n\n1\nless sequenza_results/CanGenWorkshop_segments.txt\n\n\n\n\n\n\n\nThere is only one at a copy number of 4 (CNt column) and it starts at\n21,051,700 to 21,522,065 which is 470kb and corresponds to the small block\nwe see in the genome view PDF.\n\n\nQuit out of viewing the segments file by pressing \nq\n.\n\n\nWe will now open \nIGV\n and see if we can observe the predicted increase in\ncopy number within these genomic coordinates.\n\n\n1\n/home/trainee/snv/Applications/igv/igv.sh\n\n\n\n\n\n\nIGV will take 30 seconds or so to open so just be patient.\n\n\n\nFor a duplication of this size, we will not be able to easily observe it\njust by looking at the raw read alignments. In order to see it we will\ngenerate two tiled data files (TDFs) within IGV which contain the\naverage read depth for a given window size through the genome. This\nmeans that we can aggregate the average read depth over relatively large\nchunks of the genome and compare these values between the normal and\ntumour genomes.\n\n\nTo begin, select the genome \nHuman (b37)\n, go to \nTools\n then \nRun igvtools...\n in the IGV\nmenubar. Specify the normal bam file (under \ncnv\n then \ndata\n) as the\ninput file and change the window size to 100,000 (one hundred thousand).\nThen press the \nRun\n button and IGV will make the TDF file. This takes\nabout 5 minutes. \nRepeat this for the tumour genome.\n\n\nAfter you have both TDF files, go to \nFile\n and \nLoad from file...\n in\nthe menubar and select the BAM and TDF files to open. Once you have\nopened them, they will appear as tracks along with the BAM tracks we\nloaded initially. Navigate to the genomic coordinates of our event\n(5:21,051,700-21,522,065) by typing it in the coordinate box at the top.\nMouse over the two blue tracks to get the average depth values for the\n100,000 bp windows. What you should see is that the liverMets sample has\n3-6X more coverage than the Blood sample for the four windows that cover\nthis region.\n\n\nThis may seem a bit underwhelming, after all, wasn\u2019t the increase of the\nregion to a copy number of 4, i.e. we expect a doubling of reads in the\ntumour? To explain why we are only seeing such a small coverage\nincrease, we need to turn to our good friend mathematics!\n\n\nImagine we have two 30X genomes for the normal and tumour samples and\nthe tumour is at 100% purity. If there is a copy number increase to 4 in\nthe tumour from 2 in the normal, the duplicated segment should indeed\nhave twice as many reads as the same segment in the normal genome. Now,\nlets imagine the tumour genome was only at a purity of 50% (i.e. it\ncontains 50% normal cells and 50% tumour cells). Now, half of the\nduplicated \u201ctumour genome\u201d segment will be at a copy number of 2 and\nhalf will be at 4. What does this mean when we sequence them as a\nmixture? The resulting average copy number of the block will be\n\n(0.5*2)+(0.5*4) = 3\n. Now what if we only have 16% tumour cells in our\n\u201ctumour genome\u201d? This will be \n(0.84*2)+(0.16*4) = 2.32\n. You can see\nhow sequencing a low cellularity tumour at a low depth makes it much\nharder to infer copy number variations!\n\n\nReturning to our genomes at hand, when we previously looked at the\ncellularity estimate of this tumour we saw it was 20% from the small\nblock we ran or 16% from the whole genome. Thus, the read depth increase\nof just 3-6X (about 10-20% more reads) in this segment is not\nsurprising. A low cellularity tumour greatly reduces our power to infer\ncopy number events as relatively small changes in depth can occur by\nchance in the genome and these can be mis-identified as copy number\nchanges. As well as this, it reduces our power for other analyses since\nwe must also remember that a tumour can itself contain multiple clones\nwhich have to share just 16% of reads.\n\n\nIt is possible to sequence through a low-cellularity sample when, for\nexample, there is no way to take another sample (as is the case of most\nbiopsies). \u201cSequencing through\u201d means to simply sequence the tumour at a\nmuch higher coverage, usually 90-120X. This will mean that there will be\na net increase in reads supplying evidence for copy number events and\nvariants and in aggregate these will still retain power to infer these\nevents when using tools that look at the whole genome like Sequenza\ndoes.\n\n\n\n\nReferences\n\u00b6\n\n\n\n\nF. Favero, T. Joshi, A. M. Marquard, N. J. Birkbak, M. Krzystanek,\n    Q. Li, Z. Szallasi, and A. C. Eklund. \u201cSequenza: allele-specific\n    copy number and mutation profiles from tumor sequencing data\u201d.\n    Annals of Oncology, 2015, vol. 26, issue 1, 64-70.",
            "title": "Copy Number Variation"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Understand and perform a simple copy number variation analysis on\n    NGS data    Become familiar with Sequenza    Understand the CNV inference process as an interplay between depth\n    of sequencing, cellularity and B-allele frequency    Visualize CNV events by manual inspection",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#tools-used",
            "text": "Sequenza:  http://www.cbs.dtu.dk/biotools/sequenza/  IGV:  http://www.broadinstitute.org/igv/",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#sources-of-data",
            "text": "Raw data download:  http://sra.dnanexus.com/studies/ERP001071  Data publication:  http://www.ncbi.nlm.nih.gov/pubmed/22194472",
            "title": "Sources of Data"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#author-information",
            "text": "Primary Author(s): \nVelimir Gayevskiy, Garvan Institute  v.gayevskiy@garvan.org.au \nSonika Tyagi, AGRF  sonika.tyagi@agrf.org.au     Contributor(s):",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#introduction",
            "text": "The goal of this hands-on session is to perform a copy number variation\nanalysis (CNV) on a normal/tumour pair of alignment files (BAMs)\nproduced by the mapping of Illumina short read sequencing data.  To ensure reasonable analysis times, we will perform the analysis on a\nheavily subsetted pair of BAM files. These files contain just the first\n60Mb of chromosome 5 but contain several examples of inferred copy\nnumber events to enable interpretation and visualisation of the copy\nnumber variation that is present in entire cancer genomes.  Sequenza  is\nthe tool we will use to perform this analysis. It consists of two Python  pre-processing steps followed by a third step in  R  to infer the\ndepth ratio, cellularity, ploidy and to plot the results for\ninterpretation.  In the second part of the tutorial we will also be using  IGV  to\nvisualise and manually inspect the copy number variation we inferred in\nthe first part for validation purposes. This section will also include a\ndiscussion on the importance of good quality data by highlighting the\ninadequacies of the workshop dataset and the implications this has on\nanalysis results.",
            "title": "Introduction"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#prepare-the-environment",
            "text": "We will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.  The data files are contained in the subdirectory called  data  and are\nthe following:   normal.chr5.60Mb.bam  and  normal.chr5.60Mb.bam.bai  tumour.chr5.60Mb.bam  and  tumour.chr5.60Mb.bam.bai   \nThese files are based on subsetting the whole genomes derived from  blood \nand  liver metastases  to the first 60Mb of chromosome 5. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that we are analysing just 1.9% of the genome which\nwill highlight the length of time and resources required to perform\ncancer genomics on full genomes!  Open the Terminal and go to the  cnv  working directory:  1 cd /home/trainee/cnv/    All commands entered into the terminal for this tutorial should be from\nwithin the  cnv  directory.   Check that the  data  directory contains the above-mentioned files by\ntyping:  1 ls -l data",
            "title": "Prepare the Environment"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#sequenza-cnv-analysis",
            "text": "Sequenza is run in three steps. The first pre-processing step is run on\nthe final normal and tumour mapped data (BAM files) in order to walk the\ngenome in a pileup format (automatically generated by  samtools ). This\nfirst step finds high quality sites in the genomes and extracts their\ndepth and genotype in the normal genome and calculates the variant\nalleles and allele frequencies in the tumour genome. The second step is\nto perform a binning on these sites to save space and analysis time in\nthe third step. Finally, the third step is run in  R  to normalise the depth\nratio between the normal/tumour genomes, infer cellularity and ploidy\nand graphically output results for interpretation.",
            "title": "Sequenza CNV Analysis"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#step-1-pre-processing-walking-the-genome",
            "text": "1\n2\n3\n4\n5\n6 pypy software/sequenza/sequenza-utils.py bam2seqz \\\n-n data/normal.chr5.60Mb.bam \\\n-t data/tumour.chr5.60Mb.bam \\\n--fasta assets/human_g1k_v37.fasta \\\n-gc assets/human_g1k_v37.gc50Base.txt.gz \\\n-C 5:1-60000000 | gzip > stage1.seqz.gz    Hint  Press tab after typing a few characters of a directory of filename to\nauto-complete the rest. This makes entering long file names very quick.   Explanation of parameters:   -n : the normal BAM  -t : the tumour BAM  --fasta : the reference genome used for mapping (b37 here)  -gc : GC content as windows through the genome (pre-generated and downloadable from the Sequenza website)  -C : specifies the genomic location to process      There will not be any indication that it is running once you launch the\ncommand, to make sure it is running open a new Terminal tab with Shift + Control + T  (or from the menu with File then Open Tab) and type\nthe command  top . You should see the top line being the command \u2019pypy\u2019 with\na % CPU usage of 98/99%. Press  q  to quit out of this process view and go\nback to the tab running  Sequenza .  If everything is running correctly, it\nwill take approximately 40 minutes to run . Go have a coffee!   \nOnce the command is done you will be returned to the terminal prompt.\nMake sure the output file is the correct size by typing  ls -lh  from the\nTerminal window that you ran  Sequenza  from, there should be a file\ncalled  stage1.seqz.gz  of the size ~395M.  You can look at the first few lines of the output in the file stage1.seqz.gz  with:  1 zcat stage1.seqz.gz  |  head -n  20    This output has one line for each position in the BAMs and includes\ninformation on the position, depths, allele frequencies, zygosity, GC in\nthe location.",
            "title": "Step 1: Pre-processing \u2013 Walking the Genome"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#step-2-perform-binning",
            "text": "The binning step takes the rows of genomic positions and compresses them\ndown to 1 row for every 200 rows previously. This massively reduces the\nfile size and processing time in the third step.  1\n2\n3 pypy software/sequenza/sequenza-utils.py seqz-binning \\\n-w 200 \\\n-s stage1.seqz.gz | gzip > stage2.seqz.gz   Explanation of parameters:   -w : the window size (typically 50 for exomes, 200 for genomes)  -s : the large seqz file generated in the first step   This step should take approximately 4 minutes to complete.",
            "title": "Step 2: Perform Binning"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#step-3-running-sequenza-algorithms-and-plotting-results",
            "text": "We will now perform the CNV analysis and output the results using the  R \npart of Sequenza.  Open the  R  terminal:  1 R   You should now see the  R  prompt identified with \u201c>\u201d.  Run the Sequenza  R  commands:  1\n2\n3\n4\n5\n6 library ( \"sequenza\" )  setwd ( \"/home/trainee/cnv\" ) \ndata.file  <-   \"stage2.seqz.gz\" \nseqzdata  <-  sequenza.extract ( data.file ) \nCP.example  <-  sequenza.fit ( seqzdata ) \nsequenza.results ( sequenza.extract  =  seqzdata ,  cp.table  =  CP.example ,  sample.id  =   \"CanGenWorkshop\" ,  out.dir = \"sequenza_results\" )    Quit  R :    1 q ()    Then enter  n  at the \u201cSave workspace image\u201d prompt.  If every command ran successfully, you will now have a  sequenza_results \nfolder containing 13 files (type  ls -l sequenza_results/ ).",
            "title": "Step 3: Running Sequenza Algorithms and Plotting Results"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#sequenza-analysis-results-and-visualisation",
            "text": "One of the first and most important estimates that Sequenza provides is\nthe tumour cellularity (the estimated percentage of tumour cells in the\ntumour genome). This estimate is based on the B allele frequency and\ndepth ratio through the genome and is an important metric to know for\ninterpretation of Sequenza results and for other analyses. Lets look at\nthe cellularity estimate for our analysis by opening CanGenWorkshop_model_fit.pdf  with the command:  1 evince sequenza_results/CanGenWorkshop_model_fit.pdf   \nThe cellularity estimate is at the top along with the average ploidy\nestimate and the standard deviation of the B allele frequency. We can\nsee that the cellularity has been estimated at 24% which is fairly low\nand we will see why this is bad in the next section on CNV\nvisualisation. The ploidy value of 2.1 indicates this piece of the\ngenome is not hugely amplified or deleted and the BAF standard deviation\nindicates there are no significant long losses of heterozygosity.  Close the PDF window to resume the Terminal prompt.  Let\u2019s now look at the CNV inferences through our genomic block. Open the\ngenome copy number visualisation file with:  1 evince sequenza_results/CanGenWorkshop_genome_view.pdf   \nThis file contains three \u201cpages\u201d of copy number events through the\nentire genomic block.   The first page shows copy numbers of the A (red) and B (blue) alleles,  The second page shows overall copy number changes, and  The third page shows the B allele frequency and depth ratio through\ngenomic block.     Looking at the overall copy number changes, we see that\nour block is at a copy number of 2 with a small duplication to copy\nnumber 4 about \u2153 of the way through the block and another just after\nhalfway through the block. There is also a reduction in copy number to 1\ncopy about \u2158 of the way through the block. The gap that you see just\nbefore this reduction in copy number is the chromosomal centromere - an\narea that is notoriously difficult to sequence so always ends up in a\ngap with short read data.  You can see how this is a very easy to read output and lets you\nimmediately see the frequency and severity of copy number events through\nyour genome. Let\u2019s compare the small genomic block we ran with the same\noutput from the entire genome which has been pre-computed for you. This\nis located in the  pre_generated/results_whole_genome  folder and\ncontains the same 13 output files as for the small genomic block. As\nbefore, let\u2019s look at the cellularity estimate with:  1 evince pre_generated/results_whole_genome/CanGenWorkshop_model_fit.pdf   \nIt now looks like it\u2019s even worse at just 16%! A change is to be\nexpected as we were only analysing 1.9% of the genome. Let\u2019s now look at\nthe whole genome copy number profile with:  1 evince pre_generated/results_whole_genome/CanGenWorkshop_genome_view.pdf   \nYou can see that there are a number of copy number events across the\ngenome and our genomic block (the first 60Mb of chromosome 5) is\ninferred as mostly copy number 4 followed by a reduction to copy number\n2, rather than 2 to 1 as we saw in the output we generated. The reason\nfor this is that  Sequenza  uses the genome-wide depth ratio and BAF in\norder to estimate copy number, if you ask it to analyse a small block\nmostly at copy number 4 with a small reduction to copy number 2, the\nmost likely scenario in lieu of more data is that this is a copy number\n2 block with a reduction to 1. It\u2019s important to carefully examine the\ncellularity, ploidy and BAF estimates of your sample along with the\nplots of model fit ( CanGenWorkshop_model_fit.pdf ) and\ncellularity/ploidy contours ( CanGenWorkshop_CP_contours.pdf ) in order\nto decide if you believe Sequenza\u2019s inference of the copy numbers. Have\na look at these for yourself if you want to get a better idea of how Sequenza  makes its inferences and conclusions.",
            "title": "Sequenza Analysis Results and Visualisation"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#cnv-visualisationconfirmation-in-igv",
            "text": "Let\u2019s see if we can visualise one of the CNV events where copy number\nincreased significantly. We\u2019ll focus on the copy number 4 event seen at\nabout \u2153 of the way through the  CanGenWorkshop_genome_view.pdf  output\nwe generated. First, we need to find the coordinates that have been\npredicted for this event. Have a look at the  CanGenWorkshop_segments.txt \nfile in the results folder to view all predicted CNV events with:  1 less sequenza_results/CanGenWorkshop_segments.txt   \nThere is only one at a copy number of 4 (CNt column) and it starts at\n21,051,700 to 21,522,065 which is 470kb and corresponds to the small block\nwe see in the genome view PDF.  Quit out of viewing the segments file by pressing  q .  We will now open  IGV  and see if we can observe the predicted increase in\ncopy number within these genomic coordinates.  1 /home/trainee/snv/Applications/igv/igv.sh   IGV will take 30 seconds or so to open so just be patient.  \nFor a duplication of this size, we will not be able to easily observe it\njust by looking at the raw read alignments. In order to see it we will\ngenerate two tiled data files (TDFs) within IGV which contain the\naverage read depth for a given window size through the genome. This\nmeans that we can aggregate the average read depth over relatively large\nchunks of the genome and compare these values between the normal and\ntumour genomes.  To begin, select the genome  Human (b37) , go to  Tools  then  Run igvtools...  in the IGV\nmenubar. Specify the normal bam file (under  cnv  then  data ) as the\ninput file and change the window size to 100,000 (one hundred thousand).\nThen press the  Run  button and IGV will make the TDF file. This takes\nabout 5 minutes.  Repeat this for the tumour genome.  After you have both TDF files, go to  File  and  Load from file...  in\nthe menubar and select the BAM and TDF files to open. Once you have\nopened them, they will appear as tracks along with the BAM tracks we\nloaded initially. Navigate to the genomic coordinates of our event\n(5:21,051,700-21,522,065) by typing it in the coordinate box at the top.\nMouse over the two blue tracks to get the average depth values for the\n100,000 bp windows. What you should see is that the liverMets sample has\n3-6X more coverage than the Blood sample for the four windows that cover\nthis region.  This may seem a bit underwhelming, after all, wasn\u2019t the increase of the\nregion to a copy number of 4, i.e. we expect a doubling of reads in the\ntumour? To explain why we are only seeing such a small coverage\nincrease, we need to turn to our good friend mathematics!  Imagine we have two 30X genomes for the normal and tumour samples and\nthe tumour is at 100% purity. If there is a copy number increase to 4 in\nthe tumour from 2 in the normal, the duplicated segment should indeed\nhave twice as many reads as the same segment in the normal genome. Now,\nlets imagine the tumour genome was only at a purity of 50% (i.e. it\ncontains 50% normal cells and 50% tumour cells). Now, half of the\nduplicated \u201ctumour genome\u201d segment will be at a copy number of 2 and\nhalf will be at 4. What does this mean when we sequence them as a\nmixture? The resulting average copy number of the block will be (0.5*2)+(0.5*4) = 3 . Now what if we only have 16% tumour cells in our\n\u201ctumour genome\u201d? This will be  (0.84*2)+(0.16*4) = 2.32 . You can see\nhow sequencing a low cellularity tumour at a low depth makes it much\nharder to infer copy number variations!  Returning to our genomes at hand, when we previously looked at the\ncellularity estimate of this tumour we saw it was 20% from the small\nblock we ran or 16% from the whole genome. Thus, the read depth increase\nof just 3-6X (about 10-20% more reads) in this segment is not\nsurprising. A low cellularity tumour greatly reduces our power to infer\ncopy number events as relatively small changes in depth can occur by\nchance in the genome and these can be mis-identified as copy number\nchanges. As well as this, it reduces our power for other analyses since\nwe must also remember that a tumour can itself contain multiple clones\nwhich have to share just 16% of reads.  It is possible to sequence through a low-cellularity sample when, for\nexample, there is no way to take another sample (as is the case of most\nbiopsies). \u201cSequencing through\u201d means to simply sequence the tumour at a\nmuch higher coverage, usually 90-120X. This will mean that there will be\na net increase in reads supplying evidence for copy number events and\nvariants and in aggregate these will still retain power to infer these\nevents when using tools that look at the whole genome like Sequenza\ndoes.",
            "title": "CNV Visualisation/Confirmation in IGV"
        },
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#references",
            "text": "F. Favero, T. Joshi, A. M. Marquard, N. J. Birkbak, M. Krzystanek,\n    Q. Li, Z. Szallasi, and A. C. Eklund. \u201cSequenza: allele-specific\n    copy number and mutation profiles from tumor sequencing data\u201d.\n    Annals of Oncology, 2015, vol. 26, issue 1, 64-70.",
            "title": "References"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nBy the end of the structural variant (SV) detection practical course\nparticipants will:\n\n\n\n\n\n\nHave been provided with key fundamentals on how paired-end mappings\n    and split-read/soft-clipped read patterns are used in detecting\n    deletions, tandem duplicates, inversions and translocations.\n\n\n\n\n\n\nKnow what important quality control checks need to be evaluated\n    prior to structural variant calling.\n\n\n\n\n\n\nHave run \nDELLY\n on a subset of whole genome next generation\n    sequencing data pertaining to a single human tumour with a matched\n    normal control.\n\n\n\n\n\n\nBe able to filter high confidence SV predictions.\n\n\n\n\n\n\nHave gained basic knowledge to interpret the VCF output provided by\n    DELLY.\n\n\n\n\n\n\nHave used their understanding of distinct SV paired-end mapping and\n    soft-clipped read patterns to visually verify \nDELLY\n predicted SVs\n    using \nIGV\n.\n\n\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nDELLY:\n\n\nhttps://github.com/tobiasrausch/delly\n\n\nSamtools:\n\n\nhttp://sourceforge.net/projects/samtools/files/samtools/1.2\n\n\nTabix:\n\n\nhttp://sourceforge.net/projects/samtools/files/tabix/tabix-0.2.6.tar.bz2\n\n\nVcftools:\n\n\nhttps://vcftools.github.io/index.html\n\n\nPicard:\n\n\nhttps://broadinstitute.github.io/picard/\n\n\nPython2.7.10:\n\n\nhttps://www.python.org/downloads/release/python-2710/\n\n\nPyVCF Banyan numpy:\n\n\nhttps://pypi.python.org/pypi\n\n\nUseful Links\n\u00b6\n\n\nSAM Specification:\n\n\nhttp://samtools.sourceforge.net/SAM1.pdf\n\n\nExplain SAM Flags:\n\n\nhttps://broadinstitute.github.io/picard/explain-flags.html\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n  \n\nErdahl Teber \neteber@cmri.org.au\n\nAnn-Marie Patch \nAnn-Marie.Patch@qimrberghofer.edu.au\n\n\nContributor(s):\n  \n\nSonika Tyagi \nsonika.tyagi@agrf.org.au\n\n\n\n\nAlignment Quality Control\n\u00b6\n\n\nFor structural variant calling several alignment quality control metrics\nshould be evaluated. All paired-end mapping methods heavily rely on the\ninsert size distribution. GC-content biases are important as it can\nimpact read-depths. \nDELLY\n generates read-depth ratios between tumour and\ncontrol samples. The percentage of mapped reads, singletons, duplicates\nand properly paired reads are additional metrics you should evaluate\nprior to any structural variant calling. These statistics vary largely\nby protocol and hence, it is usually best to compare multiple different\nsequencing runs using the same against each other to highlight outliers.\n\n\nIt is recommended that Picard module commands \nCollectInsertSizeMetrics\n\nand \nCollectGcBiasMetrics\n, and \nsamtools flagstat\n command be used.\n\n\n\n\nPrepare the Environment\n\u00b6\n\n\nAs a quick introduction we will do a structural variant analysis using a\nsingle immortal cancer cell line and its control genome (mortal parental\ncells). Total execution time to run the \nDELLY\n structural discovery\ncalling program for a matched normal tumour pair will vary depending on\nthe read coverage and the size of the genome. As a guide, it can take\napproximately 10 to 50 hours (translocation predictions taking the\nlongest), for a matched normal tumour pair (each 40-50x coverage)\nrunning on 2 cpus on a server with sufficient RAM.\n\n\nThe bam files we will be working on are a subset of the original WGS bam\nfiles, limited to specific chromosomal regions to speed up the analysis\nand to meet the time constraints for this practical.\n\n\nFirstly, we will use shell variables to help improve the readability of\ncommands and streamline scripting. Each distinct variable will store a\ndirectory path to either, the input WGS bam files, hg19 reference,\nprograms or output.\n\n\nOpen the Terminal.\n\n\nFirst, go to the right folder, where the data are stored.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ncd\n /home/trainee/sv\nls\nmkdir <YourFirstName>\n\ncd\n <YourFirstName>\n\n\nexport\n \nDS\n=\n/home/trainee/sv/data\n\nexport\n \nRF\n=\n/home/trainee/sv/reference_data\n\nexport\n \nSF\n=\n/home/trainee/sv/variantFiltering/somaticVariants\n\nexport\n \nBR\n=\n/home/trainee/snv/Applications/igv\n\nexport\n \nCZ\n=\n/home/trainee/sv/converter\n\n\n\n\n\n\n\n\nSomatic Structural Variant Discovery using DELLY\n\u00b6\n\n\nIn order to generate \nputative\n somatic SVs it is crucial to account for\ngermline SVs. To facilitate, DELLY requires the joint input of a match\nnormal control and the cancer aligned sequencing data (bam files).\n\n\n1\n2\n3\n4\ndelly -t DEL -x $RF/hg19.excl -o del.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t DUP -x $RF/hg19.excl -o dup.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t INV -x $RF/hg19.excl -o inv.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t TRA -x $RF/hg19.excl -o tra.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\n\n\n\n\n\n\nDescription of the arguments used in the command:\n\n\n\n\nDEL\n: conduct deletion discovery\n\n\nDUP\n: conduct tandem duplication discovery\n\n\nINV\n: conduct inversion discovery\n\n\nTRA\n: conduct translocation discovery\n\n\n-o\n: vcf output\n\n\n-g\n: reference genome in FASTA format\n\n\n-x\n: genomic regions to exclude (e.g. centro- and telomeric regions)  \n\n\n\n\n\n\nDELLY VCF output\n\u00b6\n\n\nA VCF file has multiple header lines starting with the hash \n#\n sign.\nThere is one record for each unique structural variant. The\nrecord format is described in the table below:\n\n\n\n\n\n\n\n\nColumn\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCHROM\n\n\nChromosome name\n\n\n\n\n\n\n2\n\n\nPOS\n\n\n1-based position. For an indel, this is the position preceding the indel\n\n\n\n\n\n\n3\n\n\nID\n\n\nVariant identifier\n\n\n\n\n\n\n4\n\n\nREF\n\n\nReference sequence at POS involved in the variant\n\n\n\n\n\n\n5\n\n\nALT\n\n\nComma delimited list of alternative sequence(s)\n\n\n\n\n\n\n6\n\n\nQUAL\n\n\nPhred-scaled probability of all samples being homozygous reference\n\n\n\n\n\n\n7\n\n\nFILTER\n\n\nSemicolon delimited list of filters that the variant fails to pass\n\n\n\n\n\n\n8\n\n\nINFO\n\n\nSemicolon delimited list of variant information\n\n\n\n\n\n\n9\n\n\nFORMAT\n\n\nColon delimited list of the format of sample genotypes in subsequent fields\n\n\n\n\n\n\n10+\n\n\n\n\nIndividual genotype information defined by FORMAT\n\n\n\n\n\n\n\n\nYou can look at the header of the vcf file and the first structural variant record in the file using the below command (-A is the option which prints the specified N lines after the match):\n\n\n1\ngrep \"^#\" -A 1 del.vcf\n\n\n\n\n\n\n\nThe INFO field holds structural variant site information whereas all\ngenotype information (annotated as per the FORMAT fields) is provided in\nthe sample column. Reference supporting reads are compared to\nalternative supporting reads and mapping qualities are used to compute\ngenotype likelihoods (GL) for homozygous reference (0/0), heterozygous\nreference (0/1) and homozygous alternate (1/1) (GT). The final genotype\n(GT) is simply derived from the best GL and GQ is a phred-scaled\ngenotype quality reflecting the confidence in this genotype. If GQ<15\nthe genotype is flagged as LowQual. The genotyping takes into account\nall paired-ends with a mapping quality greater than 20 by default.\n\n\nThe INFO field provides information on the quality of the SV prediction\nand breakpoints. If you browse through the vcf file you will notice that\na subset of the DELLY structural variant predictions have been refined\nusing split-reads. These precise variants are flagged in the vcf info\nfield with the tag \nPRECISE\n. To count the number of precise and\nimprecise variants you can simply use \ngrep\n.\n\n\n1\n2\ngrep -c -w \n\"PRECISE\"\n *.vcf\ngrep -c -w \n\"IMPRECISE\"\n *.vcf\n\n\n\n\n\n\n\nDELLY clusters abnormal paired-ends and every single cluster gives rise\nto an \nIMPRECISE\n SV call. For every \nIMPRECISE\n SV call an attempt is made\nto identify supporting split-reads/soft-clipped reads. DELLY then\ncomputes a consensus sequence (INFO:CONSENSUS) out of all split-read\ncandidates and then aligns this consensus sequence to the reference\nrequiring at least -m many aligned bases to the left and right (default\nis 13). INFO:PE is the number of supporting paired-ends. INFO:CT refers\nconnection types (CT), which indicates the order and orientation of\npaired-end cluster mappings (e.g. 3to3 for 3\u2019 to 3\u2019 and 5to5 for 5\u2019 to\n5\u2019). Values can be 3to5, 5to3, 3to3 or 5to5. Different names exist for\nthese connection types in the literature, head-to-head inversions,\ntail-to-tail inversions, and so on. The consensus alignment quality\n(SRQ) is a score between 0 and 1, where 1 indicates 100% identity to the\nreference. Nearby SNPs, InDels and micro-insertions at the breakpoint\ncan lower this score but only for mis-assemblies it should be very poor.\nDELLY currently drops consensus alignments with a score <0.8 and then\nfalls back to an \nIMPRECISE\n prediction.\n\n\nSVs are flagged as FILTER:LowQual if PE <3 OR MAPQ <20 (for\ntranslocations: PE <5 OR MAPQ <20), otherwise, the SV results in a\nFILTER:PASS. \nPRECISE\n variants will have split-read support (SR >0).\n\n\n\n\nSomatic Structural Variant Filtering\n\u00b6\n\n\nPlease note that this vcf file contains germline and somatic structural\nvariants but also false positives caused by repeat induced mis-mappings\nor incomplete reference sequences. As a final step we have to use the\nstructural variant site information and the cancer and normal genotype\ninformation to filter a set of confident somatic structural variants.\nDELLY ships with a somatic filtering python script. For a set of\nconfident somatic calls one could exclude all structural variants\n<400bp, require a minimum variant allele frequency of 10%, no support\nin the matched normal and an overall confident structural variant site\nprediction with the VCF filter field being equal to PASS.\n\n\n1\n2\n3\n4\npython\n \n$\nSF\n/\nsomaticFilter\n.\npy\n \n-\nt\n \nDEL\n  \n-\nT\n \ncancer_cell_line\n \n-\nN\n \ncontrol\n \n-\nv\n \ndel\n.\nvcf\n \n-\no\n \ndel\n.\nfilt\n.\nvcf\n \n-\na\n \n0.1\n \n-\nm\n \n400\n \n-\nf\n\n\npython\n \n$\nSF\n/\nsomaticFilter\n.\npy\n \n-\nt\n \nDUP\n \n-\nT\n \ncancer_cell_line\n \n-\nN\n \ncontrol\n \n-\nv\n \ndup\n.\nvcf\n \n-\no\n \ndup\n.\nfilt\n.\nvcf\n \n-\na\n \n0.1\n \n-\nm\n \n400\n \n-\nf\n\n\npython\n \n$\nSF\n/\nsomaticFilter\n.\npy\n \n-\nt\n \nINV\n  \n-\nT\n \ncancer_cell_line\n \n-\nN\n \ncontrol\n \n-\nv\n \ninv\n.\nvcf\n \n-\no\n \ninv\n.\nfilt\n.\nvcf\n \n-\na\n \n0.1\n \n-\nm\n \n400\n \n-\nf\n\n\npython\n \n$\nSF\n/\nsomaticFilter\n.\npy\n \n-\nt\n \nTRA\n \n-\nT\n \ncancer_cell_line\n \n-\nN\n \ncontrol\n \n-\nv\n \ntra\n.\nvcf\n \n-\no\n \ntra\n.\nfilt\n.\nvcf\n \n-\na\n \n0.1\n \n-\nm\n \n400\n \n-\nf\n\n\n\n\n\n\n\n\nUsing \nVCFtools\n we can merge all somatic structural variants together in\na single vcf file.\n\n\n1\nvcf-concat del.filt.vcf dup.filt.vcf inv.filt.vcf tra.filt.vcf | vcf-sort > somatic.sv.vcf\n\n\n\n\n\n\n\nFor large VCF files you should also zip and index them using \nbgzip\n and\n\ntabix\n. Please run the below commands to meet the requirements for\nvisualising somatic structural variants using \nIGV\n.\n\n\n1\n2\nbgzip -c somatic.sv.vcf > somatic.sv.vcf.gz\ntabix somatic.sv.vcf.gz\n\n\n\n\n\n\n\n\nVisualisation of Somatic Structural Variants\n\u00b6\n\n\nThe final step will be to browse some of these somatic structural\nvariants in IGV and to visually verify the reliability of the calls. To make it easy\nto navigate through our breakpoints of interest we will create a bed file (0-index co-ordinate format file).  \n\n\n1\n2\n$CZ/sv.vcf2bed.sh somatic.sv.vcf.gz > somatic.sv.bed\nhead somatic.sv.bed\n\n\n\n\n\n\n\nLoad the IGV browser\n\n\n1\n$BR/igv.sh &\n\n\n\n\n\n\n\nOnce IGV has started use \nFile\n and \nLoad from File\n (directory /home/trainee/sv/data) to load the\n\ncancer_cell_line.bam\n and the \ncontrol.bam\n. Go to \nView\n menu and\nselect \nPreferences\n, then click on the Alignments tab\nand in the \nVisibility range threshold (kb)\n text box, enter 600. This\nwill allow you to increase your visibility of pile ups as you zoom out.\nNow look for check box for \nFilter secondary alignments\n.\nEnsure box is ticked so that you do not see secondary alignments (alternate mapped position of\na read). Also ensure that \nShow soft-clipped bases\n has been checked\nthen click \nOK\n.\n\n\nThen import \nsomatic.sv.bed\n from your working directory using \nRegions\n\nand \nImport Regions\n.\n\n\nVerify Deletion\n\u00b6\n\n\n\n\nThis is an advanced section.\n  \n\n\n\n\nThe somatic structural variants can then be browsed easily using the\n\nRegion Navigator\n. Select the deletion (chrX:76853017-77014863) from\nthe \nRegion Navigator\n and click \nView\n. This will centre the IGV\nalignment view on the selected structural variant. Close the regions of\ninterest pop up window by right clicking mouse at the top of pop up and then choose close. The red bar below the ruler marks the region of\nthe deletion.\n\n\nIt\u2019s usually best to zoom out once by clicking on the \n-\n sign in the\ntoolbar at the top, to give a wide view of the supporting abnormal\npaired-end read mappings.\n\n\nTo highlight the abnormal paired-ends right click on the main track\ndisplay panel and select \nColor alignments by\n and then switch to\n\ninsert size and pair orientation\n.\n\n\nRead pairs that have a larger than expected insert size will be\nhighlighted in red. Click \nView as pairs\n. Right click and \nSortalignments by\n then select \nstart location\n.\n\n\n\n\n\n\nQuestion\n\n\nHow many abnormal \npaired-end read pairs\n (red coloured F/R oriented read pairs) can you see that spans the deletion region? Does this number coincide with the INFO:PE?\n\n\nHint\n1\ncat somatic.sv.vcf | grep \"<DEL>\" | cut -f1,2,8\n\n\n\n\n\n\n\nAnswer\n19, YES\n\n\n\n\n\n\nQuestion\n\n\nZoom into left and right breakpoint separately and tally the number of soft-clipped reads (count the soft-clipped reads with >24 mismatched reads).\nHow many abnormal split-reads (soft-clipped reads) did\nyou observe? Clue INFO:SR.\n\n\nAnswer\n11\n\n\n\n\n\n\nQuestion\n\n\nGo to the RefSeq genes track at the bottom of \nIGV\n and right click to\n\nExpanded\n. Is the predicted deletion likely to have a deleterious\nimpact on a gene? If so, what gene and exons are deleted?\n\n\nAnswer\nATRX, exons 2 to 25.\n\n\n\n\n\n\nQuestion\n\n\nDoes this region appear to be completely removed from this cancer\ngenome? How can you tell?\n\n\nAnswer\nYes, there is no read coverage within this deletion region relative to\nthe control genome.\n\n\n\n\nVerify translocation\n\u00b6\n\n\n\n\nThis is an advanced section.\n\n\n\n\n\n\n\n\nRemove \ncontrol.bam\n and the coverage track by right clicking on the track panel and selecting remove track.\n\n\n\n\n\n\nSelect the translocation breakpoint chr18 from the Region Navigator. Highlight the abnormal paired-ends by clicking and selecting\n\nColor alignments by\n and then switch to \ninsert size and pair orientation\n. Invoke \nSort alignments by\n then select \nstart location\n.\n\n\n\n\n\n\nZoom out until you can see all the purple reads at the junction.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat is the direction of the purple cluster of reads (indicates that\nmate reads are mapped to Chr15)? Is it pointing to the tail or head of\nChr18?\n\n\nAnswer\nForward, towards the tail, or 3\u2019 (+ive)\n\n\n\n\n\n\nQuestion\n\n\nRight click on to one of the purple coloured reads and select \nView mate region in split-screen\n.\nThis will split the screen and display Chr15 on the left and place a red\nhighlighted outline on both reads, to indicate the pairs. Select \nview as pairs\n,\nthen sort alignments by start location. To control the zooming on each of the chromosome panels,\nfirst click inside of the track panel of your chromosome of interest, then to zoom in\n(\nShift\n and \n+\n key together) or out (press \nCtrl\n and \n-\n key together).\n\n\nWhat is the direction of the yellow cluster of reads (indicates that mate\nreads are mapped to Chr18)? Is it pointing to the tail or head of Chr15?\nIf you wish to zoom in or out, first click inside of the chromosome ideogram\npanel, then ctrl- to zoom out and shift+ to zoom in.\n\n\nAnswer\nReverse, towards the head, or 5\u2019 (+ive)\n\n\n\n\n\n\nQuestion\n\n\nHow is the Chr15 and Chr18 fused (which one of the four translocation\nconnection types)? If you are uncertain then run a BLAT (\nhttps://genome.ucsc.edu/cgi-bin/hgBlat?command=start\n)\nsearch using the INFO:CONSENSUS sequence.\n\n\nHint\n1\ncat somatic.sv.vcf | grep \"<TRA>\" | cut -f1,2,8\n\n\n\n\n\n\n\nAnswer\nRF, head to tail, or 5 to 3. Therefore, Chr18 left side is fused to\nChr15 right side.\n\n\n\n\n\n\nQuestion\n\n\nDid DELLY predict a reciprocal translocation? How can you tell?\n\n\nAnswer\nNo, as we would expect to observe a Chr18 right side fused to Chr15 left\nside, near the same breakpoints.\n\n\n\n\n\n\nQuestion\n\n\nWhat gene structures is this translocation predicted to impact?\n\n\nAnswer\nADAMTSL3 on Chr15 and PARD6G on Chr18.\n\n\n\n\n\n\nQuestion\n\n\nWhat is one possible reason why there is no observable read coverage\nafter Chr18 breakpoint?\n\n\nAnswer\nChromosome loss.\n\n\n\n\nVerify tandem duplication\n\u00b6\n\n\n\n\nThis is an advanced section.\n\n\n\n\n\n\n\n\nSelect the tandem duplication (chrX:45649874-45689322) from the \nRegionNavigator\n.\n\n\n\n\n\n\nHighlight the abnormal paired-ends by clicking and selecting \nColor alignments by\n\nand then switch to \ninsert size and pair orientation\n.\nAlso, invoke \nSort alignments by\n then select \nstart location\n.\n\n\n\n\n\n\nZoom out until you can see all the red paired-end reads spanning the two\njunctions. After that zoom in on the cluster of abnormal reads on the\nleft junction and then right junction.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhich is the order and orientation of these paired-end reads (FR, RF, FF\nor RR)?\n\n\nAnswer\nRF\n\n\n\n\n\n\nQuestion\n\n\nWhat is the estimated read-depth ratio of the cancer_cell_line versus\nnormal control (INFO:RDRATIO) over the duplicate region?\n\n\nHint\n1\ncat somatic.sv.vcf | grep \"<DUP>\" | cut -f1,2,8\n\n\n\n\n\n\n\nAnswer\n3.3 (\u00a03 x increased read depth)\n\n\n\n\nVerify Inversion\n\u00b6\n\n\n\n\nThis is an advanced section.\n\n\n\n\n\n\n\n\nType into the search box near at tool bar, Chr20:54834492\n\n\n\n\n\n\nRight click on main display and select \nGroup alignments by\n then switch\non \npaired-orientation\n. Also, right click and \nSort alignments by\n then\nselect \nstart location\n.\n\n\n\n\n\n\nZoom out until you can see all the red coloured cluster of reads near\nthe breakpoint.\n\n\n\n\n\n\nRight click on to one of the red coloured reads and select \nView mate region in split-screen\n.\nThis will split the screen and display the read mate on the right side.\nThis will take you to the mate-reads near the second breakpoint.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhich direction are the paired-end reads spanning (left or right spanning)?\n\n\nAnswer\nRight\n\n\n\n\n\n\nQuestion\n\n\nWhat is the estimated size of the inverted interval?\n\n\nAnswer\n55,408,660 \u2013 54,834,492 = 574,168 bp\n\n\n\n\n\n\nAcknowledgements\n\u00b6\n\n\nWe would like to thank and acknowledge Tobias Rausch (EMBL Heidelberg)\nfor his help and for allowing us to borrow and adapt his replies to\nquestions and original course material.",
            "title": "Structural Variant Analysis"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#key-learning-outcomes",
            "text": "By the end of the structural variant (SV) detection practical course\nparticipants will:    Have been provided with key fundamentals on how paired-end mappings\n    and split-read/soft-clipped read patterns are used in detecting\n    deletions, tandem duplicates, inversions and translocations.    Know what important quality control checks need to be evaluated\n    prior to structural variant calling.    Have run  DELLY  on a subset of whole genome next generation\n    sequencing data pertaining to a single human tumour with a matched\n    normal control.    Be able to filter high confidence SV predictions.    Have gained basic knowledge to interpret the VCF output provided by\n    DELLY.    Have used their understanding of distinct SV paired-end mapping and\n    soft-clipped read patterns to visually verify  DELLY  predicted SVs\n    using  IGV .",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#tools-used",
            "text": "DELLY:  https://github.com/tobiasrausch/delly  Samtools:  http://sourceforge.net/projects/samtools/files/samtools/1.2  Tabix:  http://sourceforge.net/projects/samtools/files/tabix/tabix-0.2.6.tar.bz2  Vcftools:  https://vcftools.github.io/index.html  Picard:  https://broadinstitute.github.io/picard/  Python2.7.10:  https://www.python.org/downloads/release/python-2710/  PyVCF Banyan numpy:  https://pypi.python.org/pypi",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#useful-links",
            "text": "SAM Specification:  http://samtools.sourceforge.net/SAM1.pdf  Explain SAM Flags:  https://broadinstitute.github.io/picard/explain-flags.html",
            "title": "Useful Links"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#author-information",
            "text": "Primary Author(s):    \nErdahl Teber  eteber@cmri.org.au \nAnn-Marie Patch  Ann-Marie.Patch@qimrberghofer.edu.au  Contributor(s):    \nSonika Tyagi  sonika.tyagi@agrf.org.au",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#alignment-quality-control",
            "text": "For structural variant calling several alignment quality control metrics\nshould be evaluated. All paired-end mapping methods heavily rely on the\ninsert size distribution. GC-content biases are important as it can\nimpact read-depths.  DELLY  generates read-depth ratios between tumour and\ncontrol samples. The percentage of mapped reads, singletons, duplicates\nand properly paired reads are additional metrics you should evaluate\nprior to any structural variant calling. These statistics vary largely\nby protocol and hence, it is usually best to compare multiple different\nsequencing runs using the same against each other to highlight outliers.  It is recommended that Picard module commands  CollectInsertSizeMetrics \nand  CollectGcBiasMetrics , and  samtools flagstat  command be used.",
            "title": "Alignment Quality Control"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#prepare-the-environment",
            "text": "As a quick introduction we will do a structural variant analysis using a\nsingle immortal cancer cell line and its control genome (mortal parental\ncells). Total execution time to run the  DELLY  structural discovery\ncalling program for a matched normal tumour pair will vary depending on\nthe read coverage and the size of the genome. As a guide, it can take\napproximately 10 to 50 hours (translocation predictions taking the\nlongest), for a matched normal tumour pair (each 40-50x coverage)\nrunning on 2 cpus on a server with sufficient RAM.  The bam files we will be working on are a subset of the original WGS bam\nfiles, limited to specific chromosomal regions to speed up the analysis\nand to meet the time constraints for this practical.  Firstly, we will use shell variables to help improve the readability of\ncommands and streamline scripting. Each distinct variable will store a\ndirectory path to either, the input WGS bam files, hg19 reference,\nprograms or output.  Open the Terminal.  First, go to the right folder, where the data are stored.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 cd  /home/trainee/sv\nls\nmkdir <YourFirstName> cd  <YourFirstName> export   DS = /home/trainee/sv/data export   RF = /home/trainee/sv/reference_data export   SF = /home/trainee/sv/variantFiltering/somaticVariants export   BR = /home/trainee/snv/Applications/igv export   CZ = /home/trainee/sv/converter",
            "title": "Prepare the Environment"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#somatic-structural-variant-discovery-using-delly",
            "text": "In order to generate  putative  somatic SVs it is crucial to account for\ngermline SVs. To facilitate, DELLY requires the joint input of a match\nnormal control and the cancer aligned sequencing data (bam files).  1\n2\n3\n4 delly -t DEL -x $RF/hg19.excl -o del.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t DUP -x $RF/hg19.excl -o dup.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t INV -x $RF/hg19.excl -o inv.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t TRA -x $RF/hg19.excl -o tra.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam   Description of the arguments used in the command:   DEL : conduct deletion discovery  DUP : conduct tandem duplication discovery  INV : conduct inversion discovery  TRA : conduct translocation discovery  -o : vcf output  -g : reference genome in FASTA format  -x : genomic regions to exclude (e.g. centro- and telomeric regions)",
            "title": "Somatic Structural Variant Discovery using DELLY"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#delly-vcf-output",
            "text": "A VCF file has multiple header lines starting with the hash  #  sign.\nThere is one record for each unique structural variant. The\nrecord format is described in the table below:     Column  Field  Description      1  CHROM  Chromosome name    2  POS  1-based position. For an indel, this is the position preceding the indel    3  ID  Variant identifier    4  REF  Reference sequence at POS involved in the variant    5  ALT  Comma delimited list of alternative sequence(s)    6  QUAL  Phred-scaled probability of all samples being homozygous reference    7  FILTER  Semicolon delimited list of filters that the variant fails to pass    8  INFO  Semicolon delimited list of variant information    9  FORMAT  Colon delimited list of the format of sample genotypes in subsequent fields    10+   Individual genotype information defined by FORMAT     You can look at the header of the vcf file and the first structural variant record in the file using the below command (-A is the option which prints the specified N lines after the match):  1 grep \"^#\" -A 1 del.vcf   \nThe INFO field holds structural variant site information whereas all\ngenotype information (annotated as per the FORMAT fields) is provided in\nthe sample column. Reference supporting reads are compared to\nalternative supporting reads and mapping qualities are used to compute\ngenotype likelihoods (GL) for homozygous reference (0/0), heterozygous\nreference (0/1) and homozygous alternate (1/1) (GT). The final genotype\n(GT) is simply derived from the best GL and GQ is a phred-scaled\ngenotype quality reflecting the confidence in this genotype. If GQ<15\nthe genotype is flagged as LowQual. The genotyping takes into account\nall paired-ends with a mapping quality greater than 20 by default.  The INFO field provides information on the quality of the SV prediction\nand breakpoints. If you browse through the vcf file you will notice that\na subset of the DELLY structural variant predictions have been refined\nusing split-reads. These precise variants are flagged in the vcf info\nfield with the tag  PRECISE . To count the number of precise and\nimprecise variants you can simply use  grep .  1\n2 grep -c -w  \"PRECISE\"  *.vcf\ngrep -c -w  \"IMPRECISE\"  *.vcf   \nDELLY clusters abnormal paired-ends and every single cluster gives rise\nto an  IMPRECISE  SV call. For every  IMPRECISE  SV call an attempt is made\nto identify supporting split-reads/soft-clipped reads. DELLY then\ncomputes a consensus sequence (INFO:CONSENSUS) out of all split-read\ncandidates and then aligns this consensus sequence to the reference\nrequiring at least -m many aligned bases to the left and right (default\nis 13). INFO:PE is the number of supporting paired-ends. INFO:CT refers\nconnection types (CT), which indicates the order and orientation of\npaired-end cluster mappings (e.g. 3to3 for 3\u2019 to 3\u2019 and 5to5 for 5\u2019 to\n5\u2019). Values can be 3to5, 5to3, 3to3 or 5to5. Different names exist for\nthese connection types in the literature, head-to-head inversions,\ntail-to-tail inversions, and so on. The consensus alignment quality\n(SRQ) is a score between 0 and 1, where 1 indicates 100% identity to the\nreference. Nearby SNPs, InDels and micro-insertions at the breakpoint\ncan lower this score but only for mis-assemblies it should be very poor.\nDELLY currently drops consensus alignments with a score <0.8 and then\nfalls back to an  IMPRECISE  prediction.  SVs are flagged as FILTER:LowQual if PE <3 OR MAPQ <20 (for\ntranslocations: PE <5 OR MAPQ <20), otherwise, the SV results in a\nFILTER:PASS.  PRECISE  variants will have split-read support (SR >0).",
            "title": "DELLY VCF output"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#somatic-structural-variant-filtering",
            "text": "Please note that this vcf file contains germline and somatic structural\nvariants but also false positives caused by repeat induced mis-mappings\nor incomplete reference sequences. As a final step we have to use the\nstructural variant site information and the cancer and normal genotype\ninformation to filter a set of confident somatic structural variants.\nDELLY ships with a somatic filtering python script. For a set of\nconfident somatic calls one could exclude all structural variants\n<400bp, require a minimum variant allele frequency of 10%, no support\nin the matched normal and an overall confident structural variant site\nprediction with the VCF filter field being equal to PASS.  1\n2\n3\n4 python   $ SF / somaticFilter . py   - t   DEL    - T   cancer_cell_line   - N   control   - v   del . vcf   - o   del . filt . vcf   - a   0.1   - m   400   - f  python   $ SF / somaticFilter . py   - t   DUP   - T   cancer_cell_line   - N   control   - v   dup . vcf   - o   dup . filt . vcf   - a   0.1   - m   400   - f  python   $ SF / somaticFilter . py   - t   INV    - T   cancer_cell_line   - N   control   - v   inv . vcf   - o   inv . filt . vcf   - a   0.1   - m   400   - f  python   $ SF / somaticFilter . py   - t   TRA   - T   cancer_cell_line   - N   control   - v   tra . vcf   - o   tra . filt . vcf   - a   0.1   - m   400   - f    \nUsing  VCFtools  we can merge all somatic structural variants together in\na single vcf file.  1 vcf-concat del.filt.vcf dup.filt.vcf inv.filt.vcf tra.filt.vcf | vcf-sort > somatic.sv.vcf   \nFor large VCF files you should also zip and index them using  bgzip  and tabix . Please run the below commands to meet the requirements for\nvisualising somatic structural variants using  IGV .  1\n2 bgzip -c somatic.sv.vcf > somatic.sv.vcf.gz\ntabix somatic.sv.vcf.gz",
            "title": "Somatic Structural Variant Filtering"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#visualisation-of-somatic-structural-variants",
            "text": "The final step will be to browse some of these somatic structural\nvariants in IGV and to visually verify the reliability of the calls. To make it easy\nto navigate through our breakpoints of interest we will create a bed file (0-index co-ordinate format file).    1\n2 $CZ/sv.vcf2bed.sh somatic.sv.vcf.gz > somatic.sv.bed\nhead somatic.sv.bed   \nLoad the IGV browser  1 $BR/igv.sh &   \nOnce IGV has started use  File  and  Load from File  (directory /home/trainee/sv/data) to load the cancer_cell_line.bam  and the  control.bam . Go to  View  menu and\nselect  Preferences , then click on the Alignments tab\nand in the  Visibility range threshold (kb)  text box, enter 600. This\nwill allow you to increase your visibility of pile ups as you zoom out.\nNow look for check box for  Filter secondary alignments .\nEnsure box is ticked so that you do not see secondary alignments (alternate mapped position of\na read). Also ensure that  Show soft-clipped bases  has been checked\nthen click  OK .  Then import  somatic.sv.bed  from your working directory using  Regions \nand  Import Regions .",
            "title": "Visualisation of Somatic Structural Variants"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-deletion",
            "text": "This is an advanced section.      The somatic structural variants can then be browsed easily using the Region Navigator . Select the deletion (chrX:76853017-77014863) from\nthe  Region Navigator  and click  View . This will centre the IGV\nalignment view on the selected structural variant. Close the regions of\ninterest pop up window by right clicking mouse at the top of pop up and then choose close. The red bar below the ruler marks the region of\nthe deletion.  It\u2019s usually best to zoom out once by clicking on the  -  sign in the\ntoolbar at the top, to give a wide view of the supporting abnormal\npaired-end read mappings.  To highlight the abnormal paired-ends right click on the main track\ndisplay panel and select  Color alignments by  and then switch to insert size and pair orientation .  Read pairs that have a larger than expected insert size will be\nhighlighted in red. Click  View as pairs . Right click and  Sortalignments by  then select  start location .    Question  How many abnormal  paired-end read pairs  (red coloured F/R oriented read pairs) can you see that spans the deletion region? Does this number coincide with the INFO:PE?  Hint 1 cat somatic.sv.vcf | grep \"<DEL>\" | cut -f1,2,8    Answer 19, YES    Question  Zoom into left and right breakpoint separately and tally the number of soft-clipped reads (count the soft-clipped reads with >24 mismatched reads).\nHow many abnormal split-reads (soft-clipped reads) did\nyou observe? Clue INFO:SR.  Answer 11    Question  Go to the RefSeq genes track at the bottom of  IGV  and right click to Expanded . Is the predicted deletion likely to have a deleterious\nimpact on a gene? If so, what gene and exons are deleted?  Answer ATRX, exons 2 to 25.    Question  Does this region appear to be completely removed from this cancer\ngenome? How can you tell?  Answer Yes, there is no read coverage within this deletion region relative to\nthe control genome.",
            "title": "Verify Deletion"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-translocation",
            "text": "This is an advanced section.     Remove  control.bam  and the coverage track by right clicking on the track panel and selecting remove track.    Select the translocation breakpoint chr18 from the Region Navigator. Highlight the abnormal paired-ends by clicking and selecting Color alignments by  and then switch to  insert size and pair orientation . Invoke  Sort alignments by  then select  start location .    Zoom out until you can see all the purple reads at the junction.      Question  What is the direction of the purple cluster of reads (indicates that\nmate reads are mapped to Chr15)? Is it pointing to the tail or head of\nChr18?  Answer Forward, towards the tail, or 3\u2019 (+ive)    Question  Right click on to one of the purple coloured reads and select  View mate region in split-screen .\nThis will split the screen and display Chr15 on the left and place a red\nhighlighted outline on both reads, to indicate the pairs. Select  view as pairs ,\nthen sort alignments by start location. To control the zooming on each of the chromosome panels,\nfirst click inside of the track panel of your chromosome of interest, then to zoom in\n( Shift  and  +  key together) or out (press  Ctrl  and  -  key together).  What is the direction of the yellow cluster of reads (indicates that mate\nreads are mapped to Chr18)? Is it pointing to the tail or head of Chr15?\nIf you wish to zoom in or out, first click inside of the chromosome ideogram\npanel, then ctrl- to zoom out and shift+ to zoom in.  Answer Reverse, towards the head, or 5\u2019 (+ive)    Question  How is the Chr15 and Chr18 fused (which one of the four translocation\nconnection types)? If you are uncertain then run a BLAT ( https://genome.ucsc.edu/cgi-bin/hgBlat?command=start )\nsearch using the INFO:CONSENSUS sequence.  Hint 1 cat somatic.sv.vcf | grep \"<TRA>\" | cut -f1,2,8    Answer RF, head to tail, or 5 to 3. Therefore, Chr18 left side is fused to\nChr15 right side.    Question  Did DELLY predict a reciprocal translocation? How can you tell?  Answer No, as we would expect to observe a Chr18 right side fused to Chr15 left\nside, near the same breakpoints.    Question  What gene structures is this translocation predicted to impact?  Answer ADAMTSL3 on Chr15 and PARD6G on Chr18.    Question  What is one possible reason why there is no observable read coverage\nafter Chr18 breakpoint?  Answer Chromosome loss.",
            "title": "Verify translocation"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-tandem-duplication",
            "text": "This is an advanced section.     Select the tandem duplication (chrX:45649874-45689322) from the  RegionNavigator .    Highlight the abnormal paired-ends by clicking and selecting  Color alignments by \nand then switch to  insert size and pair orientation .\nAlso, invoke  Sort alignments by  then select  start location .    Zoom out until you can see all the red paired-end reads spanning the two\njunctions. After that zoom in on the cluster of abnormal reads on the\nleft junction and then right junction.      Question  Which is the order and orientation of these paired-end reads (FR, RF, FF\nor RR)?  Answer RF    Question  What is the estimated read-depth ratio of the cancer_cell_line versus\nnormal control (INFO:RDRATIO) over the duplicate region?  Hint 1 cat somatic.sv.vcf | grep \"<DUP>\" | cut -f1,2,8    Answer 3.3 (\u00a03 x increased read depth)",
            "title": "Verify tandem duplication"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-inversion",
            "text": "This is an advanced section.     Type into the search box near at tool bar, Chr20:54834492    Right click on main display and select  Group alignments by  then switch\non  paired-orientation . Also, right click and  Sort alignments by  then\nselect  start location .    Zoom out until you can see all the red coloured cluster of reads near\nthe breakpoint.    Right click on to one of the red coloured reads and select  View mate region in split-screen .\nThis will split the screen and display the read mate on the right side.\nThis will take you to the mate-reads near the second breakpoint.      Question  Which direction are the paired-end reads spanning (left or right spanning)?  Answer Right    Question  What is the estimated size of the inverted interval?  Answer 55,408,660 \u2013 54,834,492 = 574,168 bp",
            "title": "Verify Inversion"
        },
        {
            "location": "/modules/cancer-module-sv/sv_tut/#acknowledgements",
            "text": "We would like to thank and acknowledge Tobias Rausch (EMBL Heidelberg)\nfor his help and for allowing us to borrow and adapt his replies to\nquestions and original course material.",
            "title": "Acknowledgements"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nVisualise mutational signatures present in a cohort using somatic\n    single nucleotide mutation data in Variant Call Format (VCF) files.\n\n\n\n\n\n\nCompare analysis output with published results to identify common\n    mutational signatures.\n\n\n\n\n\n\nHave gained overview knowledge of how somatic signatures can help\n    with cohort cancer analysis.\n\n\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nR-3.2.2 statistical environment:\n\n\nhttps://www.r-project.org/\n\n\nSomaticSignatures R package:\n\n\nhttp://bioconductor.org/packages/release/bioc/html/SomaticSignatures.html\n\n\nBSgenome.Hsapiens.UCSC.hg19:\n\n\nhttp://bioconductor.org/packages/release/data/annotation/html/BSgenome.Hsapiens.UCSC.hg19.html\n\n\nVariantAnnotation:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/VariantAnnotation.html\n\n\nGenomicRanges:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/GenomicRanges.html\n\n\nCairo:\n\n\nhttps://cran.rstudio.com/web/packages/Cairo/index.html\n\n\nSources of Data\n\u00b6\n\n\nTCGA melanoma SNV data:\n\n\nhttps://tcga-data.nci.nih.gov/tcga/\n\n\nICGC ovarian SNV data: \n\n\nhttps://dcc.icgc.org/\n\n\nUseful Links\n\u00b6\n\n\nVariant Call Format (VCF) specification: \n\n\nhttp://samtools.github.io/hts-specs/VCFv4.2.pdf\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n\nAnn-Marie Patch, QIMR Berghofer \nann-marie.patch@qimrberghofer.edu.au\n\nErdahl Teber, CMRI \neteber@cmri.org.au\n  \n\n\nContributor(s):\n\nMartha Zakrzewski \nMartha.Zakrzewski@qimrberghofer.edu.au\n\n\n\n\nIntroduction\n\u00b6\n\n\nThe most common genetic model for cancer development is the accumulation\nof DNA mutations over time, eventually leading to the disruption or\ndysregulation of enough key genes that lead cells to uncontrolled\ngrowth. Cells in our bodies accumulate DNA mutations over time due to\nnormal aging processes and through exposure to carcinogens.\n\n\nRecently researchers found a method to take all the single nucleotide\nmutations identified in tumour cells (somatic SNVs) and group them\ntogether by the type of the mutation and also what the neighbouring\nbases are. This is commonly referred to as somatic mutational\nsignatures. Common mutational processes that are regularly identified in cancer\nsequencing are:\n\n\n\n\n\n\nAge: the aging process. These are high in C/T transitions due to\n    deamination of methyl-cytidine.\n\n\n\n\n\n\nSmoking: marks exposure to inhaled carcinogens and has high numbers\n    of C/A transversions.\n\n\n\n\n\n\nUV: UV exposure. These are also high in C/T transitions at\n    di-pyrimidine sites.\n\n\n\n\n\n\nBRCA: Indicates that the homologous recombination repair pathway is\n    defective.\n\n\n\n\n\n\nAPOBEC: Thought to be marking dysregulated APOBEC enzyme activity on\n    single stranded DNA produced during the repair processing of other\n    lesions such as double stand breaks.\n\n\n\n\n\n\nMMR: Mismatch repair pathway not working properly. These are high in\n    C/T mutations too.\n\n\n\n\n\n\n\nIn cohort cancer analysis it is common to try to generate subtypes to\ngroup your data based on a particular molecular phenotype. A reason for\ndoing may include finding sets of patients that have a similar form of\nthe disease and therefore all might benefit from a particular treatment.\nWe can use the somatic mutational signatures analysis to group the data\nfrom a cohort of patients to inform which genomes are most similar based\non the pattern of exposures or processes that have contributed to their\ngenome changes. The patients don\u2019t have to have the same type of cancer\nso pan-cancer studies are using this analysis to find similarities\nacross cancer types.\n\n\n\n\nPreparing the R environment\n\u00b6\n\n\nThe mathematical framework developed by Alexandrov \net al.\n was implemented\nin MATLAB. We are going to use a version implemented in R by Gehring \net\nal.\n called \nSomaticSignatures package\n, that is very quick and flexible\nbut currently only accepts point mutations not insertions or deletions\n(indels). In tests on our data we have found that the Somatic Signatures\npackage in R returns very similar results to the full implementation of\nAlexandrov\u2019s framework.\n\n\nThe data files you will need are contained in the subdirectory called\n\nsomatic/somatic_signatures\n:\n\n\nOpen the Terminal and go to the \nsomatic_signatures\n working directory:\n\n\n1\n2\ncd\n ~/somatic/somatic_signatures\n\npwd\n\n\n\n\n\n\n\nIn this folder you should find 12 files that end with the extension\n\n.vcf\n. Use the list command to make sure you can see them.\n\n\n1\nls\n\n\n\n\n\n\nThese files contain data extracted from the TCGA melanoma paper and\nAustralian ICGC ovarian paper both mentioned in the introductory slides.\nThey have been edited in order to allow this practical to run quickly\nand are not good examples of VCF files.\n\n\nStart R and set the working directory. Just start by typing R onto the\ncommand line.\n\n\n1\nR\n\n\n\n\n\n\nLoad all the package libraries needed for this analysis by running the\ncommands.\n\n\n1\n2\n3\n4\nlibrary\n(\nSomaticSignatures\n)\n\n\nlibrary\n(\nBSgenome.Hsapiens.UCSC.hg19\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\nCairo\n)\n\n\n\n\n\n\n\nSet the directory where any output files will be generated\n\n\n1\nsetwd\n(\n\"~/somatic/somatic_signatures\"\n)\n\n\n\n\n\n\n\n\n\nLoading and preparing the SNV mutation data\n\u00b6\n\n\nThe mutations used in this analysis need to be high quality somatic\nmutations\n\n\n\n\n\n\nRemember the goal is to find the key mutational processes that these\n    tumours have been exposed to, so you need to exclude germline\n    mutations (mutations that the person was born with that can be seen\n    in the sequencing of matched normal samples).\n\n\n\n\n\n\nSequencing errors can also occur at particular DNA sequence contexts\n    and can also be picked up using this method. To avoid this use only\n    high quality mutation calls.\n\n\n\n\n\n\nRead in the mutations from the 12 VCF files\n\n\n1\nfiles \n<-\n \nlist.files\n(\n\"~/somatic/somatic_signatures\"\n,\n pattern\n=\n\"vcf$\"\n,\n full.names\n=\nTRUE\n)\n\n\n\n\n\n\n\nTo make sure all the files are listed run the command.\n\n\n1\nfiles\n\n\n\n\n\n\nYou should see a list of 12 sample files.\n\n\nNext read in all the genomic positions of variants in the VCF files\nusing the \nvranges\n class.\n\n\n1\nvranges \n<-\n \nlapply\n(\nfiles\n,\n \nfunction\n(\nv\n)\n readVcfAsVRanges\n(\nv\n,\n\"hg19\"\n))\n\n\n\n\n\n\n\nJoin all the lists of variant positions into one big data set so that it\ncan be processed together and look at what is contained in the\nconcatenated \nvranges\n data\n\n\n1\n2\nvranges.cat \n<-\n \ndo.call\n(\nc\n,\nvranges\n)\n\nvranges.cat\n\n\n\n\n\n\nThe first line of output of the \nvranges.cat\n shows us that in total we\nhave put over 100,000 mutations recording the chromosome positions and\nmutation base changes along with what sample they were seen in.\n\n\nNote there are a lot of NA values in this data set because we have left\nout non-essential information in order to cut down on the processing\ntime.\n\n\nNext we need to ensure all the positions in the \nvranges\n object have been\nrecorded in UCSC notation form so that they will match up with the\nreference we are using.\n\n\n1\nvranges.cat \n<-\n ucsc\n(\nvranges.cat\n)\n\n\n\n\n\n\n\nIt is always important to select the correct reference for your data.\n\n\nWe can print out how many mutations we have read in for each of the\ncancer samples we are using by using the command.\n\n\n1\nprint\n(\ntable\n(\nsampleNames\n(\nvranges.cat\n)))\n\n\n\n\n\n\n\nWe have now added all the positional and base change information now we\ncan use the reference and the position of the mutation to look up the\nbases on either side of the mutation i.e. the mutation context.\n\n\nRun the mutationContext function of SomaticSignatures.\n\n\n1\nmc \n<-\n mutationContext\n(\nvranges.cat\n,\n BSgenome.Hsapiens.UCSC.hg19\n)\n\n\n\n\n\n\n\nWe can inspect what information we had added to the \nvranges.cat\n object\nby typing \nmc\n on the command line. Notice that the mutation and its\ncontext have been added to the last two columns.\n\n\n1\nmc\n\n\n\n\n\n\n\n\nSNV mutation context\n\u00b6\n\n\nThere are a total of 96 possible single base mutations and context\ncombinations. We can calculate this by listing out the six possible\ntypes of single nucleotide mutations:\n\n\n\n  -   C/A   the reverse compliment (G/T) is also in this group\n  -   C/G   includes (G/C)\n  -   C/T   includes (G/C)\n  -   T/A   includes (A/T)\n  -   T/C   includes (A/G)\n  -   T/G   includes (A/C)\n  \n\n\nThe neighbouring bases, on either side of a mutation, are referred to as\nthe mutation context. There are 16 possible combinations of mutation\ncontexts. Here [.] stands for one of the mutations listed above.\n\n\n\n  -   A[.]A   A[.]C   A[.]G   A[.]T\n  -   C[.]A   C[.]C   C[.]G   C[.]T\n  -   G[.]A   G[.]C   G[.]G   G[.]T\n  -   T[.]A   T[.]C   T[.]G   T[.]T\n  \n\n\nNow if we substitute the [.]\u2019s with each of the 6 different mutations\nyou will find there are 96 possible types of combined mutations and\ncontexts (6 x 16).\n\n\nStart by substituting [.] for the A/C mutation type\n\n\n\n  -   A[C/A]A\n  -   A[C/A]C\n  -   A[C/A]G\n  -   A[C/A]T\n  -   C[C/A]A\n  -   C[C/A]C\n  -   C[C/A]G\n  \n\n\nand so on\u2026\n\n\nWe assign all the somatic mutations identified in a single tumour to one\nof these categories and total up the number in each.\n\n\n\n\nQuestion\n\n\nWhat about a mutation that looks like G[A/C]A, where should this go?\n\n\nHint\nRemember to reverse compliment all the nucleotides.\n\n\nAnswer\nIn the T[T/G]C context count.\n\n\n\n\n\nNow we have all the information that is needed for each sample we can\nmake a matrix that contains counts of mutations in each of the 96\npossible combinations of mutations and contexts counting up the totals\nseparately for each sample\n\n\n1\n2\nmm \n<-\n motifMatrix\n(\nmc\n,\n group \n=\n \n\"sampleNames\"\n,\n normalize\n=\nTRUE\n)\n\n\ndim\n(\nmm\n)\n\n\n\n\n\n\n\nThe output of the \ndim(mm)\n command show us that there are 96 rows\n(these are the context values) and 12 columns which are the 12 samples.\n\n\n\n\nRunning the NMF analysis\n\u00b6\n\n\nUsing the matrix we have made we can now run the non-negative matrix\nfactorisation (NMF) process that attempts to find the most stable,\ngrouping solutions for all of the combinations of mutations and\ncontexts. It does this by trying to find similar patterns, or profiles,\namongst the samples to sort the data into firstly just 2 groups. This is\nrepeated to get replicate values for each attempt and then separating\nthe data by 3 groups, and then 4 and so on.\n\n\nThese parameter choices have been made to keep running time short for\nthis practical. If you have more samples from potentially diverse\nsources you may need to run with a larger range of signatures and with\nmore replicates.\n\n\nTo find out how many signatures we have in the data run the command.\n\n\n1\ngof_nmf \n<-\n assessNumberSignatures\n(\nmm\n,\n \n2\n:\n10\n,\n nReplicates \n=\n \n5\n)\n\n\n\n\n\n\n\nVisualise the results from the NMF processing by making a pdf of the\nplot\n\n\n1\n2\n3\npdf\n(\nfile\n=\n\"plotNumberOfSignatures.pdf\"\n,\n width\n=\n9\n,\n height\n=\n8\n)\n\nplotNumberSignatures\n(\ngof_nmf\n)\n\ndev.off\n()\n\n\n\n\n\n\n\nOpen up the PDF and examine the curve. The plotNumberOfSignatures PDF\nthat will have been made in the working directory that you set up at the\nbeginning\n\n\n \nFigure 1:\n This plot is used to find the number of signatures that is likely to be the best grouping solution. The top plot shows the decreasing residual sum of squares for each\nincreasing number of signatures and the bottom plot the increasing explained variance as\nthe number of potential signatures increases. Ideally the best solution will be the lowest\nnumber of signatures with a low RSS and a high explained variance.\n\n\n\nLook at the y-axis scale on the bottom panel of Figure 1. The explained\nvariance is already very high and so close to finding the correct\nsolution for the number of signatures even with just 2. The error bars\naround each point are fairly small considering we have a very small\nsample set. Deciding how many signatures are present can be tricky but\nhere let\u2019s go for 3. This is where the gradient of both curves have\nstarted to flatten out.\n\n\nNow run the NMF again but this time stipulating that you want to group\nthe data into 3 different mutational signatures.\n\n\n1\n  sigs_nmf \n=\n identifySignatures\n(\nmm\n,\n \n3\n,\n nmfDecomposition\n)\n\n\n\n\n\n\n\nVisualise the shape of the profiles for these 3 signatures\n\n\n1\n2\n3\n  pdf\n(\nfile\n=\n\"plot3Signatures.pdf\"\n,\n width\n=\n10\n,\n height\n=\n8\n)\n\n  plotSignatures\n(\nsigs_nmf\n,\nnormalize\n=\nTRUE\n,\n percent\n=\nFALSE\n)\n \n+\n ggtitle\n(\n\"Somatic Signatures: NMF - Barchart\"\n)\n \n+\n scale_fill_brewer\n(\npalette \n=\n \n\"Set2\"\n)\n\n  dev.off\n()\n\n\n\n\n\n\n\nOpen up \nplot3Signatures.pdf\n that will have been made in the working\ndirectory.\n\n\nYou should have generated a plot with three signature profiles obtained\nfrom the NMF processing of the test dataset.\n\n\nThe 96 possible mutation/context combinations are plotted along the x\naxis arranged in blocks of 6 lots of 16 (see information above). The\nheight of the bars indicates the frequency of those particular mutation\nand context combinations in each signature.\n\n\nAlthough the section colours are different to the plot you have\ngenerated the mutations are still in the same order across the plot.\n\n\n\n\nInterpreting the signature results\n\u00b6\n\n\nIn their paper Alexandrov \net al.\n used this analysis to generate profiles\nfrom the data for more than 7000 tumour samples sequenced through both\nexome and whole genome approaches. They were able to group the data to\nreveal which genomes have been exposed to similar mutational processes\ncontributing to the genome mutations.\n\n\n \nFigure 2.\n The 96 possible mutation/context\ncombinations are plotted along the x axis arranged in blocks of 6 lots of 16.\nThe y axis indicates the frequency of those particular mutation and context\ncombinations in each signature. \nSource\n: Alexandrov \net al.\n Nature 2013\n\n\n  \n\n\n\n\nQuestion\n\n\nCan you match up, by eye, the profile shapes against a selection of\nknown mutational signatures supplied (Figure 2)?\n\n\nTry to match up the patterns made by the positions of the highest peaks for each signature.\n\n\nAnswer\nAlexandrov signature 7 matches with our signature 1\nAlexandrov signature 13 matches with our signature 2\nAlexandrov signature 3 matches with our signature 3\n\n\n\n\n\nNow use the table from Alexandrov \net al.\n to identify which mutational\nprocesses our three generated signatures have been associated with.\n\n\n \nFigure 3.\n The 21 signatures identified\nare indicated as rows with the number corresponding to Figure 2 on the left. The\ntypes of tumours used in the analysis are listed as columns. A green dot at the\nintersection of a signature and tumour indicates the signature was identified in\nthat sample type. Where verified the mutational process is listed on the right.\n\nSource\n: Alexandrov \net al.\n Nature 2013\n\n\n\n\n\n\nQuestion\n\n\nWhat mutational mechanisms have been associated with the signatures that\nyou have generated?\n\n\nAnswer\nOur signature 1 (AS7) is associated with Ultraviolet radiation\n    damage to DNA. This has previously been identified in Head and Neck\n    and Melanoma cancer samples.\nOur signature 2 (AS 13) is associated with the activity of\n    anti-viral APOBEC enzymes. This has previously been seen in Breast\n    and Bladder cancer samples.\nOur signature 3 (AS3) is associated with BRCA1 and BRCA2 mutations,\n    i.e. the homologous recombination repair pathway not working\n    properly. This has been seen in Breast, Ovarian and Pancreas cancer\n    samples.\n\n\n\n\n\nNow we can plot out the results for the individual samples in our\ndataset to show what proportion of their mutations have been assigned to\neach of the signatures.\n\n\n1\n2\n3\npdf\n(\nfile\n=\n\"PlotSampleContribution3Signatures.pdf\"\n,\n width\n=\n9\n,\n height\n=\n6\n)\n\nplotSamples\n(\nsigs_nmf\n,\n normalize\n=\nTRUE\n)\n \n+\n scale_y_continuous\n(\nbreaks\n=\nseq\n(\n0\n,\n \n1\n,\n \n0.2\n),\n expand \n=\n \nc\n(\n0\n,\n0\n))\n+\n theme\n(\naxis.text.x \n=\n element_text\n(\nsize\n=\n6\n))\n\ndev.off\n()\n\n\n\n\n\n\n\nIf you don\u2019t have time to carry out the advanced questions you can exit\nR and return to the normal terminal command line.\n\n\n1\n2\nquit\n()\n\nn\n\n\n\n\n\n\n\nOpen the resulting \nPlotSampleContribution3Signatures.pdf\n.  \n\n\nThis shows the results for the mutation grouping for each sample. The\nsamples are listed on the x-axis and the proportion of all mutations for\nthat sample is shown on the y-axis. The colours of the bars indicate\nwhat proportion of the mutations for that sample were grouped into each\nof the signatures. The colour that makes up most of the bar for each\nsample is called its \u201cmajor signature\u201d.\n\n\nThe data you have been using contains samples from High Grade Serous\nOvarian Carcinomas and Cutaneous Melanoma.\n\n\n\n\n\nQuestion\n\n\nUsing the major signature found for each sample can you guess which are\novarian and which are melanoma samples?\n\n\nAnswer\nSamples 9-12 have the majority signature of our signature 1. This is\n    the UV signature and so these are likely to be Melanoma samples.\nSamples 4-8 have the majority signature of our signature 3. This is\n    the BRCA signature and these are most likely to be ovarian samples.\nSamples 1-3 have the majority signature of our signature 2. This is\n    the APOBEC signature indicating activity of the anti-viral APOBEC\n    enzymes. These are less likely to be from cutaneous melanoma because\n    they have very few UV associated mutations although it could\n    possibly be from a different subtype. However it is much more likely\n    that these will be ovarian tumours as this APOBEC signature has been\n    seen in breast tumours which can be similar to ovarian cancers in\n    terms of the mutated genes.\n\n\n\n\n\n\nQuestion\n\n\nThis is an open question for discussion at the end of the practical.\n\n\nHow can this analysis be useful for cancer genomics studies?\n\n\n\n\n\n\nAdvanced exercise\n\n\nNow rerun the process this time using 4 signatures as the solution.\n\n\nHint\nYou don\u2019t have to start back at the beginning but you can jump to\nthe step where you run the NMF but this time for 4 instead of 3\nsignatures. Then continue through making the plots. You will need to\nchange the name of each plot you remake with 4 signatures because\nCairo won\u2019t let you overwrite and existing file.\n\n\nCan you find a good match in the set of known signatures for all 4 patterns?\n\n\nCan you find a verified process for all of the profiles you are seeing?\n\n\n\n\n\n\nReferences\n\u00b6\n\n\nAlexandrov \net al.\n Nature 2013: \n\n\nhttp://www.nature.com/nature/journal/v500/n7463/pdf/nature12477.pdf\n\n\nGehring \net al.\n Bioinformatics 2015: \n\n\nhttp://bioinformatics.oxfordjournals.org/content/early/2015/07/31/bioinformatics.btv408.full",
            "title": "Assessing somatic mutational signatures"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Visualise mutational signatures present in a cohort using somatic\n    single nucleotide mutation data in Variant Call Format (VCF) files.    Compare analysis output with published results to identify common\n    mutational signatures.    Have gained overview knowledge of how somatic signatures can help\n    with cohort cancer analysis.",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#tools-used",
            "text": "R-3.2.2 statistical environment:  https://www.r-project.org/  SomaticSignatures R package:  http://bioconductor.org/packages/release/bioc/html/SomaticSignatures.html  BSgenome.Hsapiens.UCSC.hg19:  http://bioconductor.org/packages/release/data/annotation/html/BSgenome.Hsapiens.UCSC.hg19.html  VariantAnnotation:  https://bioconductor.org/packages/release/bioc/html/VariantAnnotation.html  GenomicRanges:  https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html  Cairo:  https://cran.rstudio.com/web/packages/Cairo/index.html",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#sources-of-data",
            "text": "TCGA melanoma SNV data:  https://tcga-data.nci.nih.gov/tcga/  ICGC ovarian SNV data:   https://dcc.icgc.org/",
            "title": "Sources of Data"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#useful-links",
            "text": "Variant Call Format (VCF) specification:   http://samtools.github.io/hts-specs/VCFv4.2.pdf",
            "title": "Useful Links"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#author-information",
            "text": "Primary Author(s): \nAnn-Marie Patch, QIMR Berghofer  ann-marie.patch@qimrberghofer.edu.au \nErdahl Teber, CMRI  eteber@cmri.org.au     Contributor(s): \nMartha Zakrzewski  Martha.Zakrzewski@qimrberghofer.edu.au",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#introduction",
            "text": "The most common genetic model for cancer development is the accumulation\nof DNA mutations over time, eventually leading to the disruption or\ndysregulation of enough key genes that lead cells to uncontrolled\ngrowth. Cells in our bodies accumulate DNA mutations over time due to\nnormal aging processes and through exposure to carcinogens.  Recently researchers found a method to take all the single nucleotide\nmutations identified in tumour cells (somatic SNVs) and group them\ntogether by the type of the mutation and also what the neighbouring\nbases are. This is commonly referred to as somatic mutational\nsignatures. Common mutational processes that are regularly identified in cancer\nsequencing are:    Age: the aging process. These are high in C/T transitions due to\n    deamination of methyl-cytidine.    Smoking: marks exposure to inhaled carcinogens and has high numbers\n    of C/A transversions.    UV: UV exposure. These are also high in C/T transitions at\n    di-pyrimidine sites.    BRCA: Indicates that the homologous recombination repair pathway is\n    defective.    APOBEC: Thought to be marking dysregulated APOBEC enzyme activity on\n    single stranded DNA produced during the repair processing of other\n    lesions such as double stand breaks.    MMR: Mismatch repair pathway not working properly. These are high in\n    C/T mutations too.    \nIn cohort cancer analysis it is common to try to generate subtypes to\ngroup your data based on a particular molecular phenotype. A reason for\ndoing may include finding sets of patients that have a similar form of\nthe disease and therefore all might benefit from a particular treatment.\nWe can use the somatic mutational signatures analysis to group the data\nfrom a cohort of patients to inform which genomes are most similar based\non the pattern of exposures or processes that have contributed to their\ngenome changes. The patients don\u2019t have to have the same type of cancer\nso pan-cancer studies are using this analysis to find similarities\nacross cancer types.",
            "title": "Introduction"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#preparing-the-r-environment",
            "text": "The mathematical framework developed by Alexandrov  et al.  was implemented\nin MATLAB. We are going to use a version implemented in R by Gehring  et\nal.  called  SomaticSignatures package , that is very quick and flexible\nbut currently only accepts point mutations not insertions or deletions\n(indels). In tests on our data we have found that the Somatic Signatures\npackage in R returns very similar results to the full implementation of\nAlexandrov\u2019s framework.  The data files you will need are contained in the subdirectory called somatic/somatic_signatures :  Open the Terminal and go to the  somatic_signatures  working directory:  1\n2 cd  ~/somatic/somatic_signatures pwd    In this folder you should find 12 files that end with the extension .vcf . Use the list command to make sure you can see them.  1 ls   These files contain data extracted from the TCGA melanoma paper and\nAustralian ICGC ovarian paper both mentioned in the introductory slides.\nThey have been edited in order to allow this practical to run quickly\nand are not good examples of VCF files.  Start R and set the working directory. Just start by typing R onto the\ncommand line.  1 R   Load all the package libraries needed for this analysis by running the\ncommands.  1\n2\n3\n4 library ( SomaticSignatures )  library ( BSgenome.Hsapiens.UCSC.hg19 )  library ( ggplot2 )  library ( Cairo )    Set the directory where any output files will be generated  1 setwd ( \"~/somatic/somatic_signatures\" )",
            "title": "Preparing the R environment"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#loading-and-preparing-the-snv-mutation-data",
            "text": "The mutations used in this analysis need to be high quality somatic\nmutations    Remember the goal is to find the key mutational processes that these\n    tumours have been exposed to, so you need to exclude germline\n    mutations (mutations that the person was born with that can be seen\n    in the sequencing of matched normal samples).    Sequencing errors can also occur at particular DNA sequence contexts\n    and can also be picked up using this method. To avoid this use only\n    high quality mutation calls.    Read in the mutations from the 12 VCF files  1 files  <-   list.files ( \"~/somatic/somatic_signatures\" ,  pattern = \"vcf$\" ,  full.names = TRUE )    To make sure all the files are listed run the command.  1 files   You should see a list of 12 sample files.  Next read in all the genomic positions of variants in the VCF files\nusing the  vranges  class.  1 vranges  <-   lapply ( files ,   function ( v )  readVcfAsVRanges ( v , \"hg19\" ))    Join all the lists of variant positions into one big data set so that it\ncan be processed together and look at what is contained in the\nconcatenated  vranges  data  1\n2 vranges.cat  <-   do.call ( c , vranges ) \nvranges.cat   The first line of output of the  vranges.cat  shows us that in total we\nhave put over 100,000 mutations recording the chromosome positions and\nmutation base changes along with what sample they were seen in.  Note there are a lot of NA values in this data set because we have left\nout non-essential information in order to cut down on the processing\ntime.  Next we need to ensure all the positions in the  vranges  object have been\nrecorded in UCSC notation form so that they will match up with the\nreference we are using.  1 vranges.cat  <-  ucsc ( vranges.cat )    It is always important to select the correct reference for your data.  We can print out how many mutations we have read in for each of the\ncancer samples we are using by using the command.  1 print ( table ( sampleNames ( vranges.cat )))    We have now added all the positional and base change information now we\ncan use the reference and the position of the mutation to look up the\nbases on either side of the mutation i.e. the mutation context.  Run the mutationContext function of SomaticSignatures.  1 mc  <-  mutationContext ( vranges.cat ,  BSgenome.Hsapiens.UCSC.hg19 )    We can inspect what information we had added to the  vranges.cat  object\nby typing  mc  on the command line. Notice that the mutation and its\ncontext have been added to the last two columns.  1 mc",
            "title": "Loading and preparing the SNV mutation data"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#snv-mutation-context",
            "text": "There are a total of 96 possible single base mutations and context\ncombinations. We can calculate this by listing out the six possible\ntypes of single nucleotide mutations:  \n  -   C/A   the reverse compliment (G/T) is also in this group\n  -   C/G   includes (G/C)\n  -   C/T   includes (G/C)\n  -   T/A   includes (A/T)\n  -   T/C   includes (A/G)\n  -   T/G   includes (A/C)\n    The neighbouring bases, on either side of a mutation, are referred to as\nthe mutation context. There are 16 possible combinations of mutation\ncontexts. Here [.] stands for one of the mutations listed above.  \n  -   A[.]A   A[.]C   A[.]G   A[.]T\n  -   C[.]A   C[.]C   C[.]G   C[.]T\n  -   G[.]A   G[.]C   G[.]G   G[.]T\n  -   T[.]A   T[.]C   T[.]G   T[.]T\n    Now if we substitute the [.]\u2019s with each of the 6 different mutations\nyou will find there are 96 possible types of combined mutations and\ncontexts (6 x 16).  Start by substituting [.] for the A/C mutation type  \n  -   A[C/A]A\n  -   A[C/A]C\n  -   A[C/A]G\n  -   A[C/A]T\n  -   C[C/A]A\n  -   C[C/A]C\n  -   C[C/A]G\n    and so on\u2026  We assign all the somatic mutations identified in a single tumour to one\nof these categories and total up the number in each.   Question  What about a mutation that looks like G[A/C]A, where should this go?  Hint Remember to reverse compliment all the nucleotides.  Answer In the T[T/G]C context count.   \nNow we have all the information that is needed for each sample we can\nmake a matrix that contains counts of mutations in each of the 96\npossible combinations of mutations and contexts counting up the totals\nseparately for each sample  1\n2 mm  <-  motifMatrix ( mc ,  group  =   \"sampleNames\" ,  normalize = TRUE )  dim ( mm )    The output of the  dim(mm)  command show us that there are 96 rows\n(these are the context values) and 12 columns which are the 12 samples.",
            "title": "SNV mutation context"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#running-the-nmf-analysis",
            "text": "Using the matrix we have made we can now run the non-negative matrix\nfactorisation (NMF) process that attempts to find the most stable,\ngrouping solutions for all of the combinations of mutations and\ncontexts. It does this by trying to find similar patterns, or profiles,\namongst the samples to sort the data into firstly just 2 groups. This is\nrepeated to get replicate values for each attempt and then separating\nthe data by 3 groups, and then 4 and so on.  These parameter choices have been made to keep running time short for\nthis practical. If you have more samples from potentially diverse\nsources you may need to run with a larger range of signatures and with\nmore replicates.  To find out how many signatures we have in the data run the command.  1 gof_nmf  <-  assessNumberSignatures ( mm ,   2 : 10 ,  nReplicates  =   5 )    Visualise the results from the NMF processing by making a pdf of the\nplot  1\n2\n3 pdf ( file = \"plotNumberOfSignatures.pdf\" ,  width = 9 ,  height = 8 ) \nplotNumberSignatures ( gof_nmf ) \ndev.off ()    Open up the PDF and examine the curve. The plotNumberOfSignatures PDF\nthat will have been made in the working directory that you set up at the\nbeginning    Figure 1:  This plot is used to find the number of signatures that is likely to be the best grouping solution. The top plot shows the decreasing residual sum of squares for each\nincreasing number of signatures and the bottom plot the increasing explained variance as\nthe number of potential signatures increases. Ideally the best solution will be the lowest\nnumber of signatures with a low RSS and a high explained variance.  \nLook at the y-axis scale on the bottom panel of Figure 1. The explained\nvariance is already very high and so close to finding the correct\nsolution for the number of signatures even with just 2. The error bars\naround each point are fairly small considering we have a very small\nsample set. Deciding how many signatures are present can be tricky but\nhere let\u2019s go for 3. This is where the gradient of both curves have\nstarted to flatten out.  Now run the NMF again but this time stipulating that you want to group\nthe data into 3 different mutational signatures.  1   sigs_nmf  =  identifySignatures ( mm ,   3 ,  nmfDecomposition )    Visualise the shape of the profiles for these 3 signatures  1\n2\n3   pdf ( file = \"plot3Signatures.pdf\" ,  width = 10 ,  height = 8 ) \n  plotSignatures ( sigs_nmf , normalize = TRUE ,  percent = FALSE )   +  ggtitle ( \"Somatic Signatures: NMF - Barchart\" )   +  scale_fill_brewer ( palette  =   \"Set2\" ) \n  dev.off ()    Open up  plot3Signatures.pdf  that will have been made in the working\ndirectory.  You should have generated a plot with three signature profiles obtained\nfrom the NMF processing of the test dataset.  The 96 possible mutation/context combinations are plotted along the x\naxis arranged in blocks of 6 lots of 16 (see information above). The\nheight of the bars indicates the frequency of those particular mutation\nand context combinations in each signature.  Although the section colours are different to the plot you have\ngenerated the mutations are still in the same order across the plot.",
            "title": "Running the NMF analysis"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#interpreting-the-signature-results",
            "text": "In their paper Alexandrov  et al.  used this analysis to generate profiles\nfrom the data for more than 7000 tumour samples sequenced through both\nexome and whole genome approaches. They were able to group the data to\nreveal which genomes have been exposed to similar mutational processes\ncontributing to the genome mutations.    Figure 2.  The 96 possible mutation/context\ncombinations are plotted along the x axis arranged in blocks of 6 lots of 16.\nThe y axis indicates the frequency of those particular mutation and context\ncombinations in each signature.  Source : Alexandrov  et al.  Nature 2013       Question  Can you match up, by eye, the profile shapes against a selection of\nknown mutational signatures supplied (Figure 2)?  Try to match up the patterns made by the positions of the highest peaks for each signature.  Answer Alexandrov signature 7 matches with our signature 1 Alexandrov signature 13 matches with our signature 2 Alexandrov signature 3 matches with our signature 3   \nNow use the table from Alexandrov  et al.  to identify which mutational\nprocesses our three generated signatures have been associated with.    Figure 3.  The 21 signatures identified\nare indicated as rows with the number corresponding to Figure 2 on the left. The\ntypes of tumours used in the analysis are listed as columns. A green dot at the\nintersection of a signature and tumour indicates the signature was identified in\nthat sample type. Where verified the mutational process is listed on the right. Source : Alexandrov  et al.  Nature 2013    Question  What mutational mechanisms have been associated with the signatures that\nyou have generated?  Answer Our signature 1 (AS7) is associated with Ultraviolet radiation\n    damage to DNA. This has previously been identified in Head and Neck\n    and Melanoma cancer samples. Our signature 2 (AS 13) is associated with the activity of\n    anti-viral APOBEC enzymes. This has previously been seen in Breast\n    and Bladder cancer samples. Our signature 3 (AS3) is associated with BRCA1 and BRCA2 mutations,\n    i.e. the homologous recombination repair pathway not working\n    properly. This has been seen in Breast, Ovarian and Pancreas cancer\n    samples.   \nNow we can plot out the results for the individual samples in our\ndataset to show what proportion of their mutations have been assigned to\neach of the signatures.  1\n2\n3 pdf ( file = \"PlotSampleContribution3Signatures.pdf\" ,  width = 9 ,  height = 6 ) \nplotSamples ( sigs_nmf ,  normalize = TRUE )   +  scale_y_continuous ( breaks = seq ( 0 ,   1 ,   0.2 ),  expand  =   c ( 0 , 0 )) +  theme ( axis.text.x  =  element_text ( size = 6 )) \ndev.off ()    If you don\u2019t have time to carry out the advanced questions you can exit\nR and return to the normal terminal command line.  1\n2 quit () \nn   \nOpen the resulting  PlotSampleContribution3Signatures.pdf .    This shows the results for the mutation grouping for each sample. The\nsamples are listed on the x-axis and the proportion of all mutations for\nthat sample is shown on the y-axis. The colours of the bars indicate\nwhat proportion of the mutations for that sample were grouped into each\nof the signatures. The colour that makes up most of the bar for each\nsample is called its \u201cmajor signature\u201d.  The data you have been using contains samples from High Grade Serous\nOvarian Carcinomas and Cutaneous Melanoma.   Question  Using the major signature found for each sample can you guess which are\novarian and which are melanoma samples?  Answer Samples 9-12 have the majority signature of our signature 1. This is\n    the UV signature and so these are likely to be Melanoma samples. Samples 4-8 have the majority signature of our signature 3. This is\n    the BRCA signature and these are most likely to be ovarian samples. Samples 1-3 have the majority signature of our signature 2. This is\n    the APOBEC signature indicating activity of the anti-viral APOBEC\n    enzymes. These are less likely to be from cutaneous melanoma because\n    they have very few UV associated mutations although it could\n    possibly be from a different subtype. However it is much more likely\n    that these will be ovarian tumours as this APOBEC signature has been\n    seen in breast tumours which can be similar to ovarian cancers in\n    terms of the mutated genes.    Question  This is an open question for discussion at the end of the practical.  How can this analysis be useful for cancer genomics studies?    Advanced exercise  Now rerun the process this time using 4 signatures as the solution.  Hint You don\u2019t have to start back at the beginning but you can jump to\nthe step where you run the NMF but this time for 4 instead of 3\nsignatures. Then continue through making the plots. You will need to\nchange the name of each plot you remake with 4 signatures because\nCairo won\u2019t let you overwrite and existing file.  Can you find a good match in the set of known signatures for all 4 patterns?  Can you find a verified process for all of the profiles you are seeing?",
            "title": "Interpreting the signature results"
        },
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#references",
            "text": "Alexandrov  et al.  Nature 2013:   http://www.nature.com/nature/journal/v500/n7463/pdf/nature12477.pdf  Gehring  et al.  Bioinformatics 2015:   http://bioinformatics.oxfordjournals.org/content/early/2015/07/31/bioinformatics.btv408.full",
            "title": "References"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/",
            "text": "Key Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nRun the \nIntOGen\n analysis software on cohort mutation data.\n\n\n\n\n\n\nHave gained experience of the structure of the analysis output files\n    in order to identify potential driver genes.\n\n\n\n\n\n\nHave gained overview knowledge of different methods for\n    identification of genes important in cancers.\n\n\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nIntOGen mutations platform:\n\n\nhttps://www.intogen.org/search\n\n\nSources of Data\n\u00b6\n\n\nTCGA melanoma somatic SNV data from 338 tumour samples:\n\n\nhttps://tcga-data.nci.nih.gov/tcga/\n\n\nUseful Links\n\u00b6\n\n\nMutation Annotation Format (MAF) specification:\n\n\nhttps://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+(MAF)+Specification\n\n\nIntOGen installation instructions:\n\nhttps://bitbucket.org/intogen/intogen-pipeline/overview\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n\nAnn-Marie Patch, QIMR Berghofer \nann-marie.patch@qimrberghofer.edu.au\n\nErdahl Teber, CMRI \neteber@cmri.org.au\n  \n\n\nContributor(s):\n\nScott Wood \nscott.wood@qimrberghofer.edu.au\n  \n\n\n\n\nIntroduction\n\u00b6\n\n\nCancer driver genes are commonly described as genes that when mutated\ndirectly affect the potential of a cell to become cancerous. They are\nimportant to a tumour cell as they confer a growth or survival advantage\nover the normal surrounding cells. The mutations in these driver genes\nare then clonally selected for as the population of tumour cells\nincreases. We think of the key genes driving tumour initiation\n(development), progression, metastases, resistance and survival. Driver\ngene mutations are often described as \u201cearly\u201d events because they were\nkey in turning a normally functioning and regulated cell into a\ndysregulated one. The logical assumption is that these key mutations\nwill be present in all tumour cells in a patient\u2019s sample; although\nsometime this is not true.\n\n\nThere are two major research goals that underline the need to identify\ndriver genes:\n\n\n\n\n\n\nBy identifying the early changes that take place researchers might\n    be able to find a treatment to stop the root cause of why cells\n    become malignant.\n\n\n\n\n\n\nBy identifying groups of patients with the same genes mutated then\n    we can develop therapies that will work for all of them.\n\n\n\n\n\n\nWhen we sequence tumour samples we tend to use samples that come from\nfully developed cancers that can carry hundreds to thousands of\nmutations in genes and many more outside of genes. The accumulation of\nthese passenger mutations in cancer cells can happen because often the\nrepair mechanisms or damage sensing processes are amongst the first\npathways to become disrupted accelerating the mutational rate. Mutations\nthat occur in genes after the cell has become cancerous may still affect\nthe growth rate, invasiveness and even the response to chemotherapy but\nmay not be present in all cells of a tumour. These genes may be drivers\nof chemo-resistance or metastasis and are equally good targets for\ntherapies.\n\n\nIntOGen-mutations is a platform that aims to identify driver mutations\nusing two methodologies from cancer cohort mutation data: the first\nidentifies mutations that are most likely to have a functional impact\ncombined with identifying genes that are frequently mutated; and the\nsecond, genes that harbour clustered mutations. These measures are all\nindicators of positive selection that occurs in cancer evolution and may\nhelp the identification of driver genes.\n\n\n\n\nAnalysing cancer cohort data with IntOGen\n\u00b6\n\n\nIntOGen-mutations is available as a web based service that can allow\nusers to run their analysis on the host\u2019s servers or it can be downloaded\nand run on a local server.\n\n\nFor the purposes of the course we will be using a local version of\n\nIntOGen\n so that we don\u2019t encounter any issues sharing resources.\n\n\n\n\n\n\nTo begin open a terminal and navigate to the directory \nsomatic/intogen\n.\n\n\n1\ncd\n ~/somatic/intogen\n\n\n\n\n\n\n\n\n\n\nIn this directory you will find a Mutation Annotation Format (MAF) file\ncontaining a cut down version of the somatic variant calls identified from\nmelanoma samples investigated as part of the TCGA cancer genomics projects.\nYou can see what files are in the directory by typing \nls\n, look inside the\nfile using \nless TCGA_Melanoma_SMgene.maf\n and close the file and return to\nthe command line by typing \nq\n.\n\n\n\n\n\n\nRun the \nIntOGen\n analysis by typing\n\n\n1\nintogen -i TCGA_Melanoma_slimSMgene.maf -o TCGA_Mela_out\n\n\n\n\n\n\n\n\n\n\nThe TCGA melanoma maf used in this practical has been modified from the\noriginal to reduce processing time and only contains data for the top\n680 mutated genes.\n\n\nThe tool will take around 10 minutes to run and the progress will be\nindicated by the logging lines printed to the terminal. Once complete\nthe output can be explored.\n\n\nWhilst the tool is running we can explore the options we have used to\nrun \nIntOGen\n.\n\n\n\n\n\n\nTo get a list of \nIntOGen\n options open up a new terminal\n\n\n1\nintogen --help\n\n\n\n\n\n\n\n\n\n\nThis command will list the running options that you can alter as command line\ninputs or in a configuration file. We are using the default options for this run\nso we didn\u2019t have to supply a configuration file and we only used \n-i\n to set\nthe input and \n-o\n to control the mane of the output directory.\n\n\n\n\n\n\nTo look at the default options open up the configuration file by typing\n\n\nbashless ~./intogen/task.confless ~./intogen/system.conf\n\n\n\n\n\n\nIt is important to set the correct genome assembly in the \ntask.conf\n to match\nthe one that you used as your reference when the variant were called. In our\n\ntask.conf\n this should be \nhg19\n.\n\n\n\n\nExploring the output of IntOGen\n\u00b6\n\n\nWhen you run your data over the web on the remote site there is a browse\nfacility that allows you to explore your data using the web version of\nthe database. Running \nIntOGen\n locally provides the same tabular\ninformation but in a flat file format.\n\n\nThere should be 14 files generated from a successful run of this version of \nIntOGen\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\ngene.tsv\ngene.oncodriveclust\npathway.recurrences\ngene.oncodrivefm\nsample_gene.impact\ngene.recurrences                                \nsample_variant+transcript.impact\nsummary.tsv\ntranscript.recurrences\nTCGA_Melanoma_slimSMgene.smconfig\noncodrivefm-pathways-MA_SCORE.tsv\noncodrivefm-pathways-PPH2_SCORE.tsv  \noncodrivefm-pathways-SIFT_SCORE.tsv  \npathway.oncodrivefm\n\n\n\n\n\n\nView these files by using \nls\n as below.\n\n\n1\nls ~/somatic/intogen/TCGA_Mela_out/project/TCGA_Melanoma_slimSMgene/\n\n\n\n\n\n\nThis practical will concentrate on the identification of driver genes so\nwe will look at the main output concerning genes.\nThe \ngene.tsv\n is the main gene centric output summary table.\n\n\n\n\n\n\nOpen up the \ngene.tsv\n file in \nLibreOffice\n by double clicking on the icon on your\ndesktop.\n\n\n\n\n\n\nSelect the file tab and click on open.\n\n\n\n\n\n\nNavigate to the results directory\n\n~/somatic/intogen/TCGA_Mela_out/project/TCGA_Melanoma_slimSMgene/\n\n\n\n\n\n\nDouble click on \ngene.tsv\n.\n\n\n\n\n\n\nIn the pop-up box under the \nSeparator options\n ensure only the tab box is\nchecked and click \nOK\n.\n\n\n\n\n\n\nThis file contains the overall summary results for the \nIntOGen\n pipeline\npresented by gene and reports Q values (i.e. multiple testing corrected\nP values) for the mutation frequency and cluster modules.\n\n\nSignificantly mutated genes from the cohort data are identified using both\nthe \nOncodriveFM\n and \nOncodriveClUST\n modules of \nIntOGen\n. The \nOncodriveFM\n\nmodule detects genes that have accumulated mutations with a high functional\nimpact. It uses annotations from the Ensembl variant effect predictor (VEP, V.70)\nthat includes SIFT and Polyphen2 and precomputed MutationAssessor functional\nimpacts. It calculates a P value per gene from the number of mutations detected\nacross all possible coding bases of a gene with a positive weighting for mutations\nwith a high functional impact. The \nOncodriveCLUST\n module detects genes that\nhave more variants than would be expected across the cohort that alter the\nsame region of the gene.\n\n\nThe file is sorted to bring the most significantly altered genes to the top. The key\ncolumns that help you identify the significantly mutated genes are the 3\nrd\n and 4\nth\n\n(C and D) that indicate which of the modules identified a significant result and\nthe Q-values for the modules that are in 21\nst\n and 23\nrd\n (U and W)\n\n\nThe top twelve genes have significant Q-values for both modules and include BRAF,\nNRAS and TP53. The next 35 are significant by only one of the modules.\n\n\nAll of these have small Q-values which means they are all\nsignificantly mutated genes in this TCGA Melanoma cohort of 338\npatients.\n\n\n\n\nNow look at their sample frequency count (column 9 \nMUTS_CS_SAMPLES\n) these\nare the number of samples that contain at least one mutation in the gene.\n\n\n\n\n\n\nQuestion\n\n\na)   Which significantly mutated gene has mutations in the most samples?\n\n\nb)   Which gene/genes have the lowest Q-value from OncodriveFM and OncodriveCLUST?\n\n\nc)   Why don\u2019t the genes with the lowest Q values also have the highest sample frequency value?\n\n\nAnswer\na)  BRAF has 175 out of 327 cases with a mutation.\nb)  TP53 or PTEN have the lowest OncodriveFM Q-values and NRAS\nhas the lowest Q-value for OncodriveCLUST.\nc)  The P value calculation takes into account the length of the\n    coding sequence of the gene, the mutation rate of the nucleotides\n    contained within it and for OncodriveFM the functional consequences\n    of those changes. Therefore a small gene with a small number of deleterious\n    mutations may have a lower P value and also Q value than a large\n    gene with a high mutation frequency.\n\n\n\n\nThe results for the assessment of clustered mutations in genes carried\nout by the \nOncodriveCLUST\n module of \nIntOGen\n are shown as  amino\nacid residue positions of the encoded protein.\n\n\nThe three known oncogenes BRAF, NRAS and IDH1 have very low CLUST_QVALUEs\nindicating that the mutations in these genes are highly clustered. The\n\nCLUST_COORDS\n column reports that there are 160 samples with mutations\nbetween the amino acid positions 594-601 of BRAF; 84 samples with mutations\nat amino acid position 61 of NRAS; and 15 sample with mutations at amino\nacid position 132 of IDH1.\n\n\n\n\nQuestion\n\n\nWhy are the oncogenes more likely to have clustered mutations and the\ntumour suppressor genes less likely?\n\n\nAnswer\nGain of function mutations are required to activate oncogenes and so\nonly key residues in the protein will result in activation. Tumour\nsuppressors are frequently affected by loss of function mutations and\ndeletions. A truncating mutation or frameshift indel can occur in any\nexon, except the last one, and have the same deleterious functional\nresult.\n\n\n\n\n\nThe other files in the output support the information in this sheet.\n\n\nThe \nsample_variant+transcript.impact\n file includes a summary of all mutations\nfound in each of the genes and protein coding transcripts of those genes for all\nsamples identified that have that mutation. It also reports the variant impact scores\nfrom SIFT, PolyPhen2, MutationAssesor, reporting also impact categories of which\nthere are four; high, medium, low and none.\n\n\n\n\nOpen up the \nsample_variant+transcript.impact\n file and explore the data.\n\n\n\n\n\n\nQuestion\n\n\nCan you find out what the nucleotide change details for the most common\nBRAF mutation that results in V600E amino acid change in the cohort?\nSort the data by GENE, then TRANSCRIPT and then PROTEIN_POS to make this easier. The gene ID for BRAF is \nENSG00000157764\n.\n\n\nAnswer\nIt is an A>T at position chr7:140453136 identified in 127 samples.\n\n\n\n\n\n\nReferences\n\u00b6\n\n\nGunes et al. Nat. Methods 2010\n:   \nhttp://www.nature.com/nmeth/journal/v7/n2/pdf/nmeth0210-92.pdf\n\n\nGonzalez-Perez et al. Nat. Methods 2013\n:   \nhttp://www.nature.com/nmeth/journal/v10/n11/pdf/nmeth.2642.pdf",
            "title": "Cohort analysis for the identification of driver genes"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Run the  IntOGen  analysis software on cohort mutation data.    Have gained experience of the structure of the analysis output files\n    in order to identify potential driver genes.    Have gained overview knowledge of different methods for\n    identification of genes important in cancers.",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#tools-used",
            "text": "IntOGen mutations platform:  https://www.intogen.org/search",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#sources-of-data",
            "text": "TCGA melanoma somatic SNV data from 338 tumour samples:  https://tcga-data.nci.nih.gov/tcga/",
            "title": "Sources of Data"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#useful-links",
            "text": "Mutation Annotation Format (MAF) specification:  https://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+(MAF)+Specification  IntOGen installation instructions: https://bitbucket.org/intogen/intogen-pipeline/overview",
            "title": "Useful Links"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#author-information",
            "text": "Primary Author(s): \nAnn-Marie Patch, QIMR Berghofer  ann-marie.patch@qimrberghofer.edu.au \nErdahl Teber, CMRI  eteber@cmri.org.au     Contributor(s): \nScott Wood  scott.wood@qimrberghofer.edu.au",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#introduction",
            "text": "Cancer driver genes are commonly described as genes that when mutated\ndirectly affect the potential of a cell to become cancerous. They are\nimportant to a tumour cell as they confer a growth or survival advantage\nover the normal surrounding cells. The mutations in these driver genes\nare then clonally selected for as the population of tumour cells\nincreases. We think of the key genes driving tumour initiation\n(development), progression, metastases, resistance and survival. Driver\ngene mutations are often described as \u201cearly\u201d events because they were\nkey in turning a normally functioning and regulated cell into a\ndysregulated one. The logical assumption is that these key mutations\nwill be present in all tumour cells in a patient\u2019s sample; although\nsometime this is not true.  There are two major research goals that underline the need to identify\ndriver genes:    By identifying the early changes that take place researchers might\n    be able to find a treatment to stop the root cause of why cells\n    become malignant.    By identifying groups of patients with the same genes mutated then\n    we can develop therapies that will work for all of them.    When we sequence tumour samples we tend to use samples that come from\nfully developed cancers that can carry hundreds to thousands of\nmutations in genes and many more outside of genes. The accumulation of\nthese passenger mutations in cancer cells can happen because often the\nrepair mechanisms or damage sensing processes are amongst the first\npathways to become disrupted accelerating the mutational rate. Mutations\nthat occur in genes after the cell has become cancerous may still affect\nthe growth rate, invasiveness and even the response to chemotherapy but\nmay not be present in all cells of a tumour. These genes may be drivers\nof chemo-resistance or metastasis and are equally good targets for\ntherapies.  IntOGen-mutations is a platform that aims to identify driver mutations\nusing two methodologies from cancer cohort mutation data: the first\nidentifies mutations that are most likely to have a functional impact\ncombined with identifying genes that are frequently mutated; and the\nsecond, genes that harbour clustered mutations. These measures are all\nindicators of positive selection that occurs in cancer evolution and may\nhelp the identification of driver genes.",
            "title": "Introduction"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#analysing-cancer-cohort-data-with-intogen",
            "text": "IntOGen-mutations is available as a web based service that can allow\nusers to run their analysis on the host\u2019s servers or it can be downloaded\nand run on a local server.  For the purposes of the course we will be using a local version of IntOGen  so that we don\u2019t encounter any issues sharing resources.    To begin open a terminal and navigate to the directory  somatic/intogen .  1 cd  ~/somatic/intogen     In this directory you will find a Mutation Annotation Format (MAF) file\ncontaining a cut down version of the somatic variant calls identified from\nmelanoma samples investigated as part of the TCGA cancer genomics projects.\nYou can see what files are in the directory by typing  ls , look inside the\nfile using  less TCGA_Melanoma_SMgene.maf  and close the file and return to\nthe command line by typing  q .    Run the  IntOGen  analysis by typing  1 intogen -i TCGA_Melanoma_slimSMgene.maf -o TCGA_Mela_out     The TCGA melanoma maf used in this practical has been modified from the\noriginal to reduce processing time and only contains data for the top\n680 mutated genes.  The tool will take around 10 minutes to run and the progress will be\nindicated by the logging lines printed to the terminal. Once complete\nthe output can be explored.  Whilst the tool is running we can explore the options we have used to\nrun  IntOGen .    To get a list of  IntOGen  options open up a new terminal  1 intogen --help     This command will list the running options that you can alter as command line\ninputs or in a configuration file. We are using the default options for this run\nso we didn\u2019t have to supply a configuration file and we only used  -i  to set\nthe input and  -o  to control the mane of the output directory.    To look at the default options open up the configuration file by typing  bashless ~./intogen/task.confless ~./intogen/system.conf    It is important to set the correct genome assembly in the  task.conf  to match\nthe one that you used as your reference when the variant were called. In our task.conf  this should be  hg19 .",
            "title": "Analysing cancer cohort data with IntOGen"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#exploring-the-output-of-intogen",
            "text": "When you run your data over the web on the remote site there is a browse\nfacility that allows you to explore your data using the web version of\nthe database. Running  IntOGen  locally provides the same tabular\ninformation but in a flat file format.  There should be 14 files generated from a successful run of this version of  IntOGen :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 gene.tsv\ngene.oncodriveclust\npathway.recurrences\ngene.oncodrivefm\nsample_gene.impact\ngene.recurrences                                \nsample_variant+transcript.impact\nsummary.tsv\ntranscript.recurrences\nTCGA_Melanoma_slimSMgene.smconfig\noncodrivefm-pathways-MA_SCORE.tsv\noncodrivefm-pathways-PPH2_SCORE.tsv  \noncodrivefm-pathways-SIFT_SCORE.tsv  \npathway.oncodrivefm   View these files by using  ls  as below.  1 ls ~/somatic/intogen/TCGA_Mela_out/project/TCGA_Melanoma_slimSMgene/   This practical will concentrate on the identification of driver genes so\nwe will look at the main output concerning genes.\nThe  gene.tsv  is the main gene centric output summary table.    Open up the  gene.tsv  file in  LibreOffice  by double clicking on the icon on your\ndesktop.    Select the file tab and click on open.    Navigate to the results directory ~/somatic/intogen/TCGA_Mela_out/project/TCGA_Melanoma_slimSMgene/    Double click on  gene.tsv .    In the pop-up box under the  Separator options  ensure only the tab box is\nchecked and click  OK .    This file contains the overall summary results for the  IntOGen  pipeline\npresented by gene and reports Q values (i.e. multiple testing corrected\nP values) for the mutation frequency and cluster modules.  Significantly mutated genes from the cohort data are identified using both\nthe  OncodriveFM  and  OncodriveClUST  modules of  IntOGen . The  OncodriveFM \nmodule detects genes that have accumulated mutations with a high functional\nimpact. It uses annotations from the Ensembl variant effect predictor (VEP, V.70)\nthat includes SIFT and Polyphen2 and precomputed MutationAssessor functional\nimpacts. It calculates a P value per gene from the number of mutations detected\nacross all possible coding bases of a gene with a positive weighting for mutations\nwith a high functional impact. The  OncodriveCLUST  module detects genes that\nhave more variants than would be expected across the cohort that alter the\nsame region of the gene.  The file is sorted to bring the most significantly altered genes to the top. The key\ncolumns that help you identify the significantly mutated genes are the 3 rd  and 4 th \n(C and D) that indicate which of the modules identified a significant result and\nthe Q-values for the modules that are in 21 st  and 23 rd  (U and W)  The top twelve genes have significant Q-values for both modules and include BRAF,\nNRAS and TP53. The next 35 are significant by only one of the modules.  All of these have small Q-values which means they are all\nsignificantly mutated genes in this TCGA Melanoma cohort of 338\npatients.   Now look at their sample frequency count (column 9  MUTS_CS_SAMPLES ) these\nare the number of samples that contain at least one mutation in the gene.    Question  a)   Which significantly mutated gene has mutations in the most samples?  b)   Which gene/genes have the lowest Q-value from OncodriveFM and OncodriveCLUST?  c)   Why don\u2019t the genes with the lowest Q values also have the highest sample frequency value?  Answer a)  BRAF has 175 out of 327 cases with a mutation. b)  TP53 or PTEN have the lowest OncodriveFM Q-values and NRAS\nhas the lowest Q-value for OncodriveCLUST. c)  The P value calculation takes into account the length of the\n    coding sequence of the gene, the mutation rate of the nucleotides\n    contained within it and for OncodriveFM the functional consequences\n    of those changes. Therefore a small gene with a small number of deleterious\n    mutations may have a lower P value and also Q value than a large\n    gene with a high mutation frequency.   The results for the assessment of clustered mutations in genes carried\nout by the  OncodriveCLUST  module of  IntOGen  are shown as  amino\nacid residue positions of the encoded protein.  The three known oncogenes BRAF, NRAS and IDH1 have very low CLUST_QVALUEs\nindicating that the mutations in these genes are highly clustered. The CLUST_COORDS  column reports that there are 160 samples with mutations\nbetween the amino acid positions 594-601 of BRAF; 84 samples with mutations\nat amino acid position 61 of NRAS; and 15 sample with mutations at amino\nacid position 132 of IDH1.   Question  Why are the oncogenes more likely to have clustered mutations and the\ntumour suppressor genes less likely?  Answer Gain of function mutations are required to activate oncogenes and so\nonly key residues in the protein will result in activation. Tumour\nsuppressors are frequently affected by loss of function mutations and\ndeletions. A truncating mutation or frameshift indel can occur in any\nexon, except the last one, and have the same deleterious functional\nresult.   \nThe other files in the output support the information in this sheet.  The  sample_variant+transcript.impact  file includes a summary of all mutations\nfound in each of the genes and protein coding transcripts of those genes for all\nsamples identified that have that mutation. It also reports the variant impact scores\nfrom SIFT, PolyPhen2, MutationAssesor, reporting also impact categories of which\nthere are four; high, medium, low and none.   Open up the  sample_variant+transcript.impact  file and explore the data.    Question  Can you find out what the nucleotide change details for the most common\nBRAF mutation that results in V600E amino acid change in the cohort?\nSort the data by GENE, then TRANSCRIPT and then PROTEIN_POS to make this easier. The gene ID for BRAF is  ENSG00000157764 .  Answer It is an A>T at position chr7:140453136 identified in 127 samples.",
            "title": "Exploring the output of IntOGen"
        },
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#references",
            "text": "Gunes et al. Nat. Methods 2010\n:    http://www.nature.com/nmeth/journal/v7/n2/pdf/nmeth0210-92.pdf  Gonzalez-Perez et al. Nat. Methods 2013\n:    http://www.nature.com/nmeth/journal/v10/n11/pdf/nmeth.2642.pdf",
            "title": "References"
        },
        {
            "location": "/modules/cancer-module-viz/visu/",
            "text": "Important\n\n\nThis is an advanced module. \n\n\n\n\n\nThis module was written by Mathieu Bourgey and the original on-line version is available\n\nhere.\n\n\n\n\nKey Learning Outcomes\n\u00b6\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\nGenerate Circos like graphics using R\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\nTools Used\n\u00b6\n\n\nR:\n\n\nhttps://cran.r-project.org/\n\n\nR package circlize:\n\n\nhttps://cran.r-project.org/web/packages/circlize/index.html\n\n\n\n\nAuthor Information\n\u00b6\n\n\nPrimary Author(s):\n\nMathieu Bourgey \nmathieu.bourgey@mcgill.ca\n  \n\n\nContributor(s):\n    \n\n\n\n\nIntroduction\n\u00b6\n\n\nThis short workshop will show you how to visualize your data.\n\n\nWe will be working on 3 types of somatic calls:\n\n\n\n\n\n\nSNV calls from MuTect (vcf)\n\n\n\n\n\n\nSV calls from DELLY (vcf)\n\n\n\n\n\n\nCNV calls from SCoNEs (tsv)\n\n\n\n\n\n\n\n\nPrepare the Environment\n\u00b6\n\n\nWe will use a dataset derived from the analysis of whole genome sequencing\npaired normal/tumour samples.\n\n\nThe call files are contained in the folder \nvisualization\n:\n\n\n\n\nmutect.somatic.vcf\n  \n\n\ndelly.somatic.vcf\n    \n\n\nscones.somatic.tsv\n   \n\n\n\n\nMany tools are available to do this and the most commonly known is Circos.\nCircos is a really not user-friendly. In this tutorial, we show you an\neasy alternative to build a circular representation of genomic data.\n\n\nFirst we need to go in the folder \nvisualization\n to do the analysis:\n\n\n1\ncd /home/trainee/visualization/\n\n\n\n\n\n\nLet\u2019s see what is in this folder:\n\n\n1\nls\n\n\n\n\n\n\ncircos.R  delly.somatic.vcf  mutect.somatic.vcf  scones.somatic.tsv\n\n\nTake a look at the data files.\n\n\nThis data has \nnot\n been restricted to a short piece of a chromosome and SNVs\nhave already been filtered:\n\n\n1\n2\n3\nless mutect.somatic.vcf\nless delly.somatic.vcf\nless scones.somatic.tsv\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat can you see from this data?\n\n\nAnswer\nthe filtered output of 3 different software: Mutect (SNVs),\nDelly (SVs), SCoNEs (CNVs).\nThe 3 files show 2 different formats (vcf, tsv).\nAlmost all type of variants are represented here: mutations,\ndeletion, inversion, translocation, large amplification and deletion\n(CNVs).\n\n\n\n\n\n\nQuestion\n\n\nWhy don\u2019t we use the vcf format for all types of calls?\n\n\nAnswer\nThe 1000 Genomes project tries to use/include SV calls in the vcf format.\nSome tools like Delly use this format for SVs. It is a good idea to try to\ninclude everything together but this not the be the best way to handle SVs and\nCNVs.\n\n\n\n\n\n\nQuestion\n\n\nWhy?\n\n\nAnswer\nDue to the nature of these calls, you can not easily integrate the\npositional information of the two breakpoints (that could be located\nfar away or in an other chromosome) using a single position format.\n\n\n\n\n\n\nThe analysis will be done using the R program:\n\n\n1\nR\n\n\n\n\n\n\nWe will use the circlize package from the cran R project. This package generates\ncircular plots and has the advantage of being able to provide\npre-built functions for genomic data. One of the main advantages of this\ntool is the use of bed format as input data.\n\n\n1\nlibrary(circlize)\n\n\n\n\n\n\n\nLet\u2019s import the variants:\n\n\n1\n2\n3\nsnp\n=\nread.table\n(\n\"mutect.somatic.vcf\"\n)\n\nsv\n=\nread.table\n(\n\"delly.somatic.vcf\"\n)\n\ncnv\n=\nread.table\n(\n\"scones.somatic.tsv\"\n,\nheader\n=\nT\n)\n\n\n\n\n\n\n\n\nWe need to set up the generic graphical parameters:\n\n\n1\n2\n3\n4\n5\nx11\n()\n\npar\n(\nmar \n=\n \nc\n(\n1\n,\n \n1\n,\n \n1\n,\n \n1\n))\n\ncircos.par\n(\n\"start.degree\"\n \n=\n \n90\n)\n\ncircos.par\n(\n\"track.height\"\n \n=\n \n0.05\n)\n\ncircos.par\n(\n\"canvas.xlim\"\n \n=\n \nc\n(\n-1.3\n,\n \n1.3\n),\n \n\"canvas.ylim\"\n \n=\n \nc\n(\n-1.3\n,\n \n1.3\n))\n\n\n\n\n\n\n\n\nLet\u2019s draw hg19 reference ideograms:\n\n\n1\ncircos.initializeWithIdeogram\n(\nspecies \n=\n \n\"hg19\"\n)\n\n\n\n\n\n\n\n\nWe need to ensure our data to fits the hg19 standards so the main thing to\ncheck is that we have \nchr\n at the beginning of the chromosome names.\n\n\nWe can now draw 1 track for somatic mutations:\n\n\n1\n2\n3\n4\n5\nsnv_tmp\n=\nread.table\n(\n\"mutect.somatic.vcf\"\n,\ncomment.char\n=\n\"#\"\n)\n\nsnv\n=\ncbind\n(\npaste\n(\n\"chr\"\n,\nas.character\n(\nsnp\n[,\n1\n]),\nsep\n=\n\"\"\n),\nsnp\n[\n2\n],\nsnp\n[,\n2\n]\n+1\n)\n\ncircos.genomicTrackPlotRegion\n(\nsnv\n,\nstack\n=\nTRUE\n,\n panel.fun \n=\n \nfunction\n(\nregion\n,\n value\n,\n \n...\n)\n \n{\n\n    circos.genomicPoints\n(\nregion\n,\n value\n,\n cex \n=\n \n0.05\n,\n pch \n=\n \n9\n,\ncol\n=\n'orange'\n \n,\n \n...\n)\n\n\n})\n\n\n\n\n\n\n\n\nLet\u2019s draw the 2 tracks for CNVs. One track for duplications in red and\none blue track for deletions.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ndup\n=\ncnv\n[\ncnv\n[,\n5\n]\n>\n2\n,]\n\ndup\n[,\n1\n]\n=\npaste\n(\n\"chr\"\n,\nas.character\n(\ndup\n[,\n1\n]),\nsep\n=\n\"\"\n)\n\ndel\n=\ncnv\n[\ncnv\n[,\n5\n]\n<\n2\n,]\n\ndel\n[,\n1\n]\n=\npaste\n(\n\"chr\"\n,\nas.character\n(\ndel\n[,\n1\n]),\nsep\n=\n\"\"\n)\n\ncircos.genomicTrackPlotRegion\n(\ndup\n,\n stack \n=\n \nTRUE\n,\npanel.fun \n=\n \nfunction\n(\nregion\n,\n value\n,\n \n...\n)\n \n{\n\n        circos.genomicRect\n(\nregion\n,\n value\n,\n col \n=\n \n\"red\"\n,\nbg.border \n=\n \nNA\n,\n cex\n=\n1\n \n,\n \n...\n)\n\n\n})\n\ncircos.genomicTrackPlotRegion\n(\ndel\n,\n stack \n=\n \nTRUE\n,\npanel.fun \n=\n \nfunction\n(\nregion\n,\n value\n,\n \n...\n)\n \n{\n\n        circos.genomicRect\n(\nregion\n,\n value\n,\n col \n=\n \n\"blue\"\n,\nbg.border \n=\n \nNA\n,\n cex\n=\n1\n \n,\n \n...\n)\n\n\n})\n\n\n\n\n\n\n\nWe can clearly see a massive deletion in chromosome 3.\n\n\n\nTo finish we just need to draw 3 tracks + positional links to represent\nSVs.\n\n\nUnfortunately the vcf format has not been designed for SVs. SVs are\ndefined by 2 breakpoints and the vcf format stores the second one in the\ninfo field. So we will need to extract this information to draw these\ncalls.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nchrEnd\n=\nNULL\n\nposEnd\n=\nNULL\n\n\nfor\n \n(\ni \nin\n \n1\n:\ndim\n(\nsv\n)[\n1\n])\n \n{\n\n    addInfo\n=\nstrsplit\n(\nas.character\n(\nsv\n[\ni\n,\n8\n]),\nsplit\n=\n\";\"\n)\n\n    chrInf\n=\nstrsplit\n(\naddInfo\n[[\n1\n]][\n3\n],\nsplit\n=\n\"=\"\n)\n\n    chrEnd\n=\nc\n(\nchrEnd\n,\nchrInf\n[[\n1\n]][\n2\n])\n\n    posInf\n=\nstrsplit\n(\naddInfo\n[[\n1\n]][\n4\n],\nsplit\n=\n\"=\"\n)\n\n    posEnd\n=\nc\n(\nposEnd\n,\nposInf\n[[\n1\n]][\n2\n])\n\n\n}\n\nsvTable\n=\ndata.frame\n(\npaste\n(\n\"chr\"\n,\nsv\n[,\n1\n],\nsep\n=\n\"\"\n),\nas.numeric\n(\nsv\n[,\n2\n]),\nas.numeric\n(\nposEnd\n),\npaste\n(\n\"chr\"\n,\nchrEnd\n,\nsep\n=\n\"\"\n),\nas.character\n(\nsv\n[,\n5\n]))\n\n\n\n\n\n\n\n\nNow that we have reformatted the SV calls, let\u2019s draw them.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ntypeE\n=\nc\n(\n\"<DEL>\"\n,\n\"<INS>\"\n,\n\"<INV>\"\n)\n\ncolE\n=\nc\n(\n\"blue\"\n,\n\"black\"\n,\n\"green\"\n)\n\n\nfor\n \n(\ni \nin\n \n1\n:\n3\n)\n \n{\n\n        bed_list\n=\nsvTable\n[\nsvTable\n[,\n5\n]\n==\ntypeE\n[\ni\n],]\n\n        circos.genomicTrackPlotRegion\n(\nbed_list\n,\nstack\n=\nTRUE\n,\n panel.fun \n=\n \nfunction\n(\nregion\n,\n value\n,\n \n...\n)\n \n{\n\n                circos.genomicPoints\n(\nregion\n,\n value\n,\n cex \n=\n \n0.5\n,\n pch \n=\n \n16\n,\n col \n=\n colE\n[\ni\n],\n \n...\n)\n\n        \n})\n\n\n}\n\n\nbed1\n=\ncbind\n(\nsvTable\n[\nsvTable\n[,\n5\n]\n==\n\"<TRA>\"\n,\n1\n:\n2\n],\nsvTable\n[\nsvTable\n[,\n5\n]\n==\n\"<TRA>\"\n,\n2\n]\n+5\n)\n\nbed2\n=\ncbind\n(\nsvTable\n[\nsvTable\n[,\n5\n]\n==\n\"<TRA>\"\n,\nc\n(\n4\n,\n3\n)],\nsvTable\n[\nsvTable\n[,\n5\n]\n==\n\"<TRA>\"\n,\n3\n]\n+5\n)\n\n\n\nfor\n \n(\ni \nin\n \n1\n:\ndim\n(\nbed1\n)[\n1\n])\n \n{\n\n    circos.link\n(\nbed1\n[\ni\n,\n1\n],\nbed1\n[\ni\n,\n2\n],\nbed2\n[\ni\n,\n1\n],\nbed2\n[\ni\n,\n2\n])\n\n\n}\n\n\n\n\n\n\n\n\nA good graph needs a title and legend:\n\n\n1\n2\n3\ntitle\n(\n\"Somatic calls (SNV - SV - CNV)\"\n)\n\nlegend\n(\n0.7\n,\n1.4\n,\nlegend\n=\nc\n(\n\"SNV\"\n,\n \n\"CNV-DUPLICATION\"\n,\n\"CNV-DELETION\"\n,\n\"SV-DELETION\"\n,\n\"SV-INSERTION\"\n,\n\"SV-INVERSION\"\n),\ncol\n=\nc\n(\n\"orange\"\n,\n\"red\"\n,\n\"blue\"\n,\n\"blue\"\n,\n\"black\"\n,\n\"green\"\n,\n\"red\"\n),\npch\n=\nc\n(\n16\n,\n15\n,\n15\n,\n16\n,\n16\n,\n16\n,\n16\n,\n16\n),\ncex\n=\n0.75\n,\ntitle\n=\n\"Tracks:\"\n,\nbty\n=\n'n'\n)\n\nlegend\n(\n0.6\n,\n0.95\n,\nlegend\n=\n\"SV-TRANSLOCATION\"\n,\ncol\n=\n\"black\"\n,\nlty\n=\n1\n,\ncex\n=\n0.75\n,\nlwd\n=\n1.2\n,\nbty\n=\n'n'\n)\n\n\n\n\n\n\n\n\nYou should obtain a plot like this one:\n\n\n\n\n\nNow save the image to the visualization directory as a .pdf:\n\n\n1\n2\ndev.copy2pdf\n(\nfile \n=\n \n\"/home/trainee/visualization/variant_visualization.pdf\"\n)\n\ndev.off\n()\n\n\n\n\n\n\n\nFinally exit R\n  \n1\nq\n(\n\"yes\"\n)\n\n\n\n\n\n\n\n\nAcknowledgements\n\u00b6\n\n\nMathieu Bourgey would like to thank and acknowledge Louis Letourneau for this help and\nfor sharing his material. The format of the tutorial has been inspired\nfrom Mar Gonzalez Porta. I also want to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.\n\n\nLicense\n\u00b6\n\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike\n3.0 Unported License. This means that you are able to copy, share and\nmodify the work, as long as the result is distributed under the same\nlicense.",
            "title": "Variant Visualisation"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:   Generate Circos like graphics using R",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#tools-used",
            "text": "R:  https://cran.r-project.org/  R package circlize:  https://cran.r-project.org/web/packages/circlize/index.html",
            "title": "Tools Used"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#author-information",
            "text": "Primary Author(s): \nMathieu Bourgey  mathieu.bourgey@mcgill.ca     Contributor(s):",
            "title": "Author Information"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#introduction",
            "text": "This short workshop will show you how to visualize your data.  We will be working on 3 types of somatic calls:    SNV calls from MuTect (vcf)    SV calls from DELLY (vcf)    CNV calls from SCoNEs (tsv)",
            "title": "Introduction"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#prepare-the-environment",
            "text": "We will use a dataset derived from the analysis of whole genome sequencing\npaired normal/tumour samples.  The call files are contained in the folder  visualization :   mutect.somatic.vcf     delly.somatic.vcf       scones.somatic.tsv       Many tools are available to do this and the most commonly known is Circos.\nCircos is a really not user-friendly. In this tutorial, we show you an\neasy alternative to build a circular representation of genomic data.  First we need to go in the folder  visualization  to do the analysis:  1 cd /home/trainee/visualization/   Let\u2019s see what is in this folder:  1 ls   circos.R  delly.somatic.vcf  mutect.somatic.vcf  scones.somatic.tsv  Take a look at the data files.  This data has  not  been restricted to a short piece of a chromosome and SNVs\nhave already been filtered:  1\n2\n3 less mutect.somatic.vcf\nless delly.somatic.vcf\nless scones.somatic.tsv     Question  What can you see from this data?  Answer the filtered output of 3 different software: Mutect (SNVs),\nDelly (SVs), SCoNEs (CNVs). The 3 files show 2 different formats (vcf, tsv). Almost all type of variants are represented here: mutations,\ndeletion, inversion, translocation, large amplification and deletion\n(CNVs).    Question  Why don\u2019t we use the vcf format for all types of calls?  Answer The 1000 Genomes project tries to use/include SV calls in the vcf format.\nSome tools like Delly use this format for SVs. It is a good idea to try to\ninclude everything together but this not the be the best way to handle SVs and\nCNVs.    Question  Why?  Answer Due to the nature of these calls, you can not easily integrate the\npositional information of the two breakpoints (that could be located\nfar away or in an other chromosome) using a single position format.    The analysis will be done using the R program:  1 R   We will use the circlize package from the cran R project. This package generates\ncircular plots and has the advantage of being able to provide\npre-built functions for genomic data. One of the main advantages of this\ntool is the use of bed format as input data.  1 library(circlize)   \nLet\u2019s import the variants:  1\n2\n3 snp = read.table ( \"mutect.somatic.vcf\" ) \nsv = read.table ( \"delly.somatic.vcf\" ) \ncnv = read.table ( \"scones.somatic.tsv\" , header = T )    \nWe need to set up the generic graphical parameters:  1\n2\n3\n4\n5 x11 () \npar ( mar  =   c ( 1 ,   1 ,   1 ,   1 )) \ncircos.par ( \"start.degree\"   =   90 ) \ncircos.par ( \"track.height\"   =   0.05 ) \ncircos.par ( \"canvas.xlim\"   =   c ( -1.3 ,   1.3 ),   \"canvas.ylim\"   =   c ( -1.3 ,   1.3 ))    \nLet\u2019s draw hg19 reference ideograms:  1 circos.initializeWithIdeogram ( species  =   \"hg19\" )    \nWe need to ensure our data to fits the hg19 standards so the main thing to\ncheck is that we have  chr  at the beginning of the chromosome names.  We can now draw 1 track for somatic mutations:  1\n2\n3\n4\n5 snv_tmp = read.table ( \"mutect.somatic.vcf\" , comment.char = \"#\" ) \nsnv = cbind ( paste ( \"chr\" , as.character ( snp [, 1 ]), sep = \"\" ), snp [ 2 ], snp [, 2 ] +1 ) \ncircos.genomicTrackPlotRegion ( snv , stack = TRUE ,  panel.fun  =   function ( region ,  value ,   ... )   { \n    circos.genomicPoints ( region ,  value ,  cex  =   0.05 ,  pch  =   9 , col = 'orange'   ,   ... )  })    \nLet\u2019s draw the 2 tracks for CNVs. One track for duplications in red and\none blue track for deletions.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 dup = cnv [ cnv [, 5 ] > 2 ,] \ndup [, 1 ] = paste ( \"chr\" , as.character ( dup [, 1 ]), sep = \"\" ) \ndel = cnv [ cnv [, 5 ] < 2 ,] \ndel [, 1 ] = paste ( \"chr\" , as.character ( del [, 1 ]), sep = \"\" ) \ncircos.genomicTrackPlotRegion ( dup ,  stack  =   TRUE , panel.fun  =   function ( region ,  value ,   ... )   { \n        circos.genomicRect ( region ,  value ,  col  =   \"red\" , bg.border  =   NA ,  cex = 1   ,   ... )  }) \ncircos.genomicTrackPlotRegion ( del ,  stack  =   TRUE , panel.fun  =   function ( region ,  value ,   ... )   { \n        circos.genomicRect ( region ,  value ,  col  =   \"blue\" , bg.border  =   NA ,  cex = 1   ,   ... )  })    We can clearly see a massive deletion in chromosome 3.  \nTo finish we just need to draw 3 tracks + positional links to represent\nSVs.  Unfortunately the vcf format has not been designed for SVs. SVs are\ndefined by 2 breakpoints and the vcf format stores the second one in the\ninfo field. So we will need to extract this information to draw these\ncalls.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 chrEnd = NULL \nposEnd = NULL  for   ( i  in   1 : dim ( sv )[ 1 ])   { \n    addInfo = strsplit ( as.character ( sv [ i , 8 ]), split = \";\" ) \n    chrInf = strsplit ( addInfo [[ 1 ]][ 3 ], split = \"=\" ) \n    chrEnd = c ( chrEnd , chrInf [[ 1 ]][ 2 ]) \n    posInf = strsplit ( addInfo [[ 1 ]][ 4 ], split = \"=\" ) \n    posEnd = c ( posEnd , posInf [[ 1 ]][ 2 ])  } \nsvTable = data.frame ( paste ( \"chr\" , sv [, 1 ], sep = \"\" ), as.numeric ( sv [, 2 ]), as.numeric ( posEnd ), paste ( \"chr\" , chrEnd , sep = \"\" ), as.character ( sv [, 5 ]))    \nNow that we have reformatted the SV calls, let\u2019s draw them.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 typeE = c ( \"<DEL>\" , \"<INS>\" , \"<INV>\" ) \ncolE = c ( \"blue\" , \"black\" , \"green\" )  for   ( i  in   1 : 3 )   { \n        bed_list = svTable [ svTable [, 5 ] == typeE [ i ],] \n        circos.genomicTrackPlotRegion ( bed_list , stack = TRUE ,  panel.fun  =   function ( region ,  value ,   ... )   { \n                circos.genomicPoints ( region ,  value ,  cex  =   0.5 ,  pch  =   16 ,  col  =  colE [ i ],   ... ) \n         })  } \n\nbed1 = cbind ( svTable [ svTable [, 5 ] == \"<TRA>\" , 1 : 2 ], svTable [ svTable [, 5 ] == \"<TRA>\" , 2 ] +5 ) \nbed2 = cbind ( svTable [ svTable [, 5 ] == \"<TRA>\" , c ( 4 , 3 )], svTable [ svTable [, 5 ] == \"<TRA>\" , 3 ] +5 )  for   ( i  in   1 : dim ( bed1 )[ 1 ])   { \n    circos.link ( bed1 [ i , 1 ], bed1 [ i , 2 ], bed2 [ i , 1 ], bed2 [ i , 2 ])  }    \nA good graph needs a title and legend:  1\n2\n3 title ( \"Somatic calls (SNV - SV - CNV)\" ) \nlegend ( 0.7 , 1.4 , legend = c ( \"SNV\" ,   \"CNV-DUPLICATION\" , \"CNV-DELETION\" , \"SV-DELETION\" , \"SV-INSERTION\" , \"SV-INVERSION\" ), col = c ( \"orange\" , \"red\" , \"blue\" , \"blue\" , \"black\" , \"green\" , \"red\" ), pch = c ( 16 , 15 , 15 , 16 , 16 , 16 , 16 , 16 ), cex = 0.75 , title = \"Tracks:\" , bty = 'n' ) \nlegend ( 0.6 , 0.95 , legend = \"SV-TRANSLOCATION\" , col = \"black\" , lty = 1 , cex = 0.75 , lwd = 1.2 , bty = 'n' )    \nYou should obtain a plot like this one:   \nNow save the image to the visualization directory as a .pdf:  1\n2 dev.copy2pdf ( file  =   \"/home/trainee/visualization/variant_visualization.pdf\" ) \ndev.off ()    \nFinally exit R\n   1 q ( \"yes\" )",
            "title": "Prepare the Environment"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#acknowledgements",
            "text": "Mathieu Bourgey would like to thank and acknowledge Louis Letourneau for this help and\nfor sharing his material. The format of the tutorial has been inspired\nfrom Mar Gonzalez Porta. I also want to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.",
            "title": "Acknowledgements"
        },
        {
            "location": "/modules/cancer-module-viz/visu/#license",
            "text": "This work is licensed under a Creative Commons Attribution-ShareAlike\n3.0 Unported License. This means that you are able to copy, share and\nmodify the work, as long as the result is distributed under the same\nlicense.",
            "title": "License"
        },
        {
            "location": "/license/",
            "text": "This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at \nhttp://creativecommons.org/licenses/by/3.0/legalcode\n.\n\n\nYou are free:\n  \n\n\n\n\nto copy, distribute, display, and perform the work  \n\n\nto make derivative works  \n\n\nto make commercial use of the work  \n\n\n\n\nUnder the following conditions:\n  \n\n\n\n\nAttribution\n - You must give the original author credit.\n\n\n\n\nWith the understanding that:\n\n\n\n\nWaiver\n - Any of the above conditions can be waived if you get permission from the copyright holder.\n\n\nPublic Domain\n - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.\n\n\n\n\nOther Rights\n - In no way are any of the following rights affected by the license:\n\n\n\n\nYour fair dealing or fair use rights, or other applicable copyright exceptions and limitations;  \n\n\nThe author\u2019s moral rights;  \n\n\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.  \n\n\n\n\n\n\n\n\nNotice\n - For any reuse or distribution, you must make clear to others the license terms of this work.",
            "title": "License"
        }
    ]
}